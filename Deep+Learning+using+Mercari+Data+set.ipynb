{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "In this exercise I will use the features I previously engineered using R and Kaggle Mercari Price challenge data set. \n",
    "\n",
    "We will start with loading the libraries and functions we will need during the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import SGD\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_condition_id  price  shipping  no.brand_name  log.excl.description  \\\n",
       "1                  1    8.0         0              1                   0.0   \n",
       "2                  2   39.0         1              1                   0.0   \n",
       "3                  1   30.0         1              0                   0.0   \n",
       "4                  2  470.0         1              0                   0.0   \n",
       "5                  2   22.0         0              0                   0.0   \n",
       "\n",
       "   excl.name  dollar.description  fancy.categories  cheap.categories  \\\n",
       "1          0                   0                 0                 0   \n",
       "2          0                   0                 0                 0   \n",
       "3          0                   0                 0                 0   \n",
       "4          0                   0                 0                 0   \n",
       "5          0                   0                 0                 0   \n",
       "\n",
       "   fancy.brands        ...         now  cheap  buy  excellent  great  \\\n",
       "1             0        ...           0      0    0          0      0   \n",
       "2             0        ...           0      0    0          0      0   \n",
       "3             0        ...           0      0    0          0      0   \n",
       "4             1        ...           0      0    0          0      0   \n",
       "5             0        ...           0      0    0          0      0   \n",
       "\n",
       "   michael.brand  jordan.name  iphon.name  bundl.name  cap.letter.brand  \n",
       "1              0            0           0           0                 1  \n",
       "2              0            0           0           0                 1  \n",
       "3              0            0           0           1                 0  \n",
       "4              0            0           0           0                 6  \n",
       "5              0            0           0           0                 2  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load the mini subtraining set we prepared previously\n",
    "mini_subtrain = pd.read_csv(\"mini_subtrain.csv\", index_col = 0)\n",
    "mini_subtrain.shape\n",
    "mini_subtrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_subtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "mini_subtrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sucessfully reading the verifying the training data set we have previously constructed using R, we can start building a small neural network and training it by using our data.\n",
    "\n",
    "First we start with seperating predictors and response arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = np.array(mini_subtrain.drop([\"price\"], axis=1))\n",
    "# We will log transform the target variable as we have performed in R\n",
    "target = np.array(np.log(mini_subtrain.price + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can start building our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 98us/step - loss: 1.3796 - val_loss: 0.5341\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4969 - val_loss: 0.4515\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4742 - val_loss: 0.4472\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4719 - val_loss: 0.4500\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4720 - val_loss: 0.4457\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 65us/step - loss: 0.4704 - val_loss: 0.4510\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 64us/step - loss: 0.4698 - val_loss: 0.4503\n"
     ]
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_1 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our loss function is \"mean squared error\". We will take the square root of this to follow \"root mean squared error\" (RMSE). We also keep in mind that the target is log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66764026261785125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_1 = np.sqrt(min(model_1.history[\"val_loss\"]))\n",
    "RMSE_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this RMSE is close to what we have obtained other machine learning algorithms previously. Therefore, we will continue our experiment by increasing model complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 25s 953us/step - loss: 0.6853 - val_loss: 0.4837\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 25s 949us/step - loss: 0.4860 - val_loss: 0.4547\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 25s 968us/step - loss: 0.4801 - val_loss: 0.4599\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 29s 1ms/step - loss: 0.4754 - val_loss: 0.4593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67429454720879667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1000,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_2 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_2 = np.sqrt(min(model_2.history[\"val_loss\"]))\n",
    "RMSE_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our first model is already at its capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 111us/step - loss: 1.0952 - val_loss: 0.4894\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4874 - val_loss: 0.4550\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4755 - val_loss: 0.4524\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4727 - val_loss: 0.4561\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4711 - val_loss: 0.4517\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 0.4686 - val_loss: 0.4507\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 68us/step - loss: 0.4679 - val_loss: 0.4551\n",
      "Epoch 8/30\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 0.4683 - val_loss: 0.4571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67136944170640311"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_3 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_3 = np.sqrt(min(model_3.history[\"val_loss\"]))\n",
    "RMSE_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we reached to model capacity even with a relatively simple network. Next, we will try if we can reduce the bias by training a larger data set, which we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrain = pd.read_csv(\"subtrain.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(741269, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741269 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741269 non-null int64\n",
      "price                   741269 non-null float64\n",
      "shipping                741269 non-null int64\n",
      "no.brand_name           741269 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741269 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741269 non-null int64\n",
      "cheap.categories        741269 non-null int64\n",
      "fancy.brands            741269 non-null int64\n",
      "cheap.brands            741269 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741269 non-null int64\n",
      "jordan.name             741269 non-null int64\n",
      "iphon.name              741269 non-null int64\n",
      "bundl.name              741269 non-null int64\n",
      "cap.letter.brand        741269 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "#We need to check for missing values in the data set\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741268 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741268 non-null int64\n",
      "price                   741268 non-null float64\n",
      "shipping                741268 non-null int64\n",
      "no.brand_name           741268 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741268 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741268 non-null int64\n",
      "cheap.categories        741268 non-null int64\n",
      "fancy.brands            741268 non-null int64\n",
      "cheap.brands            741268 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741268 non-null int64\n",
      "jordan.name             741268 non-null int64\n",
      "iphon.name              741268 non-null int64\n",
      "bundl.name              741268 non-null int64\n",
      "cap.letter.brand        741268 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "subtrain = subtrain.dropna()\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(subtrain.price + 1)\n",
    "X = subtrain.drop([\"price\"],axis = 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.5185 - val_loss: 0.4680\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4695 - val_loss: 0.4894\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4674 - val_loss: 0.4629\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4664 - val_loss: 0.4629\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4658 - val_loss: 0.4662\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4654 - val_loss: 0.4617\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4650 - val_loss: 0.4623\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4649 - val_loss: 0.4617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6794526310377843"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_4 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_4 = np.sqrt(min(model_4.history[\"val_loss\"]))\n",
    "RMSE_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.5029 - val_loss: 0.4714\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4684 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4669 - val_loss: 0.4626\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4658 - val_loss: 0.4621\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4655 - val_loss: 0.4626\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4646 - val_loss: 0.4618\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4644 - val_loss: 0.4605\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4641 - val_loss: 0.4640\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4639 - val_loss: 0.4614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67862169258002358"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_5 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_5 = np.sqrt(min(model_5.history[\"val_loss\"]))\n",
    "RMSE_model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4934 - val_loss: 0.4676\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4687 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4662 - val_loss: 0.4616\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4653 - val_loss: 0.4616\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4644 - val_loss: 0.4615\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4638 - val_loss: 0.4613\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4634 - val_loss: 0.4608\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4631 - val_loss: 0.4618\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4629 - val_loss: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67883387278347673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_6 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_6 = np.sqrt(min(model_6.history[\"val_loss\"]))\n",
    "RMSE_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4880 - val_loss: 0.4679\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4714 - val_loss: 0.4882\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4688 - val_loss: 0.4665\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4675 - val_loss: 0.4698\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4666 - val_loss: 0.4640\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 63us/step - loss: 0.4676 - val_loss: 0.4640\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 33s 65us/step - loss: 0.4661 - val_loss: 0.4626\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4653 - val_loss: 0.4641\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 30s 58us/step - loss: 0.4647 - val_loss: 0.4636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68016471861743943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing optimizer to SGD\n",
    "sgd_optimizer = SGD(lr = 0.01)\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= sgd_optimizer, loss= \"mean_squared_error\")\n",
    "model_7 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_7 = np.sqrt(min(model_7.history[\"val_loss\"]))\n",
    "RMSE_model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/20\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4953 - val_loss: 0.4869\n",
      "Epoch 2/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4685 - val_loss: 0.4672\n",
      "Epoch 3/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4669 - val_loss: 0.4635\n",
      "Epoch 4/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4659 - val_loss: 0.4623\n",
      "Epoch 5/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4668\n",
      "Epoch 6/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4623\n",
      "Epoch 7/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4645 - val_loss: 0.4626\n",
      "Epoch 8/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4645 - val_loss: 0.4622\n",
      "Epoch 9/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4639 - val_loss: 0.4612\n",
      "Epoch 10/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4658 - val_loss: 0.4638\n",
      "Epoch 11/20\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4636 - val_loss: 0.4611\n",
      "Epoch 12/20\n",
      "518887/518887 [==============================] - 40s 78us/step - loss: 0.4634 - val_loss: 0.4606\n",
      "Epoch 13/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4632 - val_loss: 0.4617\n",
      "Epoch 14/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4632 - val_loss: 0.4612\n",
      "Epoch 15/20\n",
      "518887/518887 [==============================] - 36s 70us/step - loss: 0.4631 - val_loss: 0.4602\n",
      "Epoch 16/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4629 - val_loss: 0.4640\n",
      "Epoch 17/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4628 - val_loss: 0.4610\n",
      "Epoch 18/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4629 - val_loss: 0.4642\n",
      "Epoch 19/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4625 - val_loss: 0.4607\n",
      "Epoch 20/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4624 - val_loss: 0.4647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67839322279547709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing fixed 20 epochs\n",
    "#estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_8 = model.fit(X,y, epochs= 20, validation_split= 0.3)\n",
    "RMSE_model_8 = np.sqrt(min(model_8.history[\"val_loss\"]))\n",
    "RMSE_model_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.5663 - val_loss: 0.4738\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4740 - val_loss: 0.4702\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4735 - val_loss: 0.4721\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 32s 62us/step - loss: 0.4726 - val_loss: 0.4706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68569114759373795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing final activation function\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_9 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_9 = np.sqrt(min(model_9.history[\"val_loss\"]))\n",
    "RMSE_model_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to learn other means of model optimization and try to test here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the small data set for training once again:\n",
    "os.listdir()\n",
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis= 1).values\n",
    "y = np.log(df.price+1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # This is a Keras wrapper for sklearn we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt I loaded some sklearn functions to use them in model evaluation, as well as a Keras wrapper for sklearn.\n",
    "\n",
    "In this case, the Keras wrappers we will use take a function as argument, which we will define to create the neural network model structure.\n",
    "\n",
    "Let's define the basic_model function:\n",
    "\n",
    "- We use a sequential model structure.\n",
    "- We will have a simple 2-layered model (input and output layer).\n",
    "- We don't use any activation function at the output layer (since we are performing regression, we want values to be evaluated without transformation). \n",
    "- We will use ADAM optimization function and optimize mean squared error as our loss function\n",
    "- We will use the same number of neurons(nodes) as the the number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    \"\"\"Creates, compiles and returns the basic NN\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "    model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define our regression estimator using keras wrapper KerasRegressor we imported above. This estimator function will receive:\n",
    "\n",
    "- the function which creates the NN model\n",
    "- parameters we normally enter in .fit() function (e.g: number of epochs or batch size), note that in this case we are doing the computation in batch mode, so we use nb_epoch argument\n",
    "\n",
    "we will also need to set the random number generator seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4\n",
    "np.random.seed(seed)\n",
    "regression_estimator = KerasRegressor(build_fn= basic_model, \n",
    "                                     nb_epoch = 100,\n",
    "                                     batch_size = 5,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the model using 10-fold cross-validation. Note that this is the actual step in which we are fitting the model on the data. We are collecting the model evaluation in a object we called **results**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c75cb95c7d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set of the k-folds and random state to partition the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregression_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits= 10, random_state= seed) # Set of the k-folds and random state to partition the data\n",
    "results = cross_val_score(estimator=regression_estimator, X = X,y = y,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results) # Note that results is an array that contains MSE scores for each k-fold cv step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4844004 , -0.46776465, -0.49775278, -0.4527221 , -0.46104527,\n",
       "       -0.48631333, -0.46733199, -0.45774577, -0.47870872, -0.46569664])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47194816461575667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mse returned by CV function is negative! This is quite counter intuitive and confusing. Checking the documentation in GitHub, I understand that this is not a useful way of estimating model performance, perhaps the wrapper has problems.\n",
    "\n",
    "I am going back to the original way I fit the models, that is using a single split for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 2.9459 - val_loss: 1.0557\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.9681 - val_loss: 0.8441\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.7625 - val_loss: 0.6377\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.5757 - val_loss: 0.4834\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4858 - val_loss: 0.4520\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4749 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4741 - val_loss: 0.4503\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4745 - val_loss: 0.4501\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4741 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4746 - val_loss: 0.4512\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4745 - val_loss: 0.4561\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4741 - val_loss: 0.4511\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4739 - val_loss: 0.4508\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4743 - val_loss: 0.4525\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4743 - val_loss: 0.4533\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4728 - val_loss: 0.4498\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4720 - val_loss: 0.4519\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4708 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4702 - val_loss: 0.4506\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4692 - val_loss: 0.4498\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4468\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4690 - val_loss: 0.4468\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4689 - val_loss: 0.4471\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4681 - val_loss: 0.4479\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4686 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4471\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4682 - val_loss: 0.4494\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4681 - val_loss: 0.4471\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4680 - val_loss: 0.4492\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4675 - val_loss: 0.4616\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4678 - val_loss: 0.4472\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4675 - val_loss: 0.4474\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4670 - val_loss: 0.4472\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4675 - val_loss: 0.4467\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4468\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4669 - val_loss: 0.4470\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4667 - val_loss: 0.4460\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4673 - val_loss: 0.4503\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4671 - val_loss: 0.4466\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4674 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4666 - val_loss: 0.4492\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4481\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4662 - val_loss: 0.4459\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4670 - val_loss: 0.4470\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4652 - val_loss: 0.4564\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4665 - val_loss: 0.4498\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4660 - val_loss: 0.4462\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4659 - val_loss: 0.4465\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4658 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4661 - val_loss: 0.4469\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4652 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4655 - val_loss: 0.4479\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4650 - val_loss: 0.4469\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4651 - val_loss: 0.4470\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4644 - val_loss: 0.4502\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4650 - val_loss: 0.4466\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4497\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4647 - val_loss: 0.4468\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4489\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4487\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4466\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4653 - val_loss: 0.4474\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4537\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4650 - val_loss: 0.4473\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4649 - val_loss: 0.4471\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4482\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4646 - val_loss: 0.4480\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4645 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4476\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4645 - val_loss: 0.4470\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4483\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4638 - val_loss: 0.4478\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4521\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4476\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4644 - val_loss: 0.4467\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4480\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4646 - val_loss: 0.4481\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4639 - val_loss: 0.4499\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4642 - val_loss: 0.4497\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4640 - val_loss: 0.4472\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4471\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4638 - val_loss: 0.4498\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4475\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4479\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4638 - val_loss: 0.4484\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4570\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4645 - val_loss: 0.4483\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4641 - val_loss: 0.4486\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4641 - val_loss: 0.4474\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4632 - val_loss: 0.4484\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_10 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66776206455463483"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_10 = np.sqrt(min(model_10.history[\"val_loss\"]))\n",
    "RMSE_model_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight improvement over model_1. Let's continue by trying to tune model topology. \n",
    "\n",
    "We can try a more complex network, hoping that this network will extract and recombine more interactions between the available features.\n",
    "\n",
    "- We can set up deeper NNs : i.e: adding layers\n",
    "- We can set up wider NNs: i.e: adding neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 3.2844 - val_loss: 0.9369\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.8241 - val_loss: 0.6616\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5722 - val_loss: 0.4716\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4792 - val_loss: 0.4520\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4741 - val_loss: 0.4498\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4733 - val_loss: 0.4498\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4727 - val_loss: 0.4500\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4733 - val_loss: 0.4486\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4723 - val_loss: 0.4529\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4722 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4724 - val_loss: 0.4530\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4716 - val_loss: 0.4483\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4709 - val_loss: 0.4479\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4558\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4717 - val_loss: 0.4516\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4716 - val_loss: 0.4486\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4713 - val_loss: 0.4526\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4709 - val_loss: 0.4531\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4710 - val_loss: 0.4530\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4480\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4707 - val_loss: 0.4535\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4474\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4476\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4472\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4484\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4708 - val_loss: 0.4478\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4479\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4703 - val_loss: 0.4485\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4700 - val_loss: 0.4495\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4694 - val_loss: 0.4613\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4701 - val_loss: 0.4483\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4690 - val_loss: 0.4471\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4489\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4688 - val_loss: 0.4491\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4477\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4693 - val_loss: 0.4480\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4693 - val_loss: 0.4504\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4694 - val_loss: 0.4470\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4512\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4691 - val_loss: 0.4479\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4483\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4487\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4488\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4471\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4679 - val_loss: 0.4591\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4488\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4688 - val_loss: 0.4475\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4686 - val_loss: 0.4503\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4480\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4677 - val_loss: 0.4486\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4667 - val_loss: 0.4473\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4675 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4660 - val_loss: 0.4477\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4663 - val_loss: 0.4460\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4659 - val_loss: 0.4479\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4654 - val_loss: 0.4449\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4647 - val_loss: 0.4470\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4650 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4461\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4448\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4443\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4632 - val_loss: 0.4518\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4632 - val_loss: 0.4435\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4628 - val_loss: 0.4432\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4623 - val_loss: 0.4445\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4625 - val_loss: 0.4447\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4623 - val_loss: 0.4444\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4622 - val_loss: 0.4436\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4616 - val_loss: 0.4437\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.458 - 1s 32us/step - loss: 0.4616 - val_loss: 0.4450\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4608 - val_loss: 0.4436\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4481\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4609 - val_loss: 0.4447\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4616 - val_loss: 0.4434\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4436\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4455\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4603 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4607 - val_loss: 0.4449\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4602 - val_loss: 0.4436\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4430\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4457\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4609 - val_loss: 0.4438\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4608 - val_loss: 0.4434\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4454\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4601 - val_loss: 0.4586\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4607 - val_loss: 0.4440\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4442\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4599 - val_loss: 0.4439\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4593 - val_loss: 0.4441\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(12, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_11 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66555347845455159"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_11 = np.sqrt(min(model_11.history[\"val_loss\"]))\n",
    "RMSE_model_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a wider network to evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 89us/step - loss: 1.2000 - val_loss: 0.4537\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4759 - val_loss: 0.4514\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4698 - val_loss: 0.4512\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4702 - val_loss: 0.4507\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4654 - val_loss: 0.4462\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4645 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4642 - val_loss: 0.4491\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4646 - val_loss: 0.4495\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4639 - val_loss: 0.4569\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4613 - val_loss: 0.4495\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4597 - val_loss: 0.4499\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4608 - val_loss: 0.4474\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4571 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4570 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4575 - val_loss: 0.4564\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4563 - val_loss: 0.4487\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4567 - val_loss: 0.4704\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4553 - val_loss: 0.4559\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4543 - val_loss: 0.4502\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4536 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4541 - val_loss: 0.4596\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4518 - val_loss: 0.4702\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4522 - val_loss: 0.4524\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4499 - val_loss: 0.4516\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4540 - val_loss: 0.4513\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4498 - val_loss: 0.4483\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4487 - val_loss: 0.4527\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4475 - val_loss: 0.4543\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4480 - val_loss: 0.4518\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4478 - val_loss: 0.4599\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4466 - val_loss: 0.4541\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4460 - val_loss: 0.4571\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4470 - val_loss: 0.4553\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4450 - val_loss: 0.4566\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4466 - val_loss: 0.4526\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4438 - val_loss: 0.4510\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4436 - val_loss: 0.4556\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4425 - val_loss: 0.4542\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4425 - val_loss: 0.4563\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4425 - val_loss: 0.4554\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4423 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4394 - val_loss: 0.4536\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4398 - val_loss: 0.4538\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4401 - val_loss: 0.4545\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4404 - val_loss: 0.4568\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4388 - val_loss: 0.4591\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4384 - val_loss: 0.4603\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4390 - val_loss: 0.4563\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4381 - val_loss: 0.4656\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4367 - val_loss: 0.4635\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4367 - val_loss: 0.4556\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4352 - val_loss: 0.4564\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4398 - val_loss: 0.4584\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4350 - val_loss: 0.4586\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 2s 62us/step - loss: 0.4355 - val_loss: 0.4676\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4363 - val_loss: 0.4846\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4345 - val_loss: 0.4665\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4344 - val_loss: 0.4592\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4322 - val_loss: 0.4600\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4330 - val_loss: 0.4574\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4346 - val_loss: 0.4573\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4321 - val_loss: 0.4560\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4310 - val_loss: 0.4589\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4312 - val_loss: 0.4641\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4299 - val_loss: 0.4615\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4292 - val_loss: 0.4619\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4315 - val_loss: 0.4575\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4292 - val_loss: 0.4591\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4296 - val_loss: 0.4605\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4276 - val_loss: 0.4631\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4274 - val_loss: 0.4616\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4281 - val_loss: 0.4620\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4276 - val_loss: 0.4610\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4301 - val_loss: 0.4593\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4273 - val_loss: 0.4620\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4265 - val_loss: 0.4665\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4262 - val_loss: 0.4634\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4255 - val_loss: 0.4630\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4266 - val_loss: 0.4648\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4243 - val_loss: 0.4786\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4256 - val_loss: 0.4656\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4242 - val_loss: 0.4633\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4230 - val_loss: 0.4642\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4244 - val_loss: 0.4685\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4235 - val_loss: 0.4653\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.421 - 1s 52us/step - loss: 0.4229 - val_loss: 0.4667\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4215 - val_loss: 0.4664\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4212 - val_loss: 0.4697\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4218 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4204 - val_loss: 0.4637\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4224 - val_loss: 0.4678\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4214 - val_loss: 0.4690\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4213 - val_loss: 0.4697\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4206 - val_loss: 0.4886\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4210 - val_loss: 0.4723\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4187 - val_loss: 0.4833\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4196 - val_loss: 0.4683\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4193 - val_loss: 0.4724\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4176 - val_loss: 0.4672\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4178 - val_loss: 0.4779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66800683261370652"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1] * 10, input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(120, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_12 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_12 = np.sqrt(min(model_12.history[\"val_loss\"]))\n",
    "RMSE_model_12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really not any better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can scale some of the predictors to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find which columns do not have values restricted between [0,1]\n",
    "\n",
    "select_col = (df.describe().loc[[\"max\",\"min\"],:].apply(sum,axis = 0)) != 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_condition_id',\n",
       " 'log.excl.description',\n",
       " 'excl.name',\n",
       " 'dollar.description',\n",
       " 'sale',\n",
       " 'free',\n",
       " 'save',\n",
       " 'deal',\n",
       " 'good',\n",
       " 'now',\n",
       " 'cheap',\n",
       " 'buy',\n",
       " 'excellent',\n",
       " 'great',\n",
       " 'cap.letter.brand']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_features =  df.columns[select_col].tolist() # Select the relevant columns except target\n",
    "select_features.remove(\"price\")\n",
    "select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (making features resemble z-distribution)\n",
    "stdscale = preprocessing.StandardScaler().fit(df[select_features]) # Create a preprocessing object on select features\n",
    "df_stdscale = stdscale.transform(df[select_features]) #Note that it returns a np.array with only used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.027829e-15\n",
       "1    -2.288079e-16\n",
       "2     5.450259e-15\n",
       "3     3.437801e-15\n",
       "4     8.985525e-16\n",
       "5     1.658329e-15\n",
       "6     4.924806e-17\n",
       "7     1.064549e-15\n",
       "8    -1.118217e-15\n",
       "9    -1.466933e-15\n",
       "10    1.694972e-15\n",
       "11    1.282229e-15\n",
       "12    1.338746e-16\n",
       "13   -6.921656e-16\n",
       "14   -1.552518e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_stdscale).apply(np.mean,axis = 0) # As we expected means are almost zero after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.0\n",
       "1     1.0\n",
       "2     1.0\n",
       "3     1.0\n",
       "4     1.0\n",
       "5     1.0\n",
       "6     1.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     1.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_stdscale).apply(np.std,axis = 0) # # As we expected standard deviations are 1 after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.0\n",
       "1     1.0\n",
       "2     1.0\n",
       "3     1.0\n",
       "4     1.0\n",
       "5     1.0\n",
       "6     1.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     1.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min-max scaling (making features to stay in 0-1 boundaries)\n",
    "from sklearn import preprocessing\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[select_features]) # Create the processing object\n",
    "df_minmax = minmax_scale.transform(df[select_features])\n",
    "pd.DataFrame(df_minmax).apply(max,axis =0) # As we expected, max is all 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_minmax).apply(min, axis = 0) # As we expected min is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>cheap.brands</th>\n",
       "      <th>steal</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.032003</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price      shipping  no.brand_name  fancy.categories  \\\n",
       "count  37066.000000  37066.000000   37066.000000      37066.000000   \n",
       "mean      26.581045      0.449630       0.428776          0.003480   \n",
       "std       36.188265      0.497463       0.494908          0.058892   \n",
       "min        0.000000      0.000000       0.000000          0.000000   \n",
       "25%       10.000000      0.000000       0.000000          0.000000   \n",
       "50%       17.000000      0.000000       0.000000          0.000000   \n",
       "75%       29.000000      1.000000       1.000000          0.000000   \n",
       "max     1106.000000      1.000000       1.000000          1.000000   \n",
       "\n",
       "       cheap.categories  fancy.brands  cheap.brands         steal  \\\n",
       "count      37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean           0.005126      0.009820      0.001106      0.001025   \n",
       "std            0.071413      0.098611      0.033241      0.032003   \n",
       "min            0.000000      0.000000      0.000000      0.000000   \n",
       "25%            0.000000      0.000000      0.000000      0.000000   \n",
       "50%            0.000000      0.000000      0.000000      0.000000   \n",
       "75%            0.000000      0.000000      0.000000      0.000000   \n",
       "max            1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000  \n",
       "mean        0.008229      0.008013      0.020774      0.051395  \n",
       "std         0.090339      0.089156      0.142628      0.220805  \n",
       "min         0.000000      0.000000      0.000000      0.000000  \n",
       "25%         0.000000      0.000000      0.000000      0.000000  \n",
       "50%         0.000000      0.000000      0.000000      0.000000  \n",
       "75%         0.000000      0.000000      0.000000      0.000000  \n",
       "max         1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine with other features to obtain final data sets:\n",
    "binary_features = df.drop(select_features, axis = 1)\n",
    "binary_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the standardized and scaled data framesfeatures with the binary features to give two data sets to be tested.\n",
    "binary_features = binary_features.drop(\"price\", axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 11)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_minmax = np.concatenate((binary_features,df_minmax), axis = 1)\n",
    "X_minmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stdscale = np.concatenate((binary_features,df_stdscale), axis = 1)\n",
    "X_stdscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqWOGgCR1zBCQpI4ZApLUsf8F3JGeuzUDOTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121ddacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.price.hist(bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nApLUMENAkhr2v6gh0OtofkY1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cbe67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(df.price+1), bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(df.price+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have standardized and scaled data sets ready and we can re-try training our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 4.2032 - val_loss: 1.2055\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.9293 - val_loss: 0.6881\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.6206 - val_loss: 0.5300\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5228 - val_loss: 0.4773\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4916 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4812 - val_loss: 0.4550\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4764 - val_loss: 0.4514\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4746 - val_loss: 0.4505\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4737 - val_loss: 0.4519\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4731 - val_loss: 0.4505\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4715 - val_loss: 0.4504\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4710 - val_loss: 0.4485\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4703 - val_loss: 0.4511\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4702 - val_loss: 0.4479\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4698 - val_loss: 0.4474\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4687 - val_loss: 0.4477\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4689 - val_loss: 0.4481\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4480\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4673 - val_loss: 0.4472\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4671 - val_loss: 0.4470\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4666 - val_loss: 0.4469\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4659 - val_loss: 0.4457\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4469\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4655 - val_loss: 0.4459\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4654 - val_loss: 0.4482\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4648 - val_loss: 0.4454\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4647 - val_loss: 0.4460\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4643 - val_loss: 0.4455\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4645 - val_loss: 0.4450\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4639 - val_loss: 0.4460\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4639 - val_loss: 0.4455\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4635 - val_loss: 0.4481\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4638 - val_loss: 0.4462\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4631 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4630 - val_loss: 0.4459\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4633 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4628 - val_loss: 0.4457\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4631 - val_loss: 0.4457\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4629 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4626 - val_loss: 0.4456\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4627 - val_loss: 0.4456\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4625 - val_loss: 0.4462\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4625 - val_loss: 0.4459\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4632 - val_loss: 0.4464\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4628 - val_loss: 0.4502\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4622 - val_loss: 0.4500\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4625 - val_loss: 0.4464\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4622 - val_loss: 0.4461\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4621 - val_loss: 0.4469\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4621 - val_loss: 0.4464\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4619 - val_loss: 0.4461\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4468\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4616 - val_loss: 0.4463\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4616 - val_loss: 0.4483\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4612 - val_loss: 0.4514\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4618 - val_loss: 0.4464\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4614 - val_loss: 0.4467\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4614 - val_loss: 0.4473\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4615 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4613 - val_loss: 0.4472\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4617 - val_loss: 0.4466\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4614 - val_loss: 0.4488\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4473\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4489\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4614 - val_loss: 0.4464\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4607 - val_loss: 0.4467\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4609 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4607 - val_loss: 0.4478\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4468\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4615 - val_loss: 0.4469\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4608 - val_loss: 0.4486\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4611 - val_loss: 0.4469\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4469\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4467\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4473\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4608 - val_loss: 0.4471\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4609 - val_loss: 0.4470\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4482\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4472\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4606 - val_loss: 0.4467\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4503\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4465\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4464\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4603 - val_loss: 0.4472\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4472\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4469\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4473\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4461\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4467\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4604 - val_loss: 0.4474\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4472\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4485\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4465\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4483\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4612 - val_loss: 0.4470\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4609 - val_loss: 0.4476\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4470\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4602 - val_loss: 0.4479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66694540986076201"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with X_minmax (scaled features)\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_13 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_13 = np.sqrt(min(model_13.history[\"val_loss\"]))\n",
    "RMSE_model_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was promising! Minmax scaling seems to help for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 3.9986 - val_loss: 1.0113\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.8118 - val_loss: 0.6163\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.5915 - val_loss: 0.5279\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5431 - val_loss: 0.5095\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5262 - val_loss: 0.5001\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5154 - val_loss: 0.4930\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5084 - val_loss: 0.4876\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5021 - val_loss: 0.4829\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4964 - val_loss: 0.4787\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4914 - val_loss: 0.4772\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4871 - val_loss: 0.4721\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4835 - val_loss: 0.4678\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4798 - val_loss: 0.4691\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4779 - val_loss: 0.4633\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4754 - val_loss: 0.4606\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4736 - val_loss: 0.4599\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4711 - val_loss: 0.4604\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4692 - val_loss: 0.4602\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4685 - val_loss: 0.4587\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4560\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4662 - val_loss: 0.4590\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4653 - val_loss: 0.4562\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4650 - val_loss: 0.4571\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4644 - val_loss: 0.4567\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4645 - val_loss: 0.4572\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4634 - val_loss: 0.4548\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4632 - val_loss: 0.4557\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4627 - val_loss: 0.4551\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4624 - val_loss: 0.4555\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4619 - val_loss: 0.4558\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4613 - val_loss: 0.4551\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4559\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4616 - val_loss: 0.4551\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4607 - val_loss: 0.4532\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4605 - val_loss: 0.4542\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4600 - val_loss: 0.4544\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4558\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4602 - val_loss: 0.4532\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4598 - val_loss: 0.4548\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4592 - val_loss: 0.4532\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4590 - val_loss: 0.4532\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4592 - val_loss: 0.4530\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4592 - val_loss: 0.4529\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4589 - val_loss: 0.4566\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4585 - val_loss: 0.4553\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4583 - val_loss: 0.4556\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4581 - val_loss: 0.4543\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4586 - val_loss: 0.4525\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4583 - val_loss: 0.4547\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4579 - val_loss: 0.4544\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4576 - val_loss: 0.4530\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4572 - val_loss: 0.4539\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4574 - val_loss: 0.4539\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4574 - val_loss: 0.4575\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4580 - val_loss: 0.4535\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4569 - val_loss: 0.4520\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4570 - val_loss: 0.4546\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4573 - val_loss: 0.4530\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4569 - val_loss: 0.4528\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4571 - val_loss: 0.4520\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4572 - val_loss: 0.4544\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4564 - val_loss: 0.4540\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4566 - val_loss: 0.4558\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4569 - val_loss: 0.4532\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4557 - val_loss: 0.4553\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4564 - val_loss: 0.4538\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4558 - val_loss: 0.4561\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4564 - val_loss: 0.4526\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4568 - val_loss: 0.4536\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4557 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4559 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4563 - val_loss: 0.4523\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4558 - val_loss: 0.4521\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4563 - val_loss: 0.4519\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4562 - val_loss: 0.4534\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4539\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4557 - val_loss: 0.4520\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4556 - val_loss: 0.4539\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4554 - val_loss: 0.4533\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4556 - val_loss: 0.4541\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4551 - val_loss: 0.4520\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4556 - val_loss: 0.4545\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4554 - val_loss: 0.4538\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4526\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4556 - val_loss: 0.4548\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4551 - val_loss: 0.4521\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4553 - val_loss: 0.4536\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4549 - val_loss: 0.4529\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4553 - val_loss: 0.4525\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4548 - val_loss: 0.4538\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4556 - val_loss: 0.4539\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4551 - val_loss: 0.4542\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4553 - val_loss: 0.4525\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4547 - val_loss: 0.4525\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4553 - val_loss: 0.4534\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4553 - val_loss: 0.4544\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4550 - val_loss: 0.4524\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4546 - val_loss: 0.4557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6722402842940296"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_stdscale.shape[1], input_dim = X_stdscale.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_14 = model.fit(X_stdscale,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_14 = np.sqrt(min(model_14.history[\"val_loss\"]))\n",
    "RMSE_model_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 3.0712 - val_loss: 0.5992\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.5269 - val_loss: 0.4636\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4811 - val_loss: 0.4525\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4736 - val_loss: 0.4504\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4717 - val_loss: 0.4485\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4704 - val_loss: 0.4486\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4480\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4693 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4683 - val_loss: 0.4526\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4681 - val_loss: 0.4488\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4672 - val_loss: 0.4514\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4664 - val_loss: 0.4469\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4656 - val_loss: 0.4460\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4656 - val_loss: 0.4515\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4660 - val_loss: 0.4515\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4654 - val_loss: 0.4478\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4655 - val_loss: 0.4491\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4518\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4503\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4649 - val_loss: 0.4473\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4643 - val_loss: 0.4505\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4642 - val_loss: 0.4460\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4643 - val_loss: 0.4465\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4642 - val_loss: 0.4467\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4644 - val_loss: 0.4464\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4486\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4643 - val_loss: 0.4468\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4635 - val_loss: 0.4463\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4637 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4640 - val_loss: 0.4501\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4635 - val_loss: 0.4587\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4639 - val_loss: 0.4474\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4630 - val_loss: 0.4457\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4637 - val_loss: 0.4465\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4630 - val_loss: 0.4479\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4634 - val_loss: 0.4456\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4631 - val_loss: 0.4472\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4632 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4631 - val_loss: 0.4458\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4488\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4628 - val_loss: 0.4463\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4632 - val_loss: 0.4470\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4637 - val_loss: 0.4466\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4632 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4623 - val_loss: 0.4476\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4633 - val_loss: 0.4450\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4626 - val_loss: 0.4445\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4627 - val_loss: 0.4465\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4615 - val_loss: 0.4548\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4622 - val_loss: 0.4470\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4621 - val_loss: 0.4449\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4627 - val_loss: 0.4453\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4617 - val_loss: 0.4466\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4614 - val_loss: 0.4495\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4625 - val_loss: 0.4453\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4610 - val_loss: 0.4453\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4613 - val_loss: 0.4459\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4610 - val_loss: 0.4444\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4609 - val_loss: 0.4469\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4448\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4604 - val_loss: 0.4505\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4609 - val_loss: 0.4450\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4603 - val_loss: 0.4489\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4604 - val_loss: 0.4450\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4598 - val_loss: 0.4448\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4603 - val_loss: 0.4453\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4451\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4600 - val_loss: 0.4446\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4611 - val_loss: 0.4446\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4598 - val_loss: 0.4451\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4511\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4601 - val_loss: 0.4446\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4596 - val_loss: 0.4450\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4595 - val_loss: 0.4454\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4597 - val_loss: 0.4443\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4591 - val_loss: 0.4455\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4454\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4587 - val_loss: 0.4446\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4591 - val_loss: 0.4494\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4590 - val_loss: 0.4451\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4591 - val_loss: 0.4445\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4589 - val_loss: 0.4440\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4595 - val_loss: 0.4471\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4589 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4590 - val_loss: 0.4471\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4586 - val_loss: 0.4442\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4590 - val_loss: 0.4447\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4587 - val_loss: 0.4461\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4592 - val_loss: 0.4447\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4588 - val_loss: 0.4450\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4585 - val_loss: 0.4487\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4590 - val_loss: 0.4550\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4594 - val_loss: 0.4445\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4588 - val_loss: 0.4443\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4587 - val_loss: 0.4446\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4579 - val_loss: 0.4449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66633644156462402"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue with X_minmax (scaled features), slighly increasing model complexity\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_15 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_15 = np.sqrt(min(model_15.history[\"val_loss\"]))\n",
    "RMSE_model_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's getting better! Continue increasing model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 2.3449 - val_loss: 0.4819\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4852 - val_loss: 0.4507\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4734 - val_loss: 0.4493\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4706 - val_loss: 0.4488\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4693 - val_loss: 0.4465\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4685 - val_loss: 0.4479\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4679 - val_loss: 0.4522\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4696 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4684 - val_loss: 0.4489\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4678 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4673 - val_loss: 0.4473\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4664 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4654 - val_loss: 0.4448\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4647 - val_loss: 0.4547\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4659 - val_loss: 0.4490\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4655 - val_loss: 0.4485\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4649 - val_loss: 0.4549\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4640 - val_loss: 0.4557\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4463\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4635 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4458\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4636 - val_loss: 0.4456\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4629 - val_loss: 0.4445\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4636 - val_loss: 0.4460\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4624 - val_loss: 0.4521\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4638 - val_loss: 0.4456\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4629 - val_loss: 0.4463\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4620 - val_loss: 0.4445\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4628 - val_loss: 0.4490\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4631 - val_loss: 0.4461\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4616 - val_loss: 0.4549\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4624 - val_loss: 0.4479\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4614 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4624 - val_loss: 0.4458\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4606 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4613 - val_loss: 0.4456\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4608 - val_loss: 0.4482\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4615 - val_loss: 0.4507\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4453\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4630 - val_loss: 0.4455\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4614 - val_loss: 0.4457\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4614 - val_loss: 0.4461\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4604 - val_loss: 0.4473\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4616 - val_loss: 0.4464\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4610 - val_loss: 0.4457\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4613 - val_loss: 0.4498\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4598 - val_loss: 0.4597\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4458\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4610 - val_loss: 0.4456\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4619 - val_loss: 0.4475\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4489\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4601 - val_loss: 0.4542\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4616 - val_loss: 0.4455\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4597 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4598 - val_loss: 0.4462\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4594 - val_loss: 0.4444\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4595 - val_loss: 0.4457\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4606 - val_loss: 0.4458\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4591 - val_loss: 0.4462\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4598 - val_loss: 0.4465\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4592 - val_loss: 0.4494\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4588 - val_loss: 0.4452\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4585 - val_loss: 0.4455\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4592 - val_loss: 0.4472\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4586 - val_loss: 0.4453\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4592 - val_loss: 0.4474\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4601 - val_loss: 0.4449\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4588 - val_loss: 0.4467\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4592 - val_loss: 0.4530\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4587 - val_loss: 0.4454\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4592 - val_loss: 0.4453\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4589 - val_loss: 0.4449\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4585 - val_loss: 0.4474\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4590 - val_loss: 0.4467\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4581 - val_loss: 0.4469\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4585 - val_loss: 0.4459\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4572 - val_loss: 0.4468\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4577 - val_loss: 0.4467\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4567 - val_loss: 0.4480\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4571 - val_loss: 0.4497\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4567 - val_loss: 0.4457\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4570 - val_loss: 0.4463\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4564 - val_loss: 0.4444\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4569 - val_loss: 0.4495\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4559 - val_loss: 0.4525\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4561 - val_loss: 0.4457\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4554 - val_loss: 0.4455\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4565 - val_loss: 0.4453\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4556 - val_loss: 0.4455\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4560 - val_loss: 0.4454\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4559 - val_loss: 0.4454\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4497\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4549 - val_loss: 0.4664\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4563 - val_loss: 0.4449\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4553 - val_loss: 0.4458\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4549 - val_loss: 0.4452\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4540 - val_loss: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6665976297688434"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1]*2, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1]*2, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_16 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_16 = np.sqrt(min(model_16.history[\"val_loss\"]))\n",
    "RMSE_model_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we hit the model capacity again. Let's try another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 2.5496 - val_loss: 0.4932\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4890 - val_loss: 0.4526\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4753 - val_loss: 0.4499\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4718 - val_loss: 0.4510\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4706 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4698 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4574\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4723 - val_loss: 0.4477\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4706 - val_loss: 0.4511\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4702 - val_loss: 0.4524\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4692 - val_loss: 0.4484\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4696 - val_loss: 0.4471\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4682 - val_loss: 0.4503\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4677 - val_loss: 0.4478\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4688 - val_loss: 0.4489\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4686 - val_loss: 0.4557\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4679 - val_loss: 0.4640\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4671 - val_loss: 0.4495\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4538\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4677 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4666 - val_loss: 0.4483\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4673 - val_loss: 0.4470\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4667 - val_loss: 0.4466\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4656 - val_loss: 0.4465\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4672 - val_loss: 0.4466\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4650 - val_loss: 0.4539\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4668 - val_loss: 0.4496\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4661 - val_loss: 0.4482\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4647 - val_loss: 0.4464\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4657 - val_loss: 0.4503\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4660 - val_loss: 0.4488\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4652 - val_loss: 0.4483\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4652 - val_loss: 0.4509\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4460\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4653 - val_loss: 0.4464\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4635 - val_loss: 0.4461\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4637 - val_loss: 0.4462\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4637 - val_loss: 0.4491\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4643 - val_loss: 0.4552\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4642 - val_loss: 0.4467\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4662 - val_loss: 0.4467\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4639 - val_loss: 0.4473\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4474\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4640 - val_loss: 0.4478\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4635 - val_loss: 0.4482\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4643 - val_loss: 0.4472\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4648 - val_loss: 0.4460\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4638 - val_loss: 0.4511\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4546\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4630 - val_loss: 0.4467\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4636 - val_loss: 0.4466\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4653 - val_loss: 0.4507\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4646 - val_loss: 0.4500\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4634 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4639 - val_loss: 0.4465\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4633 - val_loss: 0.4476\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4628 - val_loss: 0.4471\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4625 - val_loss: 0.4479\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4634 - val_loss: 0.4464\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4468\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4627 - val_loss: 0.4468\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4629 - val_loss: 0.4481\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4626 - val_loss: 0.4544\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4625 - val_loss: 0.4461\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4622 - val_loss: 0.4469\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4633 - val_loss: 0.4490\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4624 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4636 - val_loss: 0.4508\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4635 - val_loss: 0.4466\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4625 - val_loss: 0.4482\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4635 - val_loss: 0.4542\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4632 - val_loss: 0.4460\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4633 - val_loss: 0.4470\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4628 - val_loss: 0.4462\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4630 - val_loss: 0.4466\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4629 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4628 - val_loss: 0.4500\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4474\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4622 - val_loss: 0.4476\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4626 - val_loss: 0.4468\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4618 - val_loss: 0.4477\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4624 - val_loss: 0.4489\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4614 - val_loss: 0.4462\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4624 - val_loss: 0.4502\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4618 - val_loss: 0.4454\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4612 - val_loss: 0.4466\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4603 - val_loss: 0.4557\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4612 - val_loss: 0.4480\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4602 - val_loss: 0.4457\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4615 - val_loss: 0.4459\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4605 - val_loss: 0.4463\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4599 - val_loss: 0.4459\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4611 - val_loss: 0.4458\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4598 - val_loss: 0.4475\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4591 - val_loss: 0.4721\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4605 - val_loss: 0.4468\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4596 - val_loss: 0.4469\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4588 - val_loss: 0.4464\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4581 - val_loss: 0.4462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66741381955167611"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_17 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_17 = np.sqrt(min(model_17.history[\"val_loss\"]))\n",
    "RMSE_model_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 1.3357 - val_loss: 0.4499\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4709 - val_loss: 0.4489\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4696 - val_loss: 0.4492\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4716 - val_loss: 0.4545\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4681 - val_loss: 0.4481\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4665 - val_loss: 0.4571\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4684 - val_loss: 0.4472\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4702 - val_loss: 0.4586\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4663 - val_loss: 0.4493\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4666 - val_loss: 0.4503\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4670 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4648 - val_loss: 0.4453\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4637 - val_loss: 0.4496\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4643 - val_loss: 0.4520\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4639 - val_loss: 0.4495\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4644 - val_loss: 0.4689\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4627 - val_loss: 0.4515\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4631 - val_loss: 0.4470\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4641 - val_loss: 0.4456\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4627 - val_loss: 0.4581\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4609 - val_loss: 0.4554\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4633 - val_loss: 0.4516\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4595 - val_loss: 0.4474\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4639 - val_loss: 0.4477\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4596 - val_loss: 0.4458\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4607 - val_loss: 0.4491\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4604 - val_loss: 0.4477\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4594 - val_loss: 0.4449\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4598 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4605 - val_loss: 0.4538\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4592 - val_loss: 0.4472\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4591 - val_loss: 0.4486\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4588 - val_loss: 0.4531\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4610 - val_loss: 0.4452\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4571 - val_loss: 0.4456\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4585 - val_loss: 0.4447\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4573 - val_loss: 0.4509\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4581 - val_loss: 0.4503\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4581 - val_loss: 0.4455\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4574 - val_loss: 0.4470\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4560 - val_loss: 0.4481\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4570 - val_loss: 0.4469\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4566 - val_loss: 0.4486\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4574 - val_loss: 0.4468\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4560 - val_loss: 0.4487\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4557 - val_loss: 0.4502\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4572 - val_loss: 0.4465\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4552 - val_loss: 0.4555\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4550 - val_loss: 0.4492\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4545 - val_loss: 0.4504\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4535 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.456 - 1s 42us/step - loss: 0.4571 - val_loss: 0.4500\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4543 - val_loss: 0.4468\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4541 - val_loss: 0.4505\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4538 - val_loss: 0.4551\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4534 - val_loss: 0.4506\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4515 - val_loss: 0.4471\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4510 - val_loss: 0.4492\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4509 - val_loss: 0.4481\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4524 - val_loss: 0.4465\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4498 - val_loss: 0.4471\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4493 - val_loss: 0.4469\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4489 - val_loss: 0.4577\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4481 - val_loss: 0.4477\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4469 - val_loss: 0.4476\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4488 - val_loss: 0.4462\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4462 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4479 - val_loss: 0.4486\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4462 - val_loss: 0.4504\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4457 - val_loss: 0.4480\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4463 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4450 - val_loss: 0.4505\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4459 - val_loss: 0.4482\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4449 - val_loss: 0.4494\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4438 - val_loss: 0.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4440 - val_loss: 0.4499\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4429 - val_loss: 0.4549\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4441 - val_loss: 0.4535\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4420 - val_loss: 0.4642\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4423 - val_loss: 0.4532\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4420 - val_loss: 0.4545\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4416 - val_loss: 0.4517\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4412 - val_loss: 0.4532\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4408 - val_loss: 0.4520\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4411 - val_loss: 0.4519\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4401 - val_loss: 0.4539\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4391 - val_loss: 0.4602\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4393 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4380 - val_loss: 0.4531\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4392 - val_loss: 0.4546\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4398 - val_loss: 0.4539\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4384 - val_loss: 0.4544\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4391 - val_loss: 0.4694\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4378 - val_loss: 0.4526\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4368 - val_loss: 0.4836\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4382 - val_loss: 0.4552\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4371 - val_loss: 0.4560\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4356 - val_loss: 0.4530\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4351 - val_loss: 0.4589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66687018587000157"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(100, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(100, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_18 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_18 = np.sqrt(min(model_18.history[\"val_loss\"]))\n",
    "RMSE_model_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/10\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 1.0771 - val_loss: 0.4512\n",
      "Epoch 2/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4770 - val_loss: 0.4632\n",
      "Epoch 3/10\n",
      "25946/25946 [==============================] - 2s 69us/step - loss: 0.4751 - val_loss: 0.4510\n",
      "Epoch 4/10\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4734 - val_loss: 0.4515\n",
      "Epoch 5/10\n",
      "25946/25946 [==============================] - 2s 76us/step - loss: 0.4726 - val_loss: 0.4470\n",
      "Epoch 6/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4711 - val_loss: 0.4505\n",
      "Epoch 7/10\n",
      "25946/25946 [==============================] - 2s 69us/step - loss: 0.4704 - val_loss: 0.4613\n",
      "Epoch 8/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4699 - val_loss: 0.4468\n",
      "Epoch 9/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4692 - val_loss: 0.4544\n",
      "Epoch 10/10\n",
      "25946/25946 [==============================] - 2s 63us/step - loss: 0.4676 - val_loss: 0.4505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66841383692321843"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the batch and only performing 10 epochs\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1]*2, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1]*2, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_19 = model.fit(X_minmax,y,epochs = 10,validation_split= 0.3)\n",
    "RMSE_model_19 = np.sqrt(min(model_19.history[\"val_loss\"]))\n",
    "RMSE_model_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take home message\n",
    "\n",
    "For this data set, minmax scaling of the data helped only a little bit for training a better model. We will save this version of the predictors to continue model tuning as we learn different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_minmax).to_csv(\"mini_subtrain_X_minmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).to_csv(\"mini_subtrain_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking of alternative approaches\n",
    "\n",
    "Perhaps we can try to re-frame the problem, particularly focusing on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"mini_subtrain_y.csv\", index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12608f2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y back to original scale\n",
    "y_origin = np.exp(y)-1\n",
    "plt.hist(y_origin,bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1106.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clean = y_origin[np.logical_and(y_origin != 0, y_origin <400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAFJ6pghIEkdMwQkqWP/D+FEm3u3Q2W+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124ba5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_clean,bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we also scale the y before training the model in the same way we did for the predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "y = np.exp(y)-1 # We first get into the normal scale (price) from log.price\n",
    "minmax_scaler_y = MinMaxScaler().fit(y)\n",
    "y_minmax = minmax_scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEnZJREFUeJzt3X+s3fV93/Hna3ZDSTMSCHcWs+mut3rtDGrU4DGvraps\nroQTqppJKXLWFi9DoArWZdOk1nTS+GOy5GjT2qINJitkmC4KtWg2vFG6ImdZNnWGXZo0xlDKXQjB\nrsG3JAtbqtKZvPfH+Zgd38917s09x/fcaz8f0tH5nPf38/mez0fXuq/7/XGOU1VIkjTsz0x6ApKk\n1cdwkCR1DAdJUsdwkCR1DAdJUsdwkCR1DAdJUsdwkCR1DAdJUmf9pCewXFdffXVNT09PehqStKY8\n88wzf1RVU4v1W7PhMD09zczMzKSnIUlrSpKXl9LP00qSpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnq\nGA6SpI7hIEnqGA6SpM6a/YT0uEzvffzt9lf23zzBmUjS6uGRgySpYzhIkjqGgySpYzhIkjqGgySp\nYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqLhkOSTyY5neTZodo/S/L7\nSb6U5N8lec/QtnuSzCZ5IclNQ/Ubkhxr2+5Lkla/LMmvt/pTSabHu0RJ0ndqKUcODwE759WeBK6v\nqh8E/gC4ByDJVmA3cF0bc3+SdW3MA8AdwJb2OLvP24GvV9X3Ab8MfHy5i5Ekjcei4VBVnwe+Nq/2\n21V1pr08Cmxq7V3AI1X1ZlW9BMwCNya5Briiqo5WVQEPA7cMjTnY2o8CO84eVUiSJmMc1xz+LvBE\na28EXhnadqLVNrb2/Po5Y1rgfAN47xjmJUlappHCIck/Bs4AnxrPdBZ9vzuTzCSZmZubW4m3lKRL\n0rLDIcnfAX4C+Ol2qgjgJHDtULdNrXaS/3/qabh+zpgk64F3A68v9J5VdaCqtlXVtqmpqeVOXZK0\niGWFQ5KdwC8AP1lVfzy06TCwu92BtJnBheenq+oU8EaS7e16wm3AY0Nj9rT2h4HPDoWNJGkC1i/W\nIcmngQ8AVyc5AdzL4O6ky4An27Xjo1X1c1V1PMkh4DkGp5vurqq32q7uYnDn0+UMrlGcvU7xIPBr\nSWYZXPjePZ6lSZKWa9FwqKqPLFB+8Nv03wfsW6A+A1y/QP1PgJ9abB6SpJXjJ6QlSR3DQZLUMRwk\nSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3D\nQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSZ31k57AJEzvfXzSU5CkVc0jB0lSZ9FwSPLJJKeTPDtU\nuyrJk0lebM9XDm27J8lskheS3DRUvyHJsbbtviRp9cuS/HqrP5VkerxLlCR9p5Zy5PAQsHNebS9w\npKq2AEfaa5JsBXYD17Ux9ydZ18Y8ANwBbGmPs/u8Hfh6VX0f8MvAx5e7GEnSeCwaDlX1eeBr88q7\ngIOtfRC4Zaj+SFW9WVUvAbPAjUmuAa6oqqNVVcDD88ac3dejwI6zRxWSpMlY7jWHDVV1qrVfBTa0\n9kbglaF+J1ptY2vPr58zpqrOAN8A3rvQmya5M8lMkpm5ubllTl2StJiRL0i3I4Eaw1yW8l4Hqmpb\nVW2bmppaibeUpEvScsPhtXaqiPZ8utVPAtcO9dvUaidbe379nDFJ1gPvBl5f5rwkSWOw3HA4DOxp\n7T3AY0P13e0OpM0MLjw/3U5BvZFke7uecNu8MWf39WHgs+1oRJI0IYt+CC7Jp4EPAFcnOQHcC+wH\nDiW5HXgZuBWgqo4nOQQ8B5wB7q6qt9qu7mJw59PlwBPtAfAg8GtJZhlc+N49lpVJkpZt0XCoqo+c\nZ9OO8/TfB+xboD4DXL9A/U+An1psHpKkleMnpCVJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJ\nHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNB\nktQxHCRJHcNBktQZKRyS/MMkx5M8m+TTSb47yVVJnkzyYnu+cqj/PUlmk7yQ5Kah+g1JjrVt9yXJ\nKPOSJI1m2eGQZCPw94FtVXU9sA7YDewFjlTVFuBIe02SrW37dcBO4P4k69ruHgDuALa0x87lzkuS\nNLpRTyutBy5Psh54J/CHwC7gYNt+ELiltXcBj1TVm1X1EjAL3JjkGuCKqjpaVQU8PDRGkjQByw6H\nqjoJ/HPgq8Ap4BtV9dvAhqo61bq9Cmxo7Y3AK0O7ONFqG1t7fl2SNCGjnFa6ksHRwGbgzwPfk+Rn\nhvu0I4EaaYbnvuedSWaSzMzNzY1rt5KkeUY5rfTjwEtVNVdV/xf4DPDDwGvtVBHt+XTrfxK4dmj8\nplY72drz652qOlBV26pq29TU1AhTlyR9O6OEw1eB7Une2e4u2gE8DxwG9rQ+e4DHWvswsDvJZUk2\nM7jw/HQ7BfVGku1tP7cNjZEkTcD65Q6sqqeSPAr8LnAG+AJwAHgXcCjJ7cDLwK2t//Ekh4DnWv+7\nq+qttru7gIeAy4En2kOSNCHLDgeAqroXuHde+U0GRxEL9d8H7FugPgNcP8pcJEnj4yekJUkdw0GS\n1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEc\nJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1BkpHJK8J8mjSX4/yfNJ/nqSq5I8meTF\n9nzlUP97kswmeSHJTUP1G5Ica9vuS5JR5iVJGs2oRw6/CvxWVf0A8D7geWAvcKSqtgBH2muSbAV2\nA9cBO4H7k6xr+3kAuAPY0h47R5yXJGkEyw6HJO8Gfgx4EKCq/rSq/hewCzjYuh0EbmntXcAjVfVm\nVb0EzAI3JrkGuKKqjlZVAQ8PjZEkTcAoRw6bgTng3yT5QpJPJPkeYENVnWp9XgU2tPZG4JWh8Sda\nbWNrz69LkiZklHBYD7wfeKCqfgj4Ju0U0lntSKBGeI9zJLkzyUySmbm5uXHtVpI0zyjhcAI4UVVP\ntdePMgiL19qpItrz6bb9JHDt0PhNrXaytefXO1V1oKq2VdW2qampEaYuSfp2lh0OVfUq8EqS72+l\nHcBzwGFgT6vtAR5r7cPA7iSXJdnM4MLz0+0U1BtJtre7lG4bGiNJmoD1I47/eeBTSd4BfBn4KIPA\nOZTkduBl4FaAqjqe5BCDADkD3F1Vb7X93AU8BFwOPNEeK2567+Nvt7+y/+ZJTEGSVoWRwqGqvghs\nW2DTjvP03wfsW6A+A1w/ylwkSePjJ6QlSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLU\nMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwk\nSR3DQZLUMRwkSZ2RwyHJuiRfSPIf2+urkjyZ5MX2fOVQ33uSzCZ5IclNQ/Ubkhxr2+5LklHnJUla\nvnEcOXwMeH7o9V7gSFVtAY601yTZCuwGrgN2AvcnWdfGPADcAWxpj51jmJckaZlGCockm4CbgU8M\nlXcBB1v7IHDLUP2Rqnqzql4CZoEbk1wDXFFVR6uqgIeHxkiSJmDUI4dfAX4B+NZQbUNVnWrtV4EN\nrb0ReGWo34lW29ja8+udJHcmmUkyMzc3N+LUJUnns+xwSPITwOmqeuZ8fdqRQC33PRbY34Gq2lZV\n26ampsa1W0nSPOtHGPsjwE8m+RDw3cAVSf4t8FqSa6rqVDtldLr1PwlcOzR+U6udbO35dUnShCz7\nyKGq7qmqTVU1zeBC82er6meAw8Ce1m0P8FhrHwZ2J7ksyWYGF56fbqeg3kiyvd2ldNvQGEnSBIxy\n5HA++4FDSW4HXgZuBaiq40kOAc8BZ4C7q+qtNuYu4CHgcuCJ9pAkTchYwqGqPgd8rrVfB3acp98+\nYN8C9Rng+nHMRZI0Oj8hLUnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6S\npM6F+G6li8L03sffbn9l/80TnIkkrTyPHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNB\nktQxHCRJHcNBktQxHCRJnWWHQ5Jrk/znJM8lOZ7kY61+VZInk7zYnq8cGnNPktkkLyS5aah+Q5Jj\nbdt9STLasiRJoxjlyOEM8I+qaiuwHbg7yVZgL3CkqrYAR9pr2rbdwHXATuD+JOvavh4A7gC2tMfO\nEeYlSRrRssOhqk5V1e+29v8Gngc2AruAg63bQeCW1t4FPFJVb1bVS8AscGOSa4ArqupoVRXw8NAY\nSdIEjOWaQ5Jp4IeAp4ANVXWqbXoV2NDaG4FXhoadaLWNrT2/vtD73JlkJsnM3NzcOKYuSVrAyOGQ\n5F3AbwD/oKreGN7WjgRq1PcY2t+BqtpWVdumpqbGtVtJ0jwjhUOS72IQDJ+qqs+08mvtVBHt+XSr\nnwSuHRq+qdVOtvb8uiRpQka5WynAg8DzVfUvhjYdBva09h7gsaH67iSXJdnM4MLz0+0U1BtJtrd9\n3jY0RpI0AaP8N6E/AvwscCzJF1vtl4D9wKEktwMvA7cCVNXxJIeA5xjc6XR3Vb3Vxt0FPARcDjzR\nHpKkCVl2OFTVfwPO93mEHecZsw/Yt0B9Brh+uXORJI2Xn5CWJHVGOa10yZje+/g5r7+y/+YJzUSS\nVoZHDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjp+QXobhT0z7aWlJFyOP\nHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktTxVtYReVurpIuRRw6SpI7hIEnqeFppjDzFJOlisWrC\nIclO4FeBdcAnqmr/hKc0kuGgGGZoSFoLVsVppSTrgH8FfBDYCnwkydbJzkqSLl2r5cjhRmC2qr4M\nkOQRYBfw3ERndQGc74himEcXkiZttYTDRuCVodcngL82oblM3FICZCmGQ8brIZK+E6slHJYkyZ3A\nne3l/0nywjJ3dTXwR+OZ1eqVj5/z8u01z6tfzC6Jn/M8rvnSMMqa/8JSOq2WcDgJXDv0elOrnaOq\nDgAHRn2zJDNVtW3U/awlrvnS4JovDSux5lVxQRr4H8CWJJuTvAPYDRye8Jwk6ZK1Ko4cqupMkr8H\n/CcGt7J+sqqOT3haknTJWhXhAFBVvwn85gq93cinptYg13xpcM2Xhgu+5lTVhX4PSdIas1quOUiS\nVpGLOhyS7EzyQpLZJHsX2J4k97XtX0ry/knMc5yWsOafbms9luR3krxvEvMcp8XWPNTvryY5k+TD\nKzm/C2Epa07ygSRfTHI8yX9Z6TmO0xL+Xb87yX9I8nttvR+dxDzHKcknk5xO8ux5tl/Y319VdVE+\nGFzY/p/AXwTeAfwesHVenw8BTwABtgNPTXreK7DmHwaubO0PXgprHur3WQbXtT486XmvwM/5PQy+\nYeB72+s/N+l5X+D1/hLw8daeAr4GvGPScx9x3T8GvB949jzbL+jvr4v5yOHtr+Soqj8Fzn4lx7Bd\nwMM1cBR4T5JrVnqiY7Tomqvqd6rq6+3lUQafKVnLlvJzBvh54DeA0ys5uQtkKWv+28BnquqrAFW1\nlte9lPUW8GeTBHgXg3A4s7LTHK+q+jyDdZzPBf39dTGHw0JfybFxGX3Wku90Pbcz+MtjLVt0zUk2\nAn8LeGAF53UhLeXn/JeBK5N8LskzSW5bsdmN31LW+y+BvwL8IXAM+FhVfWtlpjcxF/T316q5lVUr\nK8nfYBAOPzrpuayAXwF+saq+NfjD8pKwHrgB2AFcDvz3JEer6g8mO60L5ibgi8DfBP4S8GSS/1pV\nb0x2WmvXxRwOS/lKjiV9bccasqT1JPlB4BPAB6vq9RWa24WylDVvAx5pwXA18KEkZ6rq36/MFMdu\nKWs+AbxeVd8Evpnk88D7gLUYDktZ70eB/TU4GT+b5CXgB4CnV2aKE3FBf39dzKeVlvKVHIeB29pV\n/+3AN6rq1EpPdIwWXXOS7wU+A/zsRfJX5KJrrqrNVTVdVdPAo8BdazgYYGn/th8DfjTJ+iTvZPAt\nx8+v8DzHZSnr/SqDoySSbAC+H/jyis5y5V3Q318X7ZFDnecrOZL8XNv+rxncufIhYBb4YwZ/faxZ\nS1zzPwHeC9zf/pI+U2v4S8uWuOaLylLWXFXPJ/kt4EvAtxj874oL3hK52i3xZ/xPgYeSHGNw984v\nVtWa/qbWJJ8GPgBcneQEcC/wXbAyv7/8hLQkqXMxn1aSJC2T4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ\n6hgOkqSO4SBJ6vw/jQVruU3ZRjcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ff29fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_minmax, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distribution is exactly the same, except the data is bound between 0 and 1. Let's train a new model to see if this scaling makes any difference in model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"mini_subtrain_X_minmax.csv\", index_col= 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 1s 43us/step - loss: 0.0017 - val_loss: 8.2608e-04\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.0011 - val_loss: 7.8011e-04\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.0011 - val_loss: 7.6305e-04\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.5945e-04\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 7.5731e-04\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.6652e-04\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.5633e-04\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 7.4912e-04\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 7.5205e-04\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.0010 - val_loss: 7.5096e-04\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.0010 - val_loss: 7.4932e-04\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.4861e-04\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.0010 - val_loss: 7.5650e-04\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.9756e-04 - val_loss: 7.5135e-04\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.9488e-04 - val_loss: 7.6336e-04\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.9188e-04 - val_loss: 7.7025e-04\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.9316e-04 - val_loss: 7.5240e-04\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.9169e-04 - val_loss: 7.6147e-04\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8985e-04 - val_loss: 7.5685e-04\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.9142e-04 - val_loss: 7.4342e-04\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.9029e-04 - val_loss: 7.5710e-04\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.8590e-04 - val_loss: 7.6199e-04\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.8795e-04 - val_loss: 7.5097e-04\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.8777e-04 - val_loss: 7.6290e-04\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.8559e-04 - val_loss: 7.5260e-04\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.8629e-04 - val_loss: 7.4988e-04\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.8418e-04 - val_loss: 7.5650e-04\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.8314e-04 - val_loss: 7.5942e-04\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8263e-04 - val_loss: 7.5499e-04\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.8136e-04 - val_loss: 7.6330e-04\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8147e-04 - val_loss: 7.5576e-04\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.8026e-04 - val_loss: 7.5556e-04\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.8188e-04 - val_loss: 7.4969e-04\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8272e-04 - val_loss: 7.5923e-04\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7984e-04 - val_loss: 7.6242e-04\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.7816e-04 - val_loss: 7.6047e-04\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.8111e-04 - val_loss: 7.5372e-04\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.7881e-04 - val_loss: 7.5441e-04\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.7944e-04 - val_loss: 7.5296e-04\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7708e-04 - val_loss: 7.6502e-04\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.7539e-04 - val_loss: 7.4992e-04\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7600e-04 - val_loss: 7.6425e-04\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7572e-04 - val_loss: 7.5909e-04\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7489e-04 - val_loss: 7.6842e-04\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.7627e-04 - val_loss: 7.7012e-04\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.7474e-04 - val_loss: 7.5925e-04\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7408e-04 - val_loss: 7.6964e-04\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7555e-04 - val_loss: 7.5430e-04\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7206e-04 - val_loss: 7.7962e-04\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7332e-04 - val_loss: 7.7954e-04\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7512e-04 - val_loss: 7.6322e-04\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7125e-04 - val_loss: 7.6754e-04\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7210e-04 - val_loss: 7.6268e-04\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 9.7643e-04 - val_loss: 7.6646e-04\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.6932e-04 - val_loss: 7.5783e-04\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7162e-04 - val_loss: 7.5393e-04\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7167e-04 - val_loss: 7.6766e-04\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7198e-04 - val_loss: 7.7110e-04\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7116e-04 - val_loss: 7.6145e-04\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7135e-04 - val_loss: 7.6163e-04\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6992e-04 - val_loss: 7.7884e-04\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.6951e-04 - val_loss: 7.6217e-04\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6897e-04 - val_loss: 7.6869e-04\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.7386e-04 - val_loss: 7.8255e-04\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6828e-04 - val_loss: 7.6289e-04\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7143e-04 - val_loss: 7.6894e-04\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6709e-04 - val_loss: 7.7155e-04\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6665e-04 - val_loss: 7.6693e-04\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6788e-04 - val_loss: 7.6327e-04\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6517e-04 - val_loss: 7.6484e-04\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6605e-04 - val_loss: 7.8005e-04\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6961e-04 - val_loss: 7.6278e-04\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.6190e-04 - val_loss: 8.3903e-04\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7426e-04 - val_loss: 7.6857e-04\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6727e-04 - val_loss: 7.6418e-04\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6684e-04 - val_loss: 7.6845e-04\n",
      "Epoch 77/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.6386e-04 - val_loss: 7.7303e-04\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6340e-04 - val_loss: 7.9560e-04\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 9.6448e-04 - val_loss: 7.7746e-04\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.6174e-04 - val_loss: 7.9979e-04\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6415e-04 - val_loss: 7.7052e-04\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6019e-04 - val_loss: 7.8271e-04\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5944e-04 - val_loss: 7.7734e-04\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6078e-04 - val_loss: 7.7717e-04\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6039e-04 - val_loss: 7.7097e-04\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.5824e-04 - val_loss: 7.5623e-04\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.5879e-04 - val_loss: 7.6081e-04\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5843e-04 - val_loss: 7.7443e-04\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.5778e-04 - val_loss: 7.7814e-04\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5866e-04 - val_loss: 7.8373e-04\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.5902e-04 - val_loss: 7.7850e-04\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5656e-04 - val_loss: 7.6994e-04\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5729e-04 - val_loss: 7.8387e-04\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5892e-04 - val_loss: 7.7847e-04\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.5518e-04 - val_loss: 7.6901e-04\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.5567e-04 - val_loss: 7.8834e-04\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.5188e-04 - val_loss: 7.7806e-04\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5487e-04 - val_loss: 7.7666e-04\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 9.5689e-04 - val_loss: 7.6787e-04\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5402e-04 - val_loss: 7.6998e-04\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model_20 = model.fit(X,y_minmax, epochs = 100, batch_size= 100,validation_split= 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the predictions of the model\n",
    "model_20_pred = model_20.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02461544],\n",
       "       [ 0.0167399 ],\n",
       "       [ 0.00623888],\n",
       "       ..., \n",
       "       [ 0.02530498],\n",
       "       [ 0.020853  ],\n",
       "       [ 0.03319128]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_20_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxOBORJRC/x/mX34WF6zEEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124e62198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x = model_20_pred, y = y_minmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the model also suffers from predicting the outlier price values! We also note that the predictions come back more gaussian compared to the target variable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyJ2lAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124d2d7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model_20_pred,bins =100,color= \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevethless, less inverse transform the predictions and try to measure RMSE, we will use the same transformer object we used to transform y for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20_pred_rescaled = minmax_scaler_y.inverse_transform(model_20_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127451a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model_20_pred_rescaled, bins = 100) #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3739844235793084"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y,model_20_pred_rescaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our RMSE is actually much higher in this case! Therefore, it did not helped to train the model in this way. \n",
    "\n",
    "    How about we log transform the min_max scaled y to make it more gaussian before fitting the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 1s 44us/step - loss: 0.0014 - val_loss: 6.8909e-04\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 8.3044e-04 - val_loss: 6.4429e-04\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.9660e-04 - val_loss: 6.2674e-04\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.8311e-04 - val_loss: 6.2217e-04\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.7745e-04 - val_loss: 6.1944e-04\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.7010e-04 - val_loss: 6.2782e-04\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.6850e-04 - val_loss: 6.1771e-04\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.6211e-04 - val_loss: 6.1328e-04\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 7.5825e-04 - val_loss: 6.1377e-04\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.5420e-04 - val_loss: 6.1273e-04\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.5066e-04 - val_loss: 6.1260e-04\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.4840e-04 - val_loss: 6.1171e-04\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 7.4850e-04 - val_loss: 6.1894e-04\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.4494e-04 - val_loss: 6.1302e-04\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.4272e-04 - val_loss: 6.2130e-04\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.4097e-04 - val_loss: 6.2519e-04\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 7.4047e-04 - val_loss: 6.1376e-04\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3850e-04 - val_loss: 6.2023e-04\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.3756e-04 - val_loss: 6.1946e-04\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3797e-04 - val_loss: 6.0835e-04\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3688e-04 - val_loss: 6.1829e-04\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3334e-04 - val_loss: 6.2253e-04\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3498e-04 - val_loss: 6.1608e-04\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.3545e-04 - val_loss: 6.2251e-04\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.3342e-04 - val_loss: 6.1440e-04\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.3368e-04 - val_loss: 6.1373e-04\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.3132e-04 - val_loss: 6.1855e-04\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3093e-04 - val_loss: 6.1830e-04\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3048e-04 - val_loss: 6.1539e-04\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2913e-04 - val_loss: 6.2102e-04\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2946e-04 - val_loss: 6.1573e-04\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2793e-04 - val_loss: 6.1848e-04\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2893e-04 - val_loss: 6.1250e-04\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2972e-04 - val_loss: 6.2223e-04\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2886e-04 - val_loss: 6.2077e-04\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2627e-04 - val_loss: 6.1882e-04\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2766e-04 - val_loss: 6.1613e-04\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 7.2570e-04 - val_loss: 6.1596e-04\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.2729e-04 - val_loss: 6.1347e-04\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2508e-04 - val_loss: 6.2233e-04\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2450e-04 - val_loss: 6.1210e-04\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.2524e-04 - val_loss: 6.2480e-04\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 7.2444e-04 - val_loss: 6.1734e-04\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2432e-04 - val_loss: 6.2600e-04\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2499e-04 - val_loss: 6.2609e-04\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.2406e-04 - val_loss: 6.1848e-04\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2428e-04 - val_loss: 6.2566e-04\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 7.2497e-04 - val_loss: 6.1588e-04\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2271e-04 - val_loss: 6.3479e-04\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.2248e-04 - val_loss: 6.3195e-04\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2357e-04 - val_loss: 6.2100e-04\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2160e-04 - val_loss: 6.2320e-04\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2253e-04 - val_loss: 6.2119e-04\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2547e-04 - val_loss: 6.2281e-04\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2085e-04 - val_loss: 6.1915e-04\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2076e-04 - val_loss: 6.1616e-04\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2210e-04 - val_loss: 6.2763e-04\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2218e-04 - val_loss: 6.3242e-04\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2143e-04 - val_loss: 6.2316e-04\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 7.2158e-04 - val_loss: 6.2115e-04\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2036e-04 - val_loss: 6.3406e-04\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2019e-04 - val_loss: 6.2169e-04\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1958e-04 - val_loss: 6.2643e-04\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.2266e-04 - val_loss: 6.3831e-04\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1968e-04 - val_loss: 6.2210e-04\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2044e-04 - val_loss: 6.2471e-04\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.1776e-04 - val_loss: 6.2275e-04\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1778e-04 - val_loss: 6.2830e-04\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1882e-04 - val_loss: 6.2175e-04\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1648e-04 - val_loss: 6.2535e-04\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1765e-04 - val_loss: 6.2968e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1905e-04 - val_loss: 6.2238e-04\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.1409e-04 - val_loss: 6.6533e-04\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.2287e-04 - val_loss: 6.2740e-04\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.1732e-04 - val_loss: 6.2746e-04\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1810e-04 - val_loss: 6.3010e-04\n",
      "Epoch 77/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1694e-04 - val_loss: 6.2972e-04\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1552e-04 - val_loss: 6.4109e-04\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1686e-04 - val_loss: 6.3289e-04\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1568e-04 - val_loss: 6.4260e-04\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1731e-04 - val_loss: 6.2749e-04\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 7.1453e-04 - val_loss: 6.3641e-04\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1462e-04 - val_loss: 6.3191e-04\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1540e-04 - val_loss: 6.3012e-04\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.1464e-04 - val_loss: 6.2538e-04\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1381e-04 - val_loss: 6.1843e-04\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1418e-04 - val_loss: 6.1959e-04\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1421e-04 - val_loss: 6.3247e-04\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1362e-04 - val_loss: 6.3165e-04\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1391e-04 - val_loss: 6.3788e-04\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.1403e-04 - val_loss: 6.3229e-04\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.1218e-04 - val_loss: 6.2961e-04\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.1200e-04 - val_loss: 6.3618e-04\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 7.1329e-04 - val_loss: 6.3317e-04\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1173e-04 - val_loss: 6.2756e-04\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1194e-04 - val_loss: 6.3673e-04\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 7.0831e-04 - val_loss: 6.3377e-04\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1092e-04 - val_loss: 6.3502e-04\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1144e-04 - val_loss: 6.2572e-04\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1049e-04 - val_loss: 6.2944e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.474969229500143"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model_21 = model.fit(X,np.log(y_minmax+1), epochs = 100, batch_size= 100,validation_split= 0.4)\n",
    "# We get the predictions of the model\n",
    "model_21_pred = model_21.model.predict(X)\n",
    "# First exponentiate back\n",
    "model_21_pred = np.exp(model_21_pred)-1\n",
    "# Then inverse transform\n",
    "model_21_pred_rescaled = minmax_scaler_y.inverse_transform(model_21_pred)\n",
    "# Calculate test RMSE\n",
    "np.sqrt(mean_squared_error(y,model_21_pred_rescaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turned as not so helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
