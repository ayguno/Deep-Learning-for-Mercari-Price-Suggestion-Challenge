{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "In this exercise I will use the features I previously engineered using R and Kaggle Mercari Price challenge data set. \n",
    "\n",
    "We will start with loading the libraries and functions we will need during the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import SGD\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_condition_id  price  shipping  no.brand_name  log.excl.description  \\\n",
       "1                  1    8.0         0              1                   0.0   \n",
       "2                  2   39.0         1              1                   0.0   \n",
       "3                  1   30.0         1              0                   0.0   \n",
       "4                  2  470.0         1              0                   0.0   \n",
       "5                  2   22.0         0              0                   0.0   \n",
       "\n",
       "   excl.name  dollar.description  fancy.categories  cheap.categories  \\\n",
       "1          0                   0                 0                 0   \n",
       "2          0                   0                 0                 0   \n",
       "3          0                   0                 0                 0   \n",
       "4          0                   0                 0                 0   \n",
       "5          0                   0                 0                 0   \n",
       "\n",
       "   fancy.brands        ...         now  cheap  buy  excellent  great  \\\n",
       "1             0        ...           0      0    0          0      0   \n",
       "2             0        ...           0      0    0          0      0   \n",
       "3             0        ...           0      0    0          0      0   \n",
       "4             1        ...           0      0    0          0      0   \n",
       "5             0        ...           0      0    0          0      0   \n",
       "\n",
       "   michael.brand  jordan.name  iphon.name  bundl.name  cap.letter.brand  \n",
       "1              0            0           0           0                 1  \n",
       "2              0            0           0           0                 1  \n",
       "3              0            0           0           1                 0  \n",
       "4              0            0           0           0                 6  \n",
       "5              0            0           0           0                 2  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load the mini subtraining set we prepared previously\n",
    "mini_subtrain = pd.read_csv(\"mini_subtrain.csv\", index_col = 0)\n",
    "mini_subtrain.shape\n",
    "mini_subtrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_subtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "mini_subtrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sucessfully reading the verifying the training data set we have previously constructed using R, we can start building a small neural network and training it by using our data.\n",
    "\n",
    "First we start with seperating predictors and response arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = np.array(mini_subtrain.drop([\"price\"], axis=1))\n",
    "# We will log transform the target variable as we have performed in R\n",
    "target = np.array(np.log(mini_subtrain.price + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can start building our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 98us/step - loss: 1.3796 - val_loss: 0.5341\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4969 - val_loss: 0.4515\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4742 - val_loss: 0.4472\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4719 - val_loss: 0.4500\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4720 - val_loss: 0.4457\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 65us/step - loss: 0.4704 - val_loss: 0.4510\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 64us/step - loss: 0.4698 - val_loss: 0.4503\n"
     ]
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_1 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our loss function is \"mean squared error\". We will take the square root of this to follow \"root mean squared error\" (RMSE). We also keep in mind that the target is log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66764026261785125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_1 = np.sqrt(min(model_1.history[\"val_loss\"]))\n",
    "RMSE_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this RMSE is close to what we have obtained other machine learning algorithms previously. Therefore, we will continue our experiment by increasing model complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 25s 953us/step - loss: 0.6853 - val_loss: 0.4837\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 25s 949us/step - loss: 0.4860 - val_loss: 0.4547\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 25s 968us/step - loss: 0.4801 - val_loss: 0.4599\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 29s 1ms/step - loss: 0.4754 - val_loss: 0.4593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67429454720879667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1000,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_2 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_2 = np.sqrt(min(model_2.history[\"val_loss\"]))\n",
    "RMSE_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our first model is already at its capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 111us/step - loss: 1.0952 - val_loss: 0.4894\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4874 - val_loss: 0.4550\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4755 - val_loss: 0.4524\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4727 - val_loss: 0.4561\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4711 - val_loss: 0.4517\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 0.4686 - val_loss: 0.4507\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 68us/step - loss: 0.4679 - val_loss: 0.4551\n",
      "Epoch 8/30\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 0.4683 - val_loss: 0.4571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67136944170640311"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_3 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_3 = np.sqrt(min(model_3.history[\"val_loss\"]))\n",
    "RMSE_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we reached to model capacity even with a relatively simple network. Next, we will try if we can reduce the bias by training a larger data set, which we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrain = pd.read_csv(\"subtrain.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(741269, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741269 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741269 non-null int64\n",
      "price                   741269 non-null float64\n",
      "shipping                741269 non-null int64\n",
      "no.brand_name           741269 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741269 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741269 non-null int64\n",
      "cheap.categories        741269 non-null int64\n",
      "fancy.brands            741269 non-null int64\n",
      "cheap.brands            741269 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741269 non-null int64\n",
      "jordan.name             741269 non-null int64\n",
      "iphon.name              741269 non-null int64\n",
      "bundl.name              741269 non-null int64\n",
      "cap.letter.brand        741269 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "#We need to check for missing values in the data set\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741268 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741268 non-null int64\n",
      "price                   741268 non-null float64\n",
      "shipping                741268 non-null int64\n",
      "no.brand_name           741268 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741268 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741268 non-null int64\n",
      "cheap.categories        741268 non-null int64\n",
      "fancy.brands            741268 non-null int64\n",
      "cheap.brands            741268 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741268 non-null int64\n",
      "jordan.name             741268 non-null int64\n",
      "iphon.name              741268 non-null int64\n",
      "bundl.name              741268 non-null int64\n",
      "cap.letter.brand        741268 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "subtrain = subtrain.dropna()\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(subtrain.price + 1)\n",
    "X = subtrain.drop([\"price\"],axis = 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.5185 - val_loss: 0.4680\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4695 - val_loss: 0.4894\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4674 - val_loss: 0.4629\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4664 - val_loss: 0.4629\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4658 - val_loss: 0.4662\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4654 - val_loss: 0.4617\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4650 - val_loss: 0.4623\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4649 - val_loss: 0.4617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6794526310377843"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_4 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_4 = np.sqrt(min(model_4.history[\"val_loss\"]))\n",
    "RMSE_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.5029 - val_loss: 0.4714\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4684 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4669 - val_loss: 0.4626\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4658 - val_loss: 0.4621\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4655 - val_loss: 0.4626\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4646 - val_loss: 0.4618\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4644 - val_loss: 0.4605\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4641 - val_loss: 0.4640\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4639 - val_loss: 0.4614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67862169258002358"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_5 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_5 = np.sqrt(min(model_5.history[\"val_loss\"]))\n",
    "RMSE_model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4934 - val_loss: 0.4676\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4687 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4662 - val_loss: 0.4616\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4653 - val_loss: 0.4616\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4644 - val_loss: 0.4615\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4638 - val_loss: 0.4613\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4634 - val_loss: 0.4608\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4631 - val_loss: 0.4618\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4629 - val_loss: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67883387278347673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_6 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_6 = np.sqrt(min(model_6.history[\"val_loss\"]))\n",
    "RMSE_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4880 - val_loss: 0.4679\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4714 - val_loss: 0.4882\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4688 - val_loss: 0.4665\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4675 - val_loss: 0.4698\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4666 - val_loss: 0.4640\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 63us/step - loss: 0.4676 - val_loss: 0.4640\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 33s 65us/step - loss: 0.4661 - val_loss: 0.4626\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4653 - val_loss: 0.4641\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 30s 58us/step - loss: 0.4647 - val_loss: 0.4636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68016471861743943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing optimizer to SGD\n",
    "sgd_optimizer = SGD(lr = 0.01)\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= sgd_optimizer, loss= \"mean_squared_error\")\n",
    "model_7 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_7 = np.sqrt(min(model_7.history[\"val_loss\"]))\n",
    "RMSE_model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/20\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4953 - val_loss: 0.4869\n",
      "Epoch 2/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4685 - val_loss: 0.4672\n",
      "Epoch 3/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4669 - val_loss: 0.4635\n",
      "Epoch 4/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4659 - val_loss: 0.4623\n",
      "Epoch 5/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4668\n",
      "Epoch 6/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4623\n",
      "Epoch 7/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4645 - val_loss: 0.4626\n",
      "Epoch 8/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4645 - val_loss: 0.4622\n",
      "Epoch 9/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4639 - val_loss: 0.4612\n",
      "Epoch 10/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4658 - val_loss: 0.4638\n",
      "Epoch 11/20\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4636 - val_loss: 0.4611\n",
      "Epoch 12/20\n",
      "518887/518887 [==============================] - 40s 78us/step - loss: 0.4634 - val_loss: 0.4606\n",
      "Epoch 13/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4632 - val_loss: 0.4617\n",
      "Epoch 14/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4632 - val_loss: 0.4612\n",
      "Epoch 15/20\n",
      "518887/518887 [==============================] - 36s 70us/step - loss: 0.4631 - val_loss: 0.4602\n",
      "Epoch 16/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4629 - val_loss: 0.4640\n",
      "Epoch 17/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4628 - val_loss: 0.4610\n",
      "Epoch 18/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4629 - val_loss: 0.4642\n",
      "Epoch 19/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4625 - val_loss: 0.4607\n",
      "Epoch 20/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4624 - val_loss: 0.4647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67839322279547709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing fixed 20 epochs\n",
    "#estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_8 = model.fit(X,y, epochs= 20, validation_split= 0.3)\n",
    "RMSE_model_8 = np.sqrt(min(model_8.history[\"val_loss\"]))\n",
    "RMSE_model_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.5663 - val_loss: 0.4738\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4740 - val_loss: 0.4702\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4735 - val_loss: 0.4721\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 32s 62us/step - loss: 0.4726 - val_loss: 0.4706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68569114759373795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing final activation function\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_9 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_9 = np.sqrt(min(model_9.history[\"val_loss\"]))\n",
    "RMSE_model_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to learn other means of model optimization and try to test here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the small data set for training once again:\n",
    "os.listdir()\n",
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis= 1).values\n",
    "y = np.log(df.price+1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # This is a Keras wrapper for sklearn we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt I loaded some sklearn functions to use them in model evaluation, as well as a Keras wrapper for sklearn.\n",
    "\n",
    "In this case, the Keras wrappers we will use take a function as argument, which we will define to create the neural network model structure.\n",
    "\n",
    "Let's define the basic_model function:\n",
    "\n",
    "- We use a sequential model structure.\n",
    "- We will have a simple 2-layered model (input and output layer).\n",
    "- We don't use any activation function at the output layer (since we are performing regression, we want values to be evaluated without transformation). \n",
    "- We will use ADAM optimization function and optimize mean squared error as our loss function\n",
    "- We will use the same number of neurons(nodes) as the the number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    \"\"\"Creates, compiles and returns the basic NN\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "    model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define our regression estimator using keras wrapper KerasRegressor we imported above. This estimator function will receive:\n",
    "\n",
    "- the function which creates the NN model\n",
    "- parameters we normally enter in .fit() function (e.g: number of epochs or batch size), note that in this case we are doing the computation in batch mode, so we use nb_epoch argument\n",
    "\n",
    "we will also need to set the random number generator seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4\n",
    "np.random.seed(seed)\n",
    "regression_estimator = KerasRegressor(build_fn= basic_model, \n",
    "                                     nb_epoch = 100,\n",
    "                                     batch_size = 5,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the model using 10-fold cross-validation. Note that this is the actual step in which we are fitting the model on the data. We are collecting the model evaluation in a object we called **results**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c75cb95c7d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set of the k-folds and random state to partition the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregression_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits= 10, random_state= seed) # Set of the k-folds and random state to partition the data\n",
    "results = cross_val_score(estimator=regression_estimator, X = X,y = y,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results) # Note that results is an array that contains MSE scores for each k-fold cv step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4844004 , -0.46776465, -0.49775278, -0.4527221 , -0.46104527,\n",
       "       -0.48631333, -0.46733199, -0.45774577, -0.47870872, -0.46569664])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47194816461575667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mse returned by CV function is negative! This is quite counter intuitive and confusing. Checking the documentation in GitHub, I understand that this is not a useful way of estimating model performance, perhaps the wrapper has problems.\n",
    "\n",
    "I am going back to the original way I fit the models, that is using a single split for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 2.9459 - val_loss: 1.0557\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.9681 - val_loss: 0.8441\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.7625 - val_loss: 0.6377\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.5757 - val_loss: 0.4834\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4858 - val_loss: 0.4520\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4749 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4741 - val_loss: 0.4503\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4745 - val_loss: 0.4501\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4741 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4746 - val_loss: 0.4512\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4745 - val_loss: 0.4561\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4741 - val_loss: 0.4511\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4739 - val_loss: 0.4508\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4743 - val_loss: 0.4525\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4743 - val_loss: 0.4533\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4728 - val_loss: 0.4498\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4720 - val_loss: 0.4519\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4708 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4702 - val_loss: 0.4506\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4692 - val_loss: 0.4498\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4468\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4690 - val_loss: 0.4468\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4689 - val_loss: 0.4471\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4681 - val_loss: 0.4479\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4686 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4471\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4682 - val_loss: 0.4494\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4681 - val_loss: 0.4471\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4680 - val_loss: 0.4492\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4675 - val_loss: 0.4616\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4678 - val_loss: 0.4472\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4675 - val_loss: 0.4474\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4670 - val_loss: 0.4472\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4675 - val_loss: 0.4467\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4468\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4669 - val_loss: 0.4470\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4667 - val_loss: 0.4460\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4673 - val_loss: 0.4503\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4671 - val_loss: 0.4466\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4674 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4666 - val_loss: 0.4492\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4481\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4662 - val_loss: 0.4459\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4670 - val_loss: 0.4470\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4652 - val_loss: 0.4564\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4665 - val_loss: 0.4498\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4660 - val_loss: 0.4462\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4659 - val_loss: 0.4465\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4658 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4661 - val_loss: 0.4469\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4652 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4655 - val_loss: 0.4479\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4650 - val_loss: 0.4469\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4651 - val_loss: 0.4470\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4644 - val_loss: 0.4502\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4650 - val_loss: 0.4466\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4497\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4647 - val_loss: 0.4468\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4489\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4487\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4466\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4653 - val_loss: 0.4474\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4537\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4650 - val_loss: 0.4473\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4649 - val_loss: 0.4471\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4482\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4646 - val_loss: 0.4480\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4645 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4476\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4645 - val_loss: 0.4470\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4483\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4638 - val_loss: 0.4478\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4521\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4476\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4644 - val_loss: 0.4467\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4480\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4646 - val_loss: 0.4481\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4639 - val_loss: 0.4499\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4642 - val_loss: 0.4497\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4640 - val_loss: 0.4472\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4471\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4638 - val_loss: 0.4498\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4475\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4479\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4638 - val_loss: 0.4484\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4570\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4645 - val_loss: 0.4483\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4641 - val_loss: 0.4486\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4641 - val_loss: 0.4474\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4632 - val_loss: 0.4484\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_10 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66776206455463483"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_10 = np.sqrt(min(model_10.history[\"val_loss\"]))\n",
    "RMSE_model_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight improvement over model_1. Let's continue by trying to tune model topology. \n",
    "\n",
    "We can try a more complex network, hoping that this network will extract and recombine more interactions between the available features.\n",
    "\n",
    "- We can set up deeper NNs : i.e: adding layers\n",
    "- We can set up wider NNs: i.e: adding neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 3.2844 - val_loss: 0.9369\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.8241 - val_loss: 0.6616\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5722 - val_loss: 0.4716\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4792 - val_loss: 0.4520\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4741 - val_loss: 0.4498\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4733 - val_loss: 0.4498\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4727 - val_loss: 0.4500\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4733 - val_loss: 0.4486\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4723 - val_loss: 0.4529\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4722 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4724 - val_loss: 0.4530\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4716 - val_loss: 0.4483\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4709 - val_loss: 0.4479\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4558\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4717 - val_loss: 0.4516\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4716 - val_loss: 0.4486\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4713 - val_loss: 0.4526\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4709 - val_loss: 0.4531\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4710 - val_loss: 0.4530\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4480\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4707 - val_loss: 0.4535\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4474\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4476\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4472\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4484\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4708 - val_loss: 0.4478\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4479\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4703 - val_loss: 0.4485\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4700 - val_loss: 0.4495\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4694 - val_loss: 0.4613\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4701 - val_loss: 0.4483\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4690 - val_loss: 0.4471\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4489\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4688 - val_loss: 0.4491\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4477\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4693 - val_loss: 0.4480\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4693 - val_loss: 0.4504\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4694 - val_loss: 0.4470\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4512\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4691 - val_loss: 0.4479\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4483\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4487\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4488\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4471\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4679 - val_loss: 0.4591\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4488\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4688 - val_loss: 0.4475\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4686 - val_loss: 0.4503\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4480\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4677 - val_loss: 0.4486\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4667 - val_loss: 0.4473\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4675 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4660 - val_loss: 0.4477\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4663 - val_loss: 0.4460\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4659 - val_loss: 0.4479\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4654 - val_loss: 0.4449\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4647 - val_loss: 0.4470\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4650 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4461\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4448\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4443\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4632 - val_loss: 0.4518\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4632 - val_loss: 0.4435\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4628 - val_loss: 0.4432\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4623 - val_loss: 0.4445\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4625 - val_loss: 0.4447\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4623 - val_loss: 0.4444\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4622 - val_loss: 0.4436\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4616 - val_loss: 0.4437\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.458 - 1s 32us/step - loss: 0.4616 - val_loss: 0.4450\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4608 - val_loss: 0.4436\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4481\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4609 - val_loss: 0.4447\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4616 - val_loss: 0.4434\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4436\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4455\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4603 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4607 - val_loss: 0.4449\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4602 - val_loss: 0.4436\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4430\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4457\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4609 - val_loss: 0.4438\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4608 - val_loss: 0.4434\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4454\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4601 - val_loss: 0.4586\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4607 - val_loss: 0.4440\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4442\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4599 - val_loss: 0.4439\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4593 - val_loss: 0.4441\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(12, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_11 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66555347845455159"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_11 = np.sqrt(min(model_11.history[\"val_loss\"]))\n",
    "RMSE_model_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a wider network to evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 89us/step - loss: 1.2000 - val_loss: 0.4537\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4759 - val_loss: 0.4514\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4698 - val_loss: 0.4512\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4702 - val_loss: 0.4507\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4654 - val_loss: 0.4462\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4645 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4642 - val_loss: 0.4491\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4646 - val_loss: 0.4495\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4639 - val_loss: 0.4569\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4613 - val_loss: 0.4495\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4597 - val_loss: 0.4499\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4608 - val_loss: 0.4474\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4571 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4570 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4575 - val_loss: 0.4564\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4563 - val_loss: 0.4487\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4567 - val_loss: 0.4704\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4553 - val_loss: 0.4559\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4543 - val_loss: 0.4502\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4536 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4541 - val_loss: 0.4596\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4518 - val_loss: 0.4702\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4522 - val_loss: 0.4524\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4499 - val_loss: 0.4516\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4540 - val_loss: 0.4513\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4498 - val_loss: 0.4483\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4487 - val_loss: 0.4527\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4475 - val_loss: 0.4543\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4480 - val_loss: 0.4518\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4478 - val_loss: 0.4599\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4466 - val_loss: 0.4541\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4460 - val_loss: 0.4571\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4470 - val_loss: 0.4553\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4450 - val_loss: 0.4566\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4466 - val_loss: 0.4526\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4438 - val_loss: 0.4510\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4436 - val_loss: 0.4556\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4425 - val_loss: 0.4542\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4425 - val_loss: 0.4563\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4425 - val_loss: 0.4554\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4423 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4394 - val_loss: 0.4536\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4398 - val_loss: 0.4538\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4401 - val_loss: 0.4545\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4404 - val_loss: 0.4568\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4388 - val_loss: 0.4591\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4384 - val_loss: 0.4603\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4390 - val_loss: 0.4563\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4381 - val_loss: 0.4656\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4367 - val_loss: 0.4635\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4367 - val_loss: 0.4556\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4352 - val_loss: 0.4564\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4398 - val_loss: 0.4584\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4350 - val_loss: 0.4586\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 2s 62us/step - loss: 0.4355 - val_loss: 0.4676\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4363 - val_loss: 0.4846\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4345 - val_loss: 0.4665\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4344 - val_loss: 0.4592\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4322 - val_loss: 0.4600\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4330 - val_loss: 0.4574\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4346 - val_loss: 0.4573\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4321 - val_loss: 0.4560\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4310 - val_loss: 0.4589\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4312 - val_loss: 0.4641\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4299 - val_loss: 0.4615\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4292 - val_loss: 0.4619\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4315 - val_loss: 0.4575\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4292 - val_loss: 0.4591\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4296 - val_loss: 0.4605\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4276 - val_loss: 0.4631\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4274 - val_loss: 0.4616\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4281 - val_loss: 0.4620\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4276 - val_loss: 0.4610\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4301 - val_loss: 0.4593\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4273 - val_loss: 0.4620\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4265 - val_loss: 0.4665\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4262 - val_loss: 0.4634\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4255 - val_loss: 0.4630\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4266 - val_loss: 0.4648\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4243 - val_loss: 0.4786\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4256 - val_loss: 0.4656\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4242 - val_loss: 0.4633\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4230 - val_loss: 0.4642\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4244 - val_loss: 0.4685\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4235 - val_loss: 0.4653\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.421 - 1s 52us/step - loss: 0.4229 - val_loss: 0.4667\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4215 - val_loss: 0.4664\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4212 - val_loss: 0.4697\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4218 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4204 - val_loss: 0.4637\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4224 - val_loss: 0.4678\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4214 - val_loss: 0.4690\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4213 - val_loss: 0.4697\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4206 - val_loss: 0.4886\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4210 - val_loss: 0.4723\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4187 - val_loss: 0.4833\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4196 - val_loss: 0.4683\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4193 - val_loss: 0.4724\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4176 - val_loss: 0.4672\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4178 - val_loss: 0.4779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66800683261370652"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1] * 10, input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(120, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_12 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_12 = np.sqrt(min(model_12.history[\"val_loss\"]))\n",
    "RMSE_model_12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really not any better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can scale some of the predictors to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find which columns do not have values restricted between [0,1]\n",
    "\n",
    "select_col = (df.describe().loc[[\"max\",\"min\"],:].apply(sum,axis = 0)) != 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_condition_id',\n",
       " 'log.excl.description',\n",
       " 'excl.name',\n",
       " 'dollar.description',\n",
       " 'sale',\n",
       " 'free',\n",
       " 'save',\n",
       " 'deal',\n",
       " 'good',\n",
       " 'now',\n",
       " 'cheap',\n",
       " 'buy',\n",
       " 'excellent',\n",
       " 'great',\n",
       " 'cap.letter.brand']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_features =  df.columns[select_col].tolist() # Select the relevant columns except target\n",
    "select_features.remove(\"price\")\n",
    "select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (making features resemble z-distribution)\n",
    "stdscale = preprocessing.StandardScaler().fit(df[select_features]) # Create a preprocessing object on select features\n",
    "df_stdscale = stdscale.transform(df[select_features]) #Note that it returns a np.array with only used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.027829e-15\n",
       "1    -2.288079e-16\n",
       "2     5.450259e-15\n",
       "3     3.437801e-15\n",
       "4     8.985525e-16\n",
       "5     1.658329e-15\n",
       "6     4.924806e-17\n",
       "7     1.064549e-15\n",
       "8    -1.118217e-15\n",
       "9    -1.466933e-15\n",
       "10    1.694972e-15\n",
       "11    1.282229e-15\n",
       "12    1.338746e-16\n",
       "13   -6.921656e-16\n",
       "14   -1.552518e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_stdscale).apply(np.mean,axis = 0) # As we expected means are almost zero after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.0\n",
       "1     1.0\n",
       "2     1.0\n",
       "3     1.0\n",
       "4     1.0\n",
       "5     1.0\n",
       "6     1.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     1.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_stdscale).apply(np.std,axis = 0) # # As we expected standard deviations are 1 after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.0\n",
       "1     1.0\n",
       "2     1.0\n",
       "3     1.0\n",
       "4     1.0\n",
       "5     1.0\n",
       "6     1.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     1.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min-max scaling (making features to stay in 0-1 boundaries)\n",
    "from sklearn import preprocessing\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[select_features]) # Create the processing object\n",
    "df_minmax = minmax_scale.transform(df[select_features])\n",
    "pd.DataFrame(df_minmax).apply(max,axis =0) # As we expected, max is all 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_minmax).apply(min, axis = 0) # As we expected min is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>cheap.brands</th>\n",
       "      <th>steal</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.032003</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price      shipping  no.brand_name  fancy.categories  \\\n",
       "count  37066.000000  37066.000000   37066.000000      37066.000000   \n",
       "mean      26.581045      0.449630       0.428776          0.003480   \n",
       "std       36.188265      0.497463       0.494908          0.058892   \n",
       "min        0.000000      0.000000       0.000000          0.000000   \n",
       "25%       10.000000      0.000000       0.000000          0.000000   \n",
       "50%       17.000000      0.000000       0.000000          0.000000   \n",
       "75%       29.000000      1.000000       1.000000          0.000000   \n",
       "max     1106.000000      1.000000       1.000000          1.000000   \n",
       "\n",
       "       cheap.categories  fancy.brands  cheap.brands         steal  \\\n",
       "count      37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean           0.005126      0.009820      0.001106      0.001025   \n",
       "std            0.071413      0.098611      0.033241      0.032003   \n",
       "min            0.000000      0.000000      0.000000      0.000000   \n",
       "25%            0.000000      0.000000      0.000000      0.000000   \n",
       "50%            0.000000      0.000000      0.000000      0.000000   \n",
       "75%            0.000000      0.000000      0.000000      0.000000   \n",
       "max            1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000  \n",
       "mean        0.008229      0.008013      0.020774      0.051395  \n",
       "std         0.090339      0.089156      0.142628      0.220805  \n",
       "min         0.000000      0.000000      0.000000      0.000000  \n",
       "25%         0.000000      0.000000      0.000000      0.000000  \n",
       "50%         0.000000      0.000000      0.000000      0.000000  \n",
       "75%         0.000000      0.000000      0.000000      0.000000  \n",
       "max         1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine with other features to obtain final data sets:\n",
    "binary_features = df.drop(select_features, axis = 1)\n",
    "binary_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the standardized and scaled data framesfeatures with the binary features to give two data sets to be tested.\n",
    "binary_features = binary_features.drop(\"price\", axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 11)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_minmax = np.concatenate((binary_features,df_minmax), axis = 1)\n",
    "X_minmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stdscale = np.concatenate((binary_features,df_stdscale), axis = 1)\n",
    "X_stdscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqWOGgCR1zBCQpI4ZApLUsf8F3JGeuzUDOTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121ddacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.price.hist(bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nApLUMENAkhr2v6gh0OtofkY1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cbe67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(df.price+1), bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(df.price+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have standardized and scaled data sets ready and we can re-try training our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 4.2032 - val_loss: 1.2055\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.9293 - val_loss: 0.6881\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.6206 - val_loss: 0.5300\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5228 - val_loss: 0.4773\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4916 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4812 - val_loss: 0.4550\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4764 - val_loss: 0.4514\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4746 - val_loss: 0.4505\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4737 - val_loss: 0.4519\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4731 - val_loss: 0.4505\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4715 - val_loss: 0.4504\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4710 - val_loss: 0.4485\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4703 - val_loss: 0.4511\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4702 - val_loss: 0.4479\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4698 - val_loss: 0.4474\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4687 - val_loss: 0.4477\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4689 - val_loss: 0.4481\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4480\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4673 - val_loss: 0.4472\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4671 - val_loss: 0.4470\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4666 - val_loss: 0.4469\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4659 - val_loss: 0.4457\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4469\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4655 - val_loss: 0.4459\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4654 - val_loss: 0.4482\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4648 - val_loss: 0.4454\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4647 - val_loss: 0.4460\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4643 - val_loss: 0.4455\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4645 - val_loss: 0.4450\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4639 - val_loss: 0.4460\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4639 - val_loss: 0.4455\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4635 - val_loss: 0.4481\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4638 - val_loss: 0.4462\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4631 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4630 - val_loss: 0.4459\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4633 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4628 - val_loss: 0.4457\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4631 - val_loss: 0.4457\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4629 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4626 - val_loss: 0.4456\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4627 - val_loss: 0.4456\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4625 - val_loss: 0.4462\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4625 - val_loss: 0.4459\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4632 - val_loss: 0.4464\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4628 - val_loss: 0.4502\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4622 - val_loss: 0.4500\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4625 - val_loss: 0.4464\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4622 - val_loss: 0.4461\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4621 - val_loss: 0.4469\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4621 - val_loss: 0.4464\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4619 - val_loss: 0.4461\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4468\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4616 - val_loss: 0.4463\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4616 - val_loss: 0.4483\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4612 - val_loss: 0.4514\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4618 - val_loss: 0.4464\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4614 - val_loss: 0.4467\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4614 - val_loss: 0.4473\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4615 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4613 - val_loss: 0.4472\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4617 - val_loss: 0.4466\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4614 - val_loss: 0.4488\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4473\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4489\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4614 - val_loss: 0.4464\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4607 - val_loss: 0.4467\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4609 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4607 - val_loss: 0.4478\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4468\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4615 - val_loss: 0.4469\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4608 - val_loss: 0.4486\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4611 - val_loss: 0.4469\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4469\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4467\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4473\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4608 - val_loss: 0.4471\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4609 - val_loss: 0.4470\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4482\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4472\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4606 - val_loss: 0.4467\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4503\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4465\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4464\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4603 - val_loss: 0.4472\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4472\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4469\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4473\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4461\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4467\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4604 - val_loss: 0.4474\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4472\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4485\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4465\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4483\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4612 - val_loss: 0.4470\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4609 - val_loss: 0.4476\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4470\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4602 - val_loss: 0.4479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66694540986076201"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with X_minmax (scaled features)\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_13 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_13 = np.sqrt(min(model_13.history[\"val_loss\"]))\n",
    "RMSE_model_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was promising! Minmax scaling seems to help for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 3.9986 - val_loss: 1.0113\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.8118 - val_loss: 0.6163\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.5915 - val_loss: 0.5279\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5431 - val_loss: 0.5095\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5262 - val_loss: 0.5001\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5154 - val_loss: 0.4930\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5084 - val_loss: 0.4876\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5021 - val_loss: 0.4829\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4964 - val_loss: 0.4787\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4914 - val_loss: 0.4772\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4871 - val_loss: 0.4721\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4835 - val_loss: 0.4678\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4798 - val_loss: 0.4691\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4779 - val_loss: 0.4633\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4754 - val_loss: 0.4606\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4736 - val_loss: 0.4599\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4711 - val_loss: 0.4604\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4692 - val_loss: 0.4602\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4685 - val_loss: 0.4587\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4560\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4662 - val_loss: 0.4590\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4653 - val_loss: 0.4562\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4650 - val_loss: 0.4571\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4644 - val_loss: 0.4567\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4645 - val_loss: 0.4572\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4634 - val_loss: 0.4548\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4632 - val_loss: 0.4557\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4627 - val_loss: 0.4551\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4624 - val_loss: 0.4555\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4619 - val_loss: 0.4558\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4613 - val_loss: 0.4551\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4559\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4616 - val_loss: 0.4551\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4607 - val_loss: 0.4532\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4605 - val_loss: 0.4542\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4600 - val_loss: 0.4544\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4558\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4602 - val_loss: 0.4532\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4598 - val_loss: 0.4548\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4592 - val_loss: 0.4532\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4590 - val_loss: 0.4532\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4592 - val_loss: 0.4530\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4592 - val_loss: 0.4529\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4589 - val_loss: 0.4566\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4585 - val_loss: 0.4553\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4583 - val_loss: 0.4556\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4581 - val_loss: 0.4543\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4586 - val_loss: 0.4525\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4583 - val_loss: 0.4547\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4579 - val_loss: 0.4544\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4576 - val_loss: 0.4530\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4572 - val_loss: 0.4539\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4574 - val_loss: 0.4539\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4574 - val_loss: 0.4575\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4580 - val_loss: 0.4535\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4569 - val_loss: 0.4520\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4570 - val_loss: 0.4546\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4573 - val_loss: 0.4530\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4569 - val_loss: 0.4528\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4571 - val_loss: 0.4520\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4572 - val_loss: 0.4544\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4564 - val_loss: 0.4540\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4566 - val_loss: 0.4558\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4569 - val_loss: 0.4532\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4557 - val_loss: 0.4553\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4564 - val_loss: 0.4538\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4558 - val_loss: 0.4561\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4564 - val_loss: 0.4526\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4568 - val_loss: 0.4536\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4557 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4559 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4563 - val_loss: 0.4523\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4558 - val_loss: 0.4521\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4563 - val_loss: 0.4519\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4562 - val_loss: 0.4534\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4539\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4557 - val_loss: 0.4520\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4556 - val_loss: 0.4539\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4554 - val_loss: 0.4533\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4556 - val_loss: 0.4541\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4551 - val_loss: 0.4520\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4556 - val_loss: 0.4545\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4554 - val_loss: 0.4538\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4526\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4556 - val_loss: 0.4548\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4551 - val_loss: 0.4521\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4553 - val_loss: 0.4536\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4549 - val_loss: 0.4529\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4553 - val_loss: 0.4525\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4548 - val_loss: 0.4538\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4556 - val_loss: 0.4539\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4551 - val_loss: 0.4542\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4553 - val_loss: 0.4525\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4547 - val_loss: 0.4525\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4553 - val_loss: 0.4534\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4553 - val_loss: 0.4544\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4550 - val_loss: 0.4524\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4546 - val_loss: 0.4557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6722402842940296"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_stdscale.shape[1], input_dim = X_stdscale.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_14 = model.fit(X_stdscale,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_14 = np.sqrt(min(model_14.history[\"val_loss\"]))\n",
    "RMSE_model_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 3.0712 - val_loss: 0.5992\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.5269 - val_loss: 0.4636\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4811 - val_loss: 0.4525\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4736 - val_loss: 0.4504\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4717 - val_loss: 0.4485\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4704 - val_loss: 0.4486\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4480\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4693 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4683 - val_loss: 0.4526\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4681 - val_loss: 0.4488\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4672 - val_loss: 0.4514\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4664 - val_loss: 0.4469\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4656 - val_loss: 0.4460\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4656 - val_loss: 0.4515\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4660 - val_loss: 0.4515\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4654 - val_loss: 0.4478\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4655 - val_loss: 0.4491\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4518\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4503\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4649 - val_loss: 0.4473\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4643 - val_loss: 0.4505\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4642 - val_loss: 0.4460\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4643 - val_loss: 0.4465\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4642 - val_loss: 0.4467\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4644 - val_loss: 0.4464\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4486\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4643 - val_loss: 0.4468\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4635 - val_loss: 0.4463\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4637 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4640 - val_loss: 0.4501\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4635 - val_loss: 0.4587\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4639 - val_loss: 0.4474\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4630 - val_loss: 0.4457\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4637 - val_loss: 0.4465\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4630 - val_loss: 0.4479\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4634 - val_loss: 0.4456\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4631 - val_loss: 0.4472\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4632 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4631 - val_loss: 0.4458\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4488\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4628 - val_loss: 0.4463\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4632 - val_loss: 0.4470\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4637 - val_loss: 0.4466\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4632 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4623 - val_loss: 0.4476\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4633 - val_loss: 0.4450\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4626 - val_loss: 0.4445\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4627 - val_loss: 0.4465\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4615 - val_loss: 0.4548\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4622 - val_loss: 0.4470\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4621 - val_loss: 0.4449\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4627 - val_loss: 0.4453\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4617 - val_loss: 0.4466\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4614 - val_loss: 0.4495\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4625 - val_loss: 0.4453\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4610 - val_loss: 0.4453\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4613 - val_loss: 0.4459\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4610 - val_loss: 0.4444\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4609 - val_loss: 0.4469\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4448\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4604 - val_loss: 0.4505\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4609 - val_loss: 0.4450\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4603 - val_loss: 0.4489\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4604 - val_loss: 0.4450\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4598 - val_loss: 0.4448\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4603 - val_loss: 0.4453\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4451\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4600 - val_loss: 0.4446\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4611 - val_loss: 0.4446\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4598 - val_loss: 0.4451\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4511\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4601 - val_loss: 0.4446\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4596 - val_loss: 0.4450\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4595 - val_loss: 0.4454\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4597 - val_loss: 0.4443\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4591 - val_loss: 0.4455\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4454\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4587 - val_loss: 0.4446\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4591 - val_loss: 0.4494\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4590 - val_loss: 0.4451\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4591 - val_loss: 0.4445\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4589 - val_loss: 0.4440\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4595 - val_loss: 0.4471\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4589 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4590 - val_loss: 0.4471\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4586 - val_loss: 0.4442\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4590 - val_loss: 0.4447\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4587 - val_loss: 0.4461\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4592 - val_loss: 0.4447\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4588 - val_loss: 0.4450\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4585 - val_loss: 0.4487\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4590 - val_loss: 0.4550\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4594 - val_loss: 0.4445\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4588 - val_loss: 0.4443\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4587 - val_loss: 0.4446\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4579 - val_loss: 0.4449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66633644156462402"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue with X_minmax (scaled features), slighly increasing model complexity\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_15 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_15 = np.sqrt(min(model_15.history[\"val_loss\"]))\n",
    "RMSE_model_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's getting better! Continue increasing model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 2.3449 - val_loss: 0.4819\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4852 - val_loss: 0.4507\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4734 - val_loss: 0.4493\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4706 - val_loss: 0.4488\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4693 - val_loss: 0.4465\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4685 - val_loss: 0.4479\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4679 - val_loss: 0.4522\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4696 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4684 - val_loss: 0.4489\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4678 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4673 - val_loss: 0.4473\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4664 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4654 - val_loss: 0.4448\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4647 - val_loss: 0.4547\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4659 - val_loss: 0.4490\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4655 - val_loss: 0.4485\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4649 - val_loss: 0.4549\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4640 - val_loss: 0.4557\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4463\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4635 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4458\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4636 - val_loss: 0.4456\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4629 - val_loss: 0.4445\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4636 - val_loss: 0.4460\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4624 - val_loss: 0.4521\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4638 - val_loss: 0.4456\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4629 - val_loss: 0.4463\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4620 - val_loss: 0.4445\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4628 - val_loss: 0.4490\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4631 - val_loss: 0.4461\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4616 - val_loss: 0.4549\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4624 - val_loss: 0.4479\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4614 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4624 - val_loss: 0.4458\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4606 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4613 - val_loss: 0.4456\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4608 - val_loss: 0.4482\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4615 - val_loss: 0.4507\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4453\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4630 - val_loss: 0.4455\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4614 - val_loss: 0.4457\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4614 - val_loss: 0.4461\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4604 - val_loss: 0.4473\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4616 - val_loss: 0.4464\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4610 - val_loss: 0.4457\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4613 - val_loss: 0.4498\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4598 - val_loss: 0.4597\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4458\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4610 - val_loss: 0.4456\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4619 - val_loss: 0.4475\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4489\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4601 - val_loss: 0.4542\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4616 - val_loss: 0.4455\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4597 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4598 - val_loss: 0.4462\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4594 - val_loss: 0.4444\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4595 - val_loss: 0.4457\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4606 - val_loss: 0.4458\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4591 - val_loss: 0.4462\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4598 - val_loss: 0.4465\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4592 - val_loss: 0.4494\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4588 - val_loss: 0.4452\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4585 - val_loss: 0.4455\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4592 - val_loss: 0.4472\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4586 - val_loss: 0.4453\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4592 - val_loss: 0.4474\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4601 - val_loss: 0.4449\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4588 - val_loss: 0.4467\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4592 - val_loss: 0.4530\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4587 - val_loss: 0.4454\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4592 - val_loss: 0.4453\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4589 - val_loss: 0.4449\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4585 - val_loss: 0.4474\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4590 - val_loss: 0.4467\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4581 - val_loss: 0.4469\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4585 - val_loss: 0.4459\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4572 - val_loss: 0.4468\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4577 - val_loss: 0.4467\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4567 - val_loss: 0.4480\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4571 - val_loss: 0.4497\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4567 - val_loss: 0.4457\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4570 - val_loss: 0.4463\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4564 - val_loss: 0.4444\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4569 - val_loss: 0.4495\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4559 - val_loss: 0.4525\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4561 - val_loss: 0.4457\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4554 - val_loss: 0.4455\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4565 - val_loss: 0.4453\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4556 - val_loss: 0.4455\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4560 - val_loss: 0.4454\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4559 - val_loss: 0.4454\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4497\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4549 - val_loss: 0.4664\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4563 - val_loss: 0.4449\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4553 - val_loss: 0.4458\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4549 - val_loss: 0.4452\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4540 - val_loss: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6665976297688434"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1]*2, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1]*2, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_16 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_16 = np.sqrt(min(model_16.history[\"val_loss\"]))\n",
    "RMSE_model_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we hit the model capacity again. Let's try another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 2.5496 - val_loss: 0.4932\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4890 - val_loss: 0.4526\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4753 - val_loss: 0.4499\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4718 - val_loss: 0.4510\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4706 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4698 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4574\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4723 - val_loss: 0.4477\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4706 - val_loss: 0.4511\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4702 - val_loss: 0.4524\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4692 - val_loss: 0.4484\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4696 - val_loss: 0.4471\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4682 - val_loss: 0.4503\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4677 - val_loss: 0.4478\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4688 - val_loss: 0.4489\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4686 - val_loss: 0.4557\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4679 - val_loss: 0.4640\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4671 - val_loss: 0.4495\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4538\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4677 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4666 - val_loss: 0.4483\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4673 - val_loss: 0.4470\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4667 - val_loss: 0.4466\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4656 - val_loss: 0.4465\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4672 - val_loss: 0.4466\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4650 - val_loss: 0.4539\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4668 - val_loss: 0.4496\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4661 - val_loss: 0.4482\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4647 - val_loss: 0.4464\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4657 - val_loss: 0.4503\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4660 - val_loss: 0.4488\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4652 - val_loss: 0.4483\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4652 - val_loss: 0.4509\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4460\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4653 - val_loss: 0.4464\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4635 - val_loss: 0.4461\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4637 - val_loss: 0.4462\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4637 - val_loss: 0.4491\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4643 - val_loss: 0.4552\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4642 - val_loss: 0.4467\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4662 - val_loss: 0.4467\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4639 - val_loss: 0.4473\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4474\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4640 - val_loss: 0.4478\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4635 - val_loss: 0.4482\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4643 - val_loss: 0.4472\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4648 - val_loss: 0.4460\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4638 - val_loss: 0.4511\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4546\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4630 - val_loss: 0.4467\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4636 - val_loss: 0.4466\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4653 - val_loss: 0.4507\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4646 - val_loss: 0.4500\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4634 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4639 - val_loss: 0.4465\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4633 - val_loss: 0.4476\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4628 - val_loss: 0.4471\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4625 - val_loss: 0.4479\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4634 - val_loss: 0.4464\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4468\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4627 - val_loss: 0.4468\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4629 - val_loss: 0.4481\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4626 - val_loss: 0.4544\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4625 - val_loss: 0.4461\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4622 - val_loss: 0.4469\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4633 - val_loss: 0.4490\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4624 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4636 - val_loss: 0.4508\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4635 - val_loss: 0.4466\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4625 - val_loss: 0.4482\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4635 - val_loss: 0.4542\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4632 - val_loss: 0.4460\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4633 - val_loss: 0.4470\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4628 - val_loss: 0.4462\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4630 - val_loss: 0.4466\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4629 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4628 - val_loss: 0.4500\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4474\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4622 - val_loss: 0.4476\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4626 - val_loss: 0.4468\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4618 - val_loss: 0.4477\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4624 - val_loss: 0.4489\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4614 - val_loss: 0.4462\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4624 - val_loss: 0.4502\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4618 - val_loss: 0.4454\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4612 - val_loss: 0.4466\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4603 - val_loss: 0.4557\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4612 - val_loss: 0.4480\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4602 - val_loss: 0.4457\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4615 - val_loss: 0.4459\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4605 - val_loss: 0.4463\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4599 - val_loss: 0.4459\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4611 - val_loss: 0.4458\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4598 - val_loss: 0.4475\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4591 - val_loss: 0.4721\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4605 - val_loss: 0.4468\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4596 - val_loss: 0.4469\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4588 - val_loss: 0.4464\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4581 - val_loss: 0.4462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66741381955167611"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_17 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_17 = np.sqrt(min(model_17.history[\"val_loss\"]))\n",
    "RMSE_model_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 1.3357 - val_loss: 0.4499\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4709 - val_loss: 0.4489\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4696 - val_loss: 0.4492\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4716 - val_loss: 0.4545\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4681 - val_loss: 0.4481\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4665 - val_loss: 0.4571\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4684 - val_loss: 0.4472\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4702 - val_loss: 0.4586\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4663 - val_loss: 0.4493\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4666 - val_loss: 0.4503\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4670 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4648 - val_loss: 0.4453\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4637 - val_loss: 0.4496\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4643 - val_loss: 0.4520\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4639 - val_loss: 0.4495\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4644 - val_loss: 0.4689\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4627 - val_loss: 0.4515\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4631 - val_loss: 0.4470\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4641 - val_loss: 0.4456\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4627 - val_loss: 0.4581\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4609 - val_loss: 0.4554\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4633 - val_loss: 0.4516\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4595 - val_loss: 0.4474\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4639 - val_loss: 0.4477\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4596 - val_loss: 0.4458\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4607 - val_loss: 0.4491\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4604 - val_loss: 0.4477\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4594 - val_loss: 0.4449\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4598 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4605 - val_loss: 0.4538\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4592 - val_loss: 0.4472\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4591 - val_loss: 0.4486\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4588 - val_loss: 0.4531\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4610 - val_loss: 0.4452\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4571 - val_loss: 0.4456\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4585 - val_loss: 0.4447\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4573 - val_loss: 0.4509\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4581 - val_loss: 0.4503\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4581 - val_loss: 0.4455\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4574 - val_loss: 0.4470\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4560 - val_loss: 0.4481\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4570 - val_loss: 0.4469\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4566 - val_loss: 0.4486\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4574 - val_loss: 0.4468\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4560 - val_loss: 0.4487\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4557 - val_loss: 0.4502\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4572 - val_loss: 0.4465\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4552 - val_loss: 0.4555\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4550 - val_loss: 0.4492\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4545 - val_loss: 0.4504\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4535 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.456 - 1s 42us/step - loss: 0.4571 - val_loss: 0.4500\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4543 - val_loss: 0.4468\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4541 - val_loss: 0.4505\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4538 - val_loss: 0.4551\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4534 - val_loss: 0.4506\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4515 - val_loss: 0.4471\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4510 - val_loss: 0.4492\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4509 - val_loss: 0.4481\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4524 - val_loss: 0.4465\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4498 - val_loss: 0.4471\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4493 - val_loss: 0.4469\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4489 - val_loss: 0.4577\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4481 - val_loss: 0.4477\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4469 - val_loss: 0.4476\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4488 - val_loss: 0.4462\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4462 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4479 - val_loss: 0.4486\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4462 - val_loss: 0.4504\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4457 - val_loss: 0.4480\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4463 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4450 - val_loss: 0.4505\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4459 - val_loss: 0.4482\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4449 - val_loss: 0.4494\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4438 - val_loss: 0.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4440 - val_loss: 0.4499\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4429 - val_loss: 0.4549\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4441 - val_loss: 0.4535\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4420 - val_loss: 0.4642\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4423 - val_loss: 0.4532\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4420 - val_loss: 0.4545\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4416 - val_loss: 0.4517\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4412 - val_loss: 0.4532\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4408 - val_loss: 0.4520\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4411 - val_loss: 0.4519\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4401 - val_loss: 0.4539\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4391 - val_loss: 0.4602\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4393 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4380 - val_loss: 0.4531\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4392 - val_loss: 0.4546\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4398 - val_loss: 0.4539\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4384 - val_loss: 0.4544\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4391 - val_loss: 0.4694\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4378 - val_loss: 0.4526\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4368 - val_loss: 0.4836\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4382 - val_loss: 0.4552\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4371 - val_loss: 0.4560\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4356 - val_loss: 0.4530\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4351 - val_loss: 0.4589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66687018587000157"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(100, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(100, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_18 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_18 = np.sqrt(min(model_18.history[\"val_loss\"]))\n",
    "RMSE_model_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/10\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 1.0771 - val_loss: 0.4512\n",
      "Epoch 2/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4770 - val_loss: 0.4632\n",
      "Epoch 3/10\n",
      "25946/25946 [==============================] - 2s 69us/step - loss: 0.4751 - val_loss: 0.4510\n",
      "Epoch 4/10\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4734 - val_loss: 0.4515\n",
      "Epoch 5/10\n",
      "25946/25946 [==============================] - 2s 76us/step - loss: 0.4726 - val_loss: 0.4470\n",
      "Epoch 6/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4711 - val_loss: 0.4505\n",
      "Epoch 7/10\n",
      "25946/25946 [==============================] - 2s 69us/step - loss: 0.4704 - val_loss: 0.4613\n",
      "Epoch 8/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4699 - val_loss: 0.4468\n",
      "Epoch 9/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4692 - val_loss: 0.4544\n",
      "Epoch 10/10\n",
      "25946/25946 [==============================] - 2s 63us/step - loss: 0.4676 - val_loss: 0.4505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66841383692321843"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the batch and only performing 10 epochs\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1]*2, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1]*2, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_19 = model.fit(X_minmax,y,epochs = 10,validation_split= 0.3)\n",
    "RMSE_model_19 = np.sqrt(min(model_19.history[\"val_loss\"]))\n",
    "RMSE_model_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take home message\n",
    "\n",
    "For this data set, minmax scaling of the data helped only a little bit for training a better model. We will save this version of the predictors to continue model tuning as we learn different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_minmax).to_csv(\"mini_subtrain_X_minmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).to_csv(\"mini_subtrain_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking of alternative approaches\n",
    "\n",
    "Perhaps we can try to re-frame the problem, particularly focusing on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"mini_subtrain_y.csv\", index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12608f2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y back to original scale\n",
    "y_origin = np.exp(y)-1\n",
    "plt.hist(y_origin,bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1106.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clean = y_origin[np.logical_and(y_origin != 0, y_origin <400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAFJ6pghIEkdMwQkqWP/D+FEm3u3Q2W+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124ba5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_clean,bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we also scale the y before training the model in the same way we did for the predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "y = np.exp(y)-1 # We first get into the normal scale (price) from log.price\n",
    "minmax_scaler_y = MinMaxScaler().fit(y)\n",
    "y_minmax = minmax_scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6hgOkqSO4SBJ6vw/jQVruU3ZRjcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ff29fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_minmax, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distribution is exactly the same, except the data is bound between 0 and 1. Let's train a new model to see if this scaling makes any difference in model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"mini_subtrain_X_minmax.csv\", index_col= 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 1s 43us/step - loss: 0.0017 - val_loss: 8.2608e-04\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.0011 - val_loss: 7.8011e-04\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.0011 - val_loss: 7.6305e-04\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.5945e-04\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 7.5731e-04\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.6652e-04\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.5633e-04\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 7.4912e-04\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 7.5205e-04\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.0010 - val_loss: 7.5096e-04\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.0010 - val_loss: 7.4932e-04\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 7.4861e-04\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.0010 - val_loss: 7.5650e-04\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.9756e-04 - val_loss: 7.5135e-04\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.9488e-04 - val_loss: 7.6336e-04\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.9188e-04 - val_loss: 7.7025e-04\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.9316e-04 - val_loss: 7.5240e-04\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.9169e-04 - val_loss: 7.6147e-04\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8985e-04 - val_loss: 7.5685e-04\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.9142e-04 - val_loss: 7.4342e-04\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.9029e-04 - val_loss: 7.5710e-04\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.8590e-04 - val_loss: 7.6199e-04\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.8795e-04 - val_loss: 7.5097e-04\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.8777e-04 - val_loss: 7.6290e-04\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.8559e-04 - val_loss: 7.5260e-04\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.8629e-04 - val_loss: 7.4988e-04\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.8418e-04 - val_loss: 7.5650e-04\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.8314e-04 - val_loss: 7.5942e-04\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8263e-04 - val_loss: 7.5499e-04\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.8136e-04 - val_loss: 7.6330e-04\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8147e-04 - val_loss: 7.5576e-04\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.8026e-04 - val_loss: 7.5556e-04\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.8188e-04 - val_loss: 7.4969e-04\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.8272e-04 - val_loss: 7.5923e-04\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7984e-04 - val_loss: 7.6242e-04\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.7816e-04 - val_loss: 7.6047e-04\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.8111e-04 - val_loss: 7.5372e-04\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.7881e-04 - val_loss: 7.5441e-04\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.7944e-04 - val_loss: 7.5296e-04\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7708e-04 - val_loss: 7.6502e-04\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 9.7539e-04 - val_loss: 7.4992e-04\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7600e-04 - val_loss: 7.6425e-04\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7572e-04 - val_loss: 7.5909e-04\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7489e-04 - val_loss: 7.6842e-04\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.7627e-04 - val_loss: 7.7012e-04\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.7474e-04 - val_loss: 7.5925e-04\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7408e-04 - val_loss: 7.6964e-04\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7555e-04 - val_loss: 7.5430e-04\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7206e-04 - val_loss: 7.7962e-04\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7332e-04 - val_loss: 7.7954e-04\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7512e-04 - val_loss: 7.6322e-04\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7125e-04 - val_loss: 7.6754e-04\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 9.7210e-04 - val_loss: 7.6268e-04\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 9.7643e-04 - val_loss: 7.6646e-04\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 9.6932e-04 - val_loss: 7.5783e-04\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7162e-04 - val_loss: 7.5393e-04\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7167e-04 - val_loss: 7.6766e-04\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.7198e-04 - val_loss: 7.7110e-04\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7116e-04 - val_loss: 7.6145e-04\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7135e-04 - val_loss: 7.6163e-04\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6992e-04 - val_loss: 7.7884e-04\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 9.6951e-04 - val_loss: 7.6217e-04\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6897e-04 - val_loss: 7.6869e-04\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 9.7386e-04 - val_loss: 7.8255e-04\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6828e-04 - val_loss: 7.6289e-04\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7143e-04 - val_loss: 7.6894e-04\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6709e-04 - val_loss: 7.7155e-04\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6665e-04 - val_loss: 7.6693e-04\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6788e-04 - val_loss: 7.6327e-04\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6517e-04 - val_loss: 7.6484e-04\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6605e-04 - val_loss: 7.8005e-04\n",
      "Epoch 72/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6961e-04 - val_loss: 7.6278e-04\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.6190e-04 - val_loss: 8.3903e-04\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.7426e-04 - val_loss: 7.6857e-04\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6727e-04 - val_loss: 7.6418e-04\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6684e-04 - val_loss: 7.6845e-04\n",
      "Epoch 77/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.6386e-04 - val_loss: 7.7303e-04\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.6340e-04 - val_loss: 7.9560e-04\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 9.6448e-04 - val_loss: 7.7746e-04\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.6174e-04 - val_loss: 7.9979e-04\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.6415e-04 - val_loss: 7.7052e-04\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6019e-04 - val_loss: 7.8271e-04\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5944e-04 - val_loss: 7.7734e-04\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6078e-04 - val_loss: 7.7717e-04\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.6039e-04 - val_loss: 7.7097e-04\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.5824e-04 - val_loss: 7.5623e-04\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.5879e-04 - val_loss: 7.6081e-04\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5843e-04 - val_loss: 7.7443e-04\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.5778e-04 - val_loss: 7.7814e-04\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5866e-04 - val_loss: 7.8373e-04\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.5902e-04 - val_loss: 7.7850e-04\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5656e-04 - val_loss: 7.6994e-04\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5729e-04 - val_loss: 7.8387e-04\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5892e-04 - val_loss: 7.7847e-04\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 9.5518e-04 - val_loss: 7.6901e-04\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 9.5567e-04 - val_loss: 7.8834e-04\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 9.5188e-04 - val_loss: 7.7806e-04\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5487e-04 - val_loss: 7.7666e-04\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 9.5689e-04 - val_loss: 7.6787e-04\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 9.5402e-04 - val_loss: 7.6998e-04\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model_20 = model.fit(X,y_minmax, epochs = 100, batch_size= 100,validation_split= 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the predictions of the model\n",
    "model_20_pred = model_20.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02461544],\n",
       "       [ 0.0167399 ],\n",
       "       [ 0.00623888],\n",
       "       ..., \n",
       "       [ 0.02530498],\n",
       "       [ 0.020853  ],\n",
       "       [ 0.03319128]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_20_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxOBORJRC/x/mX34WF6zEEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124e62198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x = model_20_pred, y = y_minmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the model also suffers from predicting the outlier price values! We also note that the predictions come back more gaussian compared to the target variable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyJ2lAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124d2d7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model_20_pred,bins =100,color= \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevethless, less inverse transform the predictions and try to measure RMSE, we will use the same transformer object we used to transform y for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20_pred_rescaled = minmax_scaler_y.inverse_transform(model_20_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127451a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model_20_pred_rescaled, bins = 100) #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3739844235793084"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y,model_20_pred_rescaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our RMSE is actually much higher in this case! Therefore, it did not helped to train the model in this way. \n",
    "\n",
    "    How about we log transform the min_max scaled y to make it more gaussian before fitting the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 1s 44us/step - loss: 0.0014 - val_loss: 6.8909e-04\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 8.3044e-04 - val_loss: 6.4429e-04\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.9660e-04 - val_loss: 6.2674e-04\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.8311e-04 - val_loss: 6.2217e-04\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.7745e-04 - val_loss: 6.1944e-04\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.7010e-04 - val_loss: 6.2782e-04\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.6850e-04 - val_loss: 6.1771e-04\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.6211e-04 - val_loss: 6.1328e-04\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 7.5825e-04 - val_loss: 6.1377e-04\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.5420e-04 - val_loss: 6.1273e-04\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.5066e-04 - val_loss: 6.1260e-04\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.4840e-04 - val_loss: 6.1171e-04\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 7.4850e-04 - val_loss: 6.1894e-04\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.4494e-04 - val_loss: 6.1302e-04\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.4272e-04 - val_loss: 6.2130e-04\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.4097e-04 - val_loss: 6.2519e-04\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 7.4047e-04 - val_loss: 6.1376e-04\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3850e-04 - val_loss: 6.2023e-04\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.3756e-04 - val_loss: 6.1946e-04\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3797e-04 - val_loss: 6.0835e-04\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3688e-04 - val_loss: 6.1829e-04\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3334e-04 - val_loss: 6.2253e-04\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3498e-04 - val_loss: 6.1608e-04\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.3545e-04 - val_loss: 6.2251e-04\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.3342e-04 - val_loss: 6.1440e-04\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.3368e-04 - val_loss: 6.1373e-04\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.3132e-04 - val_loss: 6.1855e-04\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3093e-04 - val_loss: 6.1830e-04\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.3048e-04 - val_loss: 6.1539e-04\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2913e-04 - val_loss: 6.2102e-04\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2946e-04 - val_loss: 6.1573e-04\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2793e-04 - val_loss: 6.1848e-04\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2893e-04 - val_loss: 6.1250e-04\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2972e-04 - val_loss: 6.2223e-04\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2886e-04 - val_loss: 6.2077e-04\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2627e-04 - val_loss: 6.1882e-04\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2766e-04 - val_loss: 6.1613e-04\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 7.2570e-04 - val_loss: 6.1596e-04\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.2729e-04 - val_loss: 6.1347e-04\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2508e-04 - val_loss: 6.2233e-04\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2450e-04 - val_loss: 6.1210e-04\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.2524e-04 - val_loss: 6.2480e-04\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 7.2444e-04 - val_loss: 6.1734e-04\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.2432e-04 - val_loss: 6.2600e-04\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2499e-04 - val_loss: 6.2609e-04\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.2406e-04 - val_loss: 6.1848e-04\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2428e-04 - val_loss: 6.2566e-04\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 7.2497e-04 - val_loss: 6.1588e-04\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2271e-04 - val_loss: 6.3479e-04\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.2248e-04 - val_loss: 6.3195e-04\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2357e-04 - val_loss: 6.2100e-04\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2160e-04 - val_loss: 6.2320e-04\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2253e-04 - val_loss: 6.2119e-04\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2547e-04 - val_loss: 6.2281e-04\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2085e-04 - val_loss: 6.1915e-04\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2076e-04 - val_loss: 6.1616e-04\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2210e-04 - val_loss: 6.2763e-04\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2218e-04 - val_loss: 6.3242e-04\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2143e-04 - val_loss: 6.2316e-04\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 7.2158e-04 - val_loss: 6.2115e-04\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.2036e-04 - val_loss: 6.3406e-04\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.2019e-04 - val_loss: 6.2169e-04\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1958e-04 - val_loss: 6.2643e-04\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.2266e-04 - val_loss: 6.3831e-04\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1968e-04 - val_loss: 6.2210e-04\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.2044e-04 - val_loss: 6.2471e-04\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.1776e-04 - val_loss: 6.2275e-04\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1778e-04 - val_loss: 6.2830e-04\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1882e-04 - val_loss: 6.2175e-04\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1648e-04 - val_loss: 6.2535e-04\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1765e-04 - val_loss: 6.2968e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1905e-04 - val_loss: 6.2238e-04\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.1409e-04 - val_loss: 6.6533e-04\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.2287e-04 - val_loss: 6.2740e-04\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 7.1732e-04 - val_loss: 6.2746e-04\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1810e-04 - val_loss: 6.3010e-04\n",
      "Epoch 77/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1694e-04 - val_loss: 6.2972e-04\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1552e-04 - val_loss: 6.4109e-04\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1686e-04 - val_loss: 6.3289e-04\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1568e-04 - val_loss: 6.4260e-04\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1731e-04 - val_loss: 6.2749e-04\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 7.1453e-04 - val_loss: 6.3641e-04\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1462e-04 - val_loss: 6.3191e-04\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1540e-04 - val_loss: 6.3012e-04\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 7.1464e-04 - val_loss: 6.2538e-04\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1381e-04 - val_loss: 6.1843e-04\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1418e-04 - val_loss: 6.1959e-04\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1421e-04 - val_loss: 6.3247e-04\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1362e-04 - val_loss: 6.3165e-04\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1391e-04 - val_loss: 6.3788e-04\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 7.1403e-04 - val_loss: 6.3229e-04\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 7.1218e-04 - val_loss: 6.2961e-04\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 7.1200e-04 - val_loss: 6.3618e-04\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 7.1329e-04 - val_loss: 6.3317e-04\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1173e-04 - val_loss: 6.2756e-04\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1194e-04 - val_loss: 6.3673e-04\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 7.0831e-04 - val_loss: 6.3377e-04\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1092e-04 - val_loss: 6.3502e-04\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 7.1144e-04 - val_loss: 6.2572e-04\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 7.1049e-04 - val_loss: 6.2944e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.474969229500143"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model_21 = model.fit(X,np.log(y_minmax+1), epochs = 100, batch_size= 100,validation_split= 0.4)\n",
    "# We get the predictions of the model\n",
    "model_21_pred = model_21.model.predict(X)\n",
    "# First exponentiate back\n",
    "model_21_pred = np.exp(model_21_pred)-1\n",
    "# Then inverse transform\n",
    "model_21_pred_rescaled = minmax_scaler_y.inverse_transform(model_21_pred)\n",
    "# Calculate test RMSE\n",
    "np.sqrt(mean_squared_error(y,model_21_pred_rescaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turned as not so helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reframing the problem once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = pd.read_csv(\"mini_subtrain_y.csv\",index_col=0)\n",
    " X = pd.read_csv(\"mini_subtrain_X_minmax.csv\",index_col= 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.433987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.154858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.135494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price\n",
       "1  2.197225\n",
       "2  3.688879\n",
       "3  3.433987\n",
       "4  6.154858\n",
       "5  3.135494"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    ...      16   17   18  \\\n",
       "0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0  0.0   \n",
       "1  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0  0.0   \n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0  0.0   \n",
       "3  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0  0.0   \n",
       "\n",
       "    19   20   21   22   23   24        25  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.058824  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.058824  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.352941  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.117647  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.price = np.exp(y.price)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12869cc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.price.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128b589e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.price[y.price <=200].hist(bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126956c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(y.price[y.price <=200]+1),bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the majority of the items are priced between 0-200$.\n",
    "\n",
    "1. Can we focus our model on this range to make predictions? \n",
    "2. Can we turn the problem into classification + regression? \n",
    "        e.g: first classify the items that are priced as 0 (and perhaps $1), then perform regression on the rest of the items. \n",
    "        \n",
    "Let's get started with the first idea:\n",
    "\n",
    "#### 1. Can we focus our model on this range to make predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36821,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_200 = np.log(y.price[y.price <=200].values+1)\n",
    "y_200.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=37066, step=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36821, 26)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_200 = X[y.price <=200]\n",
    "X_200.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 5.0627 - val_loss: 1.1569\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.7914 - val_loss: 0.5607\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.5198 - val_loss: 0.4617\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4647 - val_loss: 0.4380\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4504 - val_loss: 0.4308\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4449 - val_loss: 0.4275\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4425 - val_loss: 0.4255\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4405 - val_loss: 0.4247\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4396 - val_loss: 0.4256\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4393 - val_loss: 0.4235\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4392 - val_loss: 0.4234\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4387 - val_loss: 0.4246\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4385 - val_loss: 0.4231\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4381 - val_loss: 0.4225\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4377 - val_loss: 0.4230\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4378 - val_loss: 0.4236\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4379 - val_loss: 0.4223\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4372 - val_loss: 0.4243\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4372 - val_loss: 0.4224\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4369 - val_loss: 0.4220\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4364 - val_loss: 0.4229\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4366 - val_loss: 0.4228\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4362 - val_loss: 0.4218\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 0s 21us/step - loss: 0.4361 - val_loss: 0.4216\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4355 - val_loss: 0.4252\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4360 - val_loss: 0.4222\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4359 - val_loss: 0.4241\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4361 - val_loss: 0.4216\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4355 - val_loss: 0.4244\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4356 - val_loss: 0.4216\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4355 - val_loss: 0.4226\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4350 - val_loss: 0.4219\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4352 - val_loss: 0.4229\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4346 - val_loss: 0.4227\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4343 - val_loss: 0.4222\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4344 - val_loss: 0.4237\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4347 - val_loss: 0.4218\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4341 - val_loss: 0.4241\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4339 - val_loss: 0.4251\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4349 - val_loss: 0.4216\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4339 - val_loss: 0.4215\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4338 - val_loss: 0.4226\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4334 - val_loss: 0.4222\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4338 - val_loss: 0.4220\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4335 - val_loss: 0.4259\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4333 - val_loss: 0.4237\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4333 - val_loss: 0.4231\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4328 - val_loss: 0.4217\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4328 - val_loss: 0.4224\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4327 - val_loss: 0.4214\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4324 - val_loss: 0.4212\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4323 - val_loss: 0.4213\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 0s 23us/step - loss: 0.4323 - val_loss: 0.4210\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4324 - val_loss: 0.4215\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 0s 23us/step - loss: 0.4324 - val_loss: 0.4217\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4325 - val_loss: 0.4224\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4326 - val_loss: 0.4220\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4322 - val_loss: 0.4234\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4323 - val_loss: 0.4216\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4318 - val_loss: 0.4210\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4317 - val_loss: 0.4218\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4321 - val_loss: 0.4210\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4316 - val_loss: 0.4223\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4314 - val_loss: 0.4216\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4313 - val_loss: 0.4210\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4315 - val_loss: 0.4216\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4321 - val_loss: 0.4209\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4315 - val_loss: 0.4216\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4315 - val_loss: 0.4217\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4313 - val_loss: 0.4216\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4315 - val_loss: 0.4210\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4314 - val_loss: 0.4207\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4312 - val_loss: 0.4206\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4315 - val_loss: 0.4212\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4312 - val_loss: 0.4216\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4309 - val_loss: 0.4211\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4315 - val_loss: 0.4210\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4311 - val_loss: 0.4206\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4308 - val_loss: 0.4208\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4308 - val_loss: 0.4206\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4313 - val_loss: 0.4236\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4311 - val_loss: 0.4222\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4309 - val_loss: 0.4217\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4305 - val_loss: 0.4226\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4307 - val_loss: 0.4244\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4310 - val_loss: 0.4213\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4304 - val_loss: 0.4205\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4301 - val_loss: 0.4224\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4303 - val_loss: 0.4213\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4304 - val_loss: 0.4225\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4306 - val_loss: 0.4222\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4300 - val_loss: 0.4219\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4306 - val_loss: 0.4204\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4300 - val_loss: 0.4207\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 0s 22us/step - loss: 0.4302 - val_loss: 0.4224\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4301 - val_loss: 0.4217\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4297 - val_loss: 0.4219\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4300 - val_loss: 0.4218\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4300 - val_loss: 0.4203\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 23us/step - loss: 0.4299 - val_loss: 0.4207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64833444042860988"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_21 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_21 = np.sqrt(min(model_21.history[\"val_loss\"]))\n",
    "RMSE_model_21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like avoiding the prices above $200 indeed reduced the log RMSE. Let's try to increase this model's complexity to see how far we can get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 1.3129 - val_loss: 0.4325\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4419 - val_loss: 0.4230\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4369 - val_loss: 0.4212\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4358 - val_loss: 0.4261\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4361 - val_loss: 0.4225\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4355 - val_loss: 0.4216\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4351 - val_loss: 0.4220\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4333 - val_loss: 0.4205\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4339 - val_loss: 0.4210\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4341 - val_loss: 0.4204\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4332 - val_loss: 0.4210\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4307 - val_loss: 0.4261\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4308 - val_loss: 0.4233\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4303 - val_loss: 0.4225\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4303 - val_loss: 0.4227\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4320 - val_loss: 0.4219\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4304 - val_loss: 0.4237\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4284 - val_loss: 0.4270\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4300 - val_loss: 0.4312\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4271 - val_loss: 0.4270\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4276 - val_loss: 0.4262\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4280 - val_loss: 0.4291\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4272 - val_loss: 0.4222\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4268 - val_loss: 0.4228\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4268 - val_loss: 0.4260\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4272 - val_loss: 0.4236\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4258 - val_loss: 0.4238\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4279 - val_loss: 0.4275\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4246 - val_loss: 0.4233\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4250 - val_loss: 0.4289\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4244 - val_loss: 0.4307\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4257 - val_loss: 0.4243\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4251 - val_loss: 0.4240\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4228 - val_loss: 0.4264\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4235 - val_loss: 0.4227\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4234 - val_loss: 0.4261\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4221 - val_loss: 0.4250\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4244 - val_loss: 0.4240\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4215 - val_loss: 0.4238\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4210 - val_loss: 0.4229\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4219 - val_loss: 0.4323\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4224 - val_loss: 0.4257\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4199 - val_loss: 0.4228\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4205 - val_loss: 0.4247\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4201 - val_loss: 0.4249\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4193 - val_loss: 0.4233\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4200 - val_loss: 0.4300\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4191 - val_loss: 0.4245\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4187 - val_loss: 0.4248\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4190 - val_loss: 0.4256\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4184 - val_loss: 0.4242\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4173 - val_loss: 0.4242\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4173 - val_loss: 0.4263\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4189 - val_loss: 0.4232\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 42us/step - loss: 0.4163 - val_loss: 0.4321\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 43us/step - loss: 0.4167 - val_loss: 0.4259\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4171 - val_loss: 0.4276\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4173 - val_loss: 0.4277\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4154 - val_loss: 0.4277\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4151 - val_loss: 0.4252\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4141 - val_loss: 0.4291\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4147 - val_loss: 0.4268\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4162 - val_loss: 0.4283\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4129 - val_loss: 0.4485\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4142 - val_loss: 0.4293\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4147 - val_loss: 0.4260\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4121 - val_loss: 0.4286\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 46us/step - loss: 0.4123 - val_loss: 0.4293\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4131 - val_loss: 0.4279\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 42us/step - loss: 0.4112 - val_loss: 0.4292\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4140 - val_loss: 0.4300\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4114 - val_loss: 0.4280\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4110 - val_loss: 0.4453\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4120 - val_loss: 0.4323\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4113 - val_loss: 0.4306\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4096 - val_loss: 0.4310\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4111 - val_loss: 0.4285\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4106 - val_loss: 0.4301\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4093 - val_loss: 0.4322\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4107 - val_loss: 0.4324\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4096 - val_loss: 0.4301\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4086 - val_loss: 0.4309\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4087 - val_loss: 0.4480\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4079 - val_loss: 0.4357\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4087 - val_loss: 0.4328\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4068 - val_loss: 0.4317\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4072 - val_loss: 0.4295\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4070 - val_loss: 0.4311\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4084 - val_loss: 0.4301\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4073 - val_loss: 0.4317\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4059 - val_loss: 0.4302\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4070 - val_loss: 0.4338\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 45us/step - loss: 0.4062 - val_loss: 0.4343\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4055 - val_loss: 0.4322\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4057 - val_loss: 0.4354\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4044 - val_loss: 0.4302\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4046 - val_loss: 0.4313\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4057 - val_loss: 0.4391\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4041 - val_loss: 0.4313\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4053 - val_loss: 0.4325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64841286047778535"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(100,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_22 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_22 = np.sqrt(min(model_22.history[\"val_loss\"]))\n",
    "RMSE_model_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 3.4251 - val_loss: 0.6488\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.5285 - val_loss: 0.4522\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4575 - val_loss: 0.4338\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4461 - val_loss: 0.4275\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4410 - val_loss: 0.4246\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4383 - val_loss: 0.4237\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4379 - val_loss: 0.4233\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4368 - val_loss: 0.4221\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4370 - val_loss: 0.4219\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4361 - val_loss: 0.4221\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4362 - val_loss: 0.4217\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4354 - val_loss: 0.4217\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4353 - val_loss: 0.4224\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4349 - val_loss: 0.4218\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4352 - val_loss: 0.4208\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4346 - val_loss: 0.4210\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4347 - val_loss: 0.4210\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4340 - val_loss: 0.4214\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4346 - val_loss: 0.4205\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4353 - val_loss: 0.4237\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4338 - val_loss: 0.4207\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4332 - val_loss: 0.4213\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4338 - val_loss: 0.4229\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4333 - val_loss: 0.4210\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4324 - val_loss: 0.4215\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4324 - val_loss: 0.4205\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4321 - val_loss: 0.4206\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4318 - val_loss: 0.4201\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4322 - val_loss: 0.4245\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4319 - val_loss: 0.4211\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4323 - val_loss: 0.4208\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4314 - val_loss: 0.4207\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4318 - val_loss: 0.4230\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4315 - val_loss: 0.4210\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4313 - val_loss: 0.4209\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4319 - val_loss: 0.4225\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4310 - val_loss: 0.4212\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4314 - val_loss: 0.4206\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4309 - val_loss: 0.4227\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4306 - val_loss: 0.4211\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4310 - val_loss: 0.4237\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4302 - val_loss: 0.4206\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4305 - val_loss: 0.4204\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4301 - val_loss: 0.4227\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4299 - val_loss: 0.4210\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4304 - val_loss: 0.4265\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4312 - val_loss: 0.4246\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4296 - val_loss: 0.4207\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4301 - val_loss: 0.4211\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4295 - val_loss: 0.4224\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4295 - val_loss: 0.4249\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4303 - val_loss: 0.4221\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4292 - val_loss: 0.4259\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4298 - val_loss: 0.4223\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4290 - val_loss: 0.4240\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4302 - val_loss: 0.4222\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4306 - val_loss: 0.4214\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4285 - val_loss: 0.4232\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4297 - val_loss: 0.4272\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4303 - val_loss: 0.4226\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4290 - val_loss: 0.4212\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4284 - val_loss: 0.4225\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4290 - val_loss: 0.4212\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4289 - val_loss: 0.4218\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4286 - val_loss: 0.4216\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 24us/step - loss: 0.4289 - val_loss: 0.4214\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4286 - val_loss: 0.4224\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4304 - val_loss: 0.4222\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4287 - val_loss: 0.4203\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4286 - val_loss: 0.4230\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4279 - val_loss: 0.4226\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4290 - val_loss: 0.4249\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4286 - val_loss: 0.4209\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4298 - val_loss: 0.4228\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4283 - val_loss: 0.4206\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4285 - val_loss: 0.4230\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4289 - val_loss: 0.4231\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4284 - val_loss: 0.4251\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4290 - val_loss: 0.4216\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4285 - val_loss: 0.4219\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4280 - val_loss: 0.4236\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4289 - val_loss: 0.4241\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4280 - val_loss: 0.4224\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4282 - val_loss: 0.4209\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 25us/step - loss: 0.4279 - val_loss: 0.4221\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - ETA: 0s - loss: 0.428 - 1s 30us/step - loss: 0.4280 - val_loss: 0.4218\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4271 - val_loss: 0.4217\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4284 - val_loss: 0.4232\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4277 - val_loss: 0.4234\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4278 - val_loss: 0.4236\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4283 - val_loss: 0.4256\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4285 - val_loss: 0.4223\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4287 - val_loss: 0.4269\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4276 - val_loss: 0.4270\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4281 - val_loss: 0.4230\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4278 - val_loss: 0.4208\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4278 - val_loss: 0.4222\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4276 - val_loss: 0.4219\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4278 - val_loss: 0.4224\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4274 - val_loss: 0.4228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64841286047778535"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_23 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64817864165398598"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_23 = np.sqrt(min(model_23.history[\"val_loss\"]))\n",
    "RMSE_model_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 2s 88us/step - loss: 2.1698 - val_loss: 0.4980\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4820 - val_loss: 0.4441\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4548 - val_loss: 0.4320\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4462 - val_loss: 0.4292\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4417 - val_loss: 0.4279\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4395 - val_loss: 0.4232\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4378 - val_loss: 0.4296\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4371 - val_loss: 0.4222\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4359 - val_loss: 0.4228\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4355 - val_loss: 0.4239\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4354 - val_loss: 0.4223\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4339 - val_loss: 0.4219\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4338 - val_loss: 0.4221\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4338 - val_loss: 0.4216\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4326 - val_loss: 0.4229\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4327 - val_loss: 0.4216\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4323 - val_loss: 0.4227\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4321 - val_loss: 0.4211\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4321 - val_loss: 0.4252\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4316 - val_loss: 0.4209\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4315 - val_loss: 0.4214\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4313 - val_loss: 0.4238\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4308 - val_loss: 0.4256\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4303 - val_loss: 0.4216\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4311 - val_loss: 0.4201\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4301 - val_loss: 0.4205\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4298 - val_loss: 0.4257\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4303 - val_loss: 0.4206\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4294 - val_loss: 0.4202\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4295 - val_loss: 0.4210\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4293 - val_loss: 0.4202\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4285 - val_loss: 0.4206\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4293 - val_loss: 0.4208\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4289 - val_loss: 0.4212\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4281 - val_loss: 0.4212\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4283 - val_loss: 0.4195\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4281 - val_loss: 0.4193\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4291 - val_loss: 0.4198\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4290 - val_loss: 0.4202\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4279 - val_loss: 0.4203\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4285 - val_loss: 0.4266\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4282 - val_loss: 0.4199\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4279 - val_loss: 0.4264\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4285 - val_loss: 0.4228\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4279 - val_loss: 0.4213\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4278 - val_loss: 0.4218\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4278 - val_loss: 0.4207\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4276 - val_loss: 0.4205\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4277 - val_loss: 0.4209\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4277 - val_loss: 0.4204\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4277 - val_loss: 0.4205\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4273 - val_loss: 0.4202\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4272 - val_loss: 0.4229\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4279 - val_loss: 0.4195\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4270 - val_loss: 0.4194\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4269 - val_loss: 0.4191\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4278 - val_loss: 0.4245\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4271 - val_loss: 0.4226\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4276 - val_loss: 0.4202\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4271 - val_loss: 0.4235\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4268 - val_loss: 0.4198\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4266 - val_loss: 0.4215\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4271 - val_loss: 0.4197\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4263 - val_loss: 0.4272\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4268 - val_loss: 0.4254\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4265 - val_loss: 0.4219\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4263 - val_loss: 0.4219\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4262 - val_loss: 0.4220\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4262 - val_loss: 0.4219\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4262 - val_loss: 0.4204\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4264 - val_loss: 0.4198\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4268 - val_loss: 0.4198\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4266 - val_loss: 0.4303\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4261 - val_loss: 0.4194\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4257 - val_loss: 0.4201\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4265 - val_loss: 0.4209\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4259 - val_loss: 0.4222\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4260 - val_loss: 0.4196\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4256 - val_loss: 0.4192\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4256 - val_loss: 0.4214\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4259 - val_loss: 0.4196\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4276 - val_loss: 0.4195\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4257 - val_loss: 0.4210\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4260 - val_loss: 0.4205\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4255 - val_loss: 0.4223\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4253 - val_loss: 0.4222\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 26us/step - loss: 0.4260 - val_loss: 0.4197\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4260 - val_loss: 0.4210\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4256 - val_loss: 0.4207\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4261 - val_loss: 0.4197\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4256 - val_loss: 0.4198\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4253 - val_loss: 0.4214\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4261 - val_loss: 0.4228\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4259 - val_loss: 0.4218\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4253 - val_loss: 0.4211\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4248 - val_loss: 0.4198\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4264 - val_loss: 0.4197\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4249 - val_loss: 0.4196\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4253 - val_loss: 0.4203\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4247 - val_loss: 0.4210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64735193680360126"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_24 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_24 = np.sqrt(min(model_24.history[\"val_loss\"]))\n",
    "RMSE_model_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 2s 80us/step - loss: 2.3546 - val_loss: 0.5212\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4859 - val_loss: 0.4467\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4518 - val_loss: 0.4305\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4433 - val_loss: 0.4276\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4407 - val_loss: 0.4240\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4388 - val_loss: 0.4290\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4376 - val_loss: 0.4242\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4372 - val_loss: 0.4229\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4356 - val_loss: 0.4242\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4349 - val_loss: 0.4262\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4349 - val_loss: 0.4227\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4341 - val_loss: 0.4265\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4345 - val_loss: 0.4285\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4354 - val_loss: 0.4260\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4345 - val_loss: 0.4331\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4339 - val_loss: 0.4291\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4338 - val_loss: 0.4218\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4334 - val_loss: 0.4306\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4332 - val_loss: 0.4217\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4331 - val_loss: 0.4228\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4330 - val_loss: 0.4238\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4324 - val_loss: 0.4208\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4327 - val_loss: 0.4212\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4332 - val_loss: 0.4229\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4317 - val_loss: 0.4211\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4335 - val_loss: 0.4218\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4331 - val_loss: 0.4211\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4311 - val_loss: 0.4221\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4335 - val_loss: 0.4220\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4307 - val_loss: 0.4221\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4317 - val_loss: 0.4214\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4306 - val_loss: 0.4213\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4310 - val_loss: 0.4271\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4316 - val_loss: 0.4219\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4309 - val_loss: 0.4231\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4321 - val_loss: 0.4239\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4302 - val_loss: 0.4213\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4316 - val_loss: 0.4263\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4298 - val_loss: 0.4234\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4302 - val_loss: 0.4226\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4301 - val_loss: 0.4214\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4309 - val_loss: 0.4225\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 46us/step - loss: 0.4301 - val_loss: 0.4246\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4310 - val_loss: 0.4234\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4302 - val_loss: 0.4226\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4303 - val_loss: 0.4218\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4296 - val_loss: 0.4258\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4292 - val_loss: 0.4273\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4299 - val_loss: 0.4223\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4294 - val_loss: 0.4220\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4290 - val_loss: 0.4217\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4293 - val_loss: 0.4229\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4298 - val_loss: 0.4230\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4301 - val_loss: 0.4232\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4288 - val_loss: 0.4242\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4291 - val_loss: 0.4278\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4302 - val_loss: 0.4272\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4295 - val_loss: 0.4220\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4294 - val_loss: 0.4248\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4298 - val_loss: 0.4223\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4286 - val_loss: 0.4255\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4296 - val_loss: 0.4216\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4283 - val_loss: 0.4232\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4283 - val_loss: 0.4249\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4291 - val_loss: 0.4247\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4290 - val_loss: 0.4275\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4286 - val_loss: 0.4225\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4282 - val_loss: 0.4228\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4288 - val_loss: 0.4227\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4295 - val_loss: 0.4239\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4286 - val_loss: 0.4249\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4285 - val_loss: 0.4230\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4283 - val_loss: 0.4248\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4285 - val_loss: 0.4252\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4284 - val_loss: 0.4225\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4277 - val_loss: 0.4287\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4284 - val_loss: 0.4271\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4289 - val_loss: 0.4220\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4279 - val_loss: 0.4227\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4277 - val_loss: 0.4231\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4275 - val_loss: 0.4227\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4283 - val_loss: 0.4221\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4283 - val_loss: 0.4224\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4281 - val_loss: 0.4237\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4275 - val_loss: 0.4250\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4270 - val_loss: 0.4227\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4279 - val_loss: 0.4236\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4276 - val_loss: 0.4232\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4273 - val_loss: 0.4223\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4271 - val_loss: 0.4230\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4267 - val_loss: 0.4223\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4265 - val_loss: 0.4230\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4274 - val_loss: 0.4265\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4271 - val_loss: 0.4255\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4278 - val_loss: 0.4249\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 42us/step - loss: 0.4271 - val_loss: 0.4235\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4272 - val_loss: 0.4238\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4274 - val_loss: 0.4293\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4276 - val_loss: 0.4235\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4273 - val_loss: 0.4271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64869357389218485"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_25 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_25 = np.sqrt(min(model_25.history[\"val_loss\"]))\n",
    "RMSE_model_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 2s 93us/step - loss: 1.8937 - val_loss: 0.4648\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4611 - val_loss: 0.4321\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4455 - val_loss: 0.4292\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4410 - val_loss: 0.4239\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4386 - val_loss: 0.4224\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4373 - val_loss: 0.4222\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4379 - val_loss: 0.4213\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4359 - val_loss: 0.4223\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4352 - val_loss: 0.4217\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4350 - val_loss: 0.4200\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4340 - val_loss: 0.4201\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4333 - val_loss: 0.4204\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4344 - val_loss: 0.4220\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4322 - val_loss: 0.4209\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4315 - val_loss: 0.4200\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4318 - val_loss: 0.4196\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4317 - val_loss: 0.4251\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4309 - val_loss: 0.4198\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4307 - val_loss: 0.4212\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4308 - val_loss: 0.4197\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4300 - val_loss: 0.4213\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4312 - val_loss: 0.4233\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4300 - val_loss: 0.4401\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4301 - val_loss: 0.4237\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4305 - val_loss: 0.4210\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4291 - val_loss: 0.4218\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4306 - val_loss: 0.4203\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4295 - val_loss: 0.4229\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4298 - val_loss: 0.4201\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4293 - val_loss: 0.4207\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4287 - val_loss: 0.4207\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4282 - val_loss: 0.4269\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4280 - val_loss: 0.4246\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 27us/step - loss: 0.4285 - val_loss: 0.4225\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4283 - val_loss: 0.4208\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4272 - val_loss: 0.4224\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4293 - val_loss: 0.4329\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4275 - val_loss: 0.4210\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4274 - val_loss: 0.4213\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4274 - val_loss: 0.4211\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4276 - val_loss: 0.4228\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4271 - val_loss: 0.4198\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4270 - val_loss: 0.4224\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4268 - val_loss: 0.4260\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4264 - val_loss: 0.4210\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4265 - val_loss: 0.4209\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4257 - val_loss: 0.4214\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4269 - val_loss: 0.4223\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4265 - val_loss: 0.4232\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4265 - val_loss: 0.4209\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4260 - val_loss: 0.4206\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4256 - val_loss: 0.4245\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4253 - val_loss: 0.4211\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4250 - val_loss: 0.4209\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4249 - val_loss: 0.4230\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4247 - val_loss: 0.4212\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4262 - val_loss: 0.4349\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4257 - val_loss: 0.4265\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4253 - val_loss: 0.4213\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4251 - val_loss: 0.4221\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4243 - val_loss: 0.4221\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4259 - val_loss: 0.4267\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4239 - val_loss: 0.4208\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4235 - val_loss: 0.4226\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4241 - val_loss: 0.4209\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4236 - val_loss: 0.4218\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4248 - val_loss: 0.4231\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4235 - val_loss: 0.4234\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4243 - val_loss: 0.4238\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4238 - val_loss: 0.4223\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4249 - val_loss: 0.4311\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4246 - val_loss: 0.4252\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4229 - val_loss: 0.4232\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4239 - val_loss: 0.4223\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4232 - val_loss: 0.4217\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4229 - val_loss: 0.4240\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4233 - val_loss: 0.4242\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4232 - val_loss: 0.4225\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4230 - val_loss: 0.4246\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4239 - val_loss: 0.4272\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4227 - val_loss: 0.4269\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4220 - val_loss: 0.4255\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4238 - val_loss: 0.4260\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4218 - val_loss: 0.4264\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4232 - val_loss: 0.4257\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4225 - val_loss: 0.4275\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4225 - val_loss: 0.4248\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4220 - val_loss: 0.4246\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4222 - val_loss: 0.4246\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4226 - val_loss: 0.4239\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4216 - val_loss: 0.4235\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4222 - val_loss: 0.4255\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4230 - val_loss: 0.4233\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4208 - val_loss: 0.4238\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4213 - val_loss: 0.4246\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4226 - val_loss: 0.4311\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4211 - val_loss: 0.4252\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4214 - val_loss: 0.4239\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4210 - val_loss: 0.4246\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 42us/step - loss: 0.4204 - val_loss: 0.4333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6477779652480824"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_26 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_26 = np.sqrt(min(model_26.history[\"val_loss\"]))\n",
    "RMSE_model_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 2s 78us/step - loss: 1.7552 - val_loss: 0.4606\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4569 - val_loss: 0.4294\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4415 - val_loss: 0.4233\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4385 - val_loss: 0.4224\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4367 - val_loss: 0.4226\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4344 - val_loss: 0.4233\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4344 - val_loss: 0.4253\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 46us/step - loss: 0.4354 - val_loss: 0.4224\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4345 - val_loss: 0.4344\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4344 - val_loss: 0.4263\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4331 - val_loss: 0.4238\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4325 - val_loss: 0.4288\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4326 - val_loss: 0.4213\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4331 - val_loss: 0.4222\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4313 - val_loss: 0.4242\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4315 - val_loss: 0.4281\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4332 - val_loss: 0.4222\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4316 - val_loss: 0.4249\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4306 - val_loss: 0.4209\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4298 - val_loss: 0.4283\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4305 - val_loss: 0.4204\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4300 - val_loss: 0.4237\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4301 - val_loss: 0.4224\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4300 - val_loss: 0.4230\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4291 - val_loss: 0.4209\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4309 - val_loss: 0.4246\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4301 - val_loss: 0.4243\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4290 - val_loss: 0.4281\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4286 - val_loss: 0.4208\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4286 - val_loss: 0.4216\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4282 - val_loss: 0.4245\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4299 - val_loss: 0.4238\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4296 - val_loss: 0.4228\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4282 - val_loss: 0.4223\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4274 - val_loss: 0.4232\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - ETA: 0s - loss: 0.430 - 1s 37us/step - loss: 0.4292 - val_loss: 0.4228\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4295 - val_loss: 0.4249\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4284 - val_loss: 0.4247\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4276 - val_loss: 0.4221\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4290 - val_loss: 0.4232\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4284 - val_loss: 0.4220\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4279 - val_loss: 0.4230\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4283 - val_loss: 0.4242\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4284 - val_loss: 0.4219\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4283 - val_loss: 0.4218\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4269 - val_loss: 0.4318\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4275 - val_loss: 0.4237\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4263 - val_loss: 0.4238\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4262 - val_loss: 0.4229\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4261 - val_loss: 0.4219\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4263 - val_loss: 0.4244\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4249 - val_loss: 0.4262\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4256 - val_loss: 0.4225\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4262 - val_loss: 0.4221\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4255 - val_loss: 0.4225\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4264 - val_loss: 0.4267\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4289 - val_loss: 0.4269\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4265 - val_loss: 0.4237\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4247 - val_loss: 0.4234\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 41us/step - loss: 0.4257 - val_loss: 0.4290\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4245 - val_loss: 0.4294\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4271 - val_loss: 0.4252\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4250 - val_loss: 0.4240\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4249 - val_loss: 0.4297\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4255 - val_loss: 0.4301\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4254 - val_loss: 0.4239\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4245 - val_loss: 0.4222\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4243 - val_loss: 0.4263\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4244 - val_loss: 0.4263\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4246 - val_loss: 0.4241\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4235 - val_loss: 0.4345\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4261 - val_loss: 0.4377\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4250 - val_loss: 0.4274\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4234 - val_loss: 0.4242\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4239 - val_loss: 0.4240\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4236 - val_loss: 0.4255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4241 - val_loss: 0.4238\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4238 - val_loss: 0.4241\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4226 - val_loss: 0.4231\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4231 - val_loss: 0.4243\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4236 - val_loss: 0.4264\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4228 - val_loss: 0.4231\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 28us/step - loss: 0.4227 - val_loss: 0.4236\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4227 - val_loss: 0.4271\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4226 - val_loss: 0.4284\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4234 - val_loss: 0.4252\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4236 - val_loss: 0.4245\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4226 - val_loss: 0.4248\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4216 - val_loss: 0.4274\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4219 - val_loss: 0.4248\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4214 - val_loss: 0.4257\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4216 - val_loss: 0.4262\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4217 - val_loss: 0.4249\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4212 - val_loss: 0.4276\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4210 - val_loss: 0.4265\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4210 - val_loss: 0.4244\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4221 - val_loss: 0.4255\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4201 - val_loss: 0.4265\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4201 - val_loss: 0.4241\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4209 - val_loss: 0.4278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64840670631325314"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_27 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_27 = np.sqrt(min(model_27.history[\"val_loss\"]))\n",
    "RMSE_model_27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 2s 85us/step - loss: 1.0315 - val_loss: 0.4270\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.4402 - val_loss: 0.4242\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 53us/step - loss: 0.4372 - val_loss: 0.4235\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.4362 - val_loss: 0.4263\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4357 - val_loss: 0.4215\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4343 - val_loss: 0.4362\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.4351 - val_loss: 0.4271\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4328 - val_loss: 0.4385\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4339 - val_loss: 0.4294\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 63us/step - loss: 0.4333 - val_loss: 0.4286\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.4332 - val_loss: 0.4282\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4350 - val_loss: 0.4214\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.4319 - val_loss: 0.4226\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.4334 - val_loss: 0.4326\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4314 - val_loss: 0.4233\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.4300 - val_loss: 0.4348\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 58us/step - loss: 0.4310 - val_loss: 0.4288\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 58us/step - loss: 0.4276 - val_loss: 0.4246\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4273 - val_loss: 0.4281\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4293 - val_loss: 0.4230\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4266 - val_loss: 0.4250\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 46us/step - loss: 0.4262 - val_loss: 0.4223\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 46us/step - loss: 0.4282 - val_loss: 0.4236\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4258 - val_loss: 0.4256\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4261 - val_loss: 0.4288\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4247 - val_loss: 0.4226\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 64us/step - loss: 0.4243 - val_loss: 0.4337\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.4233 - val_loss: 0.4238\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4224 - val_loss: 0.4229\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 52us/step - loss: 0.4242 - val_loss: 0.4518\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4245 - val_loss: 0.4241\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4213 - val_loss: 0.4275\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4219 - val_loss: 0.4336\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4201 - val_loss: 0.4285\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 59us/step - loss: 0.4207 - val_loss: 0.4279\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 59us/step - loss: 0.4196 - val_loss: 0.4264\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.4217 - val_loss: 0.4244\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 53us/step - loss: 0.4189 - val_loss: 0.4273\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.4167 - val_loss: 0.4248\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4170 - val_loss: 0.4280\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 52us/step - loss: 0.4173 - val_loss: 0.4265\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4164 - val_loss: 0.4362\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 60us/step - loss: 0.4150 - val_loss: 0.4282\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.4149 - val_loss: 0.4269\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4152 - val_loss: 0.4321\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 59us/step - loss: 0.4145 - val_loss: 0.4398\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4143 - val_loss: 0.4322\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.4138 - val_loss: 0.4328\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.4137 - val_loss: 0.4328\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.4123 - val_loss: 0.4286\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4118 - val_loss: 0.4288\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 59us/step - loss: 0.4132 - val_loss: 0.4340\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4111 - val_loss: 0.4290\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4094 - val_loss: 0.4338\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.4085 - val_loss: 0.4303\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4099 - val_loss: 0.4349\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.4078 - val_loss: 0.4339\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.4084 - val_loss: 0.4332\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 52us/step - loss: 0.4059 - val_loss: 0.4511\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 61us/step - loss: 0.4078 - val_loss: 0.4386\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 66us/step - loss: 0.4063 - val_loss: 0.4314\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.4056 - val_loss: 0.4363\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.4041 - val_loss: 0.4379\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 53us/step - loss: 0.4068 - val_loss: 0.4425\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 58us/step - loss: 0.4040 - val_loss: 0.4333\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 53us/step - loss: 0.4032 - val_loss: 0.4348\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4018 - val_loss: 0.4331\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 52us/step - loss: 0.4025 - val_loss: 0.4367\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 53us/step - loss: 0.4034 - val_loss: 0.4362\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.4017 - val_loss: 0.4346\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 52us/step - loss: 0.4021 - val_loss: 0.4462\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.4011 - val_loss: 0.4397\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.4005 - val_loss: 0.4453\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.4003 - val_loss: 0.4395\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 52us/step - loss: 0.3987 - val_loss: 0.4427\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.3996 - val_loss: 0.4377\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.3984 - val_loss: 0.4398\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.3973 - val_loss: 0.4388\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 48us/step - loss: 0.3986 - val_loss: 0.4422\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.3971 - val_loss: 0.4376\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.3967 - val_loss: 0.4414\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.3966 - val_loss: 0.4445\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.3959 - val_loss: 0.4411\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.3952 - val_loss: 0.4465\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 50us/step - loss: 0.3959 - val_loss: 0.4455\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 58us/step - loss: 0.3954 - val_loss: 0.4532\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 55us/step - loss: 0.3944 - val_loss: 0.4483\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.3942 - val_loss: 0.4430\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.3931 - val_loss: 0.4474\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 49us/step - loss: 0.3932 - val_loss: 0.4478\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.3929 - val_loss: 0.4434\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 60us/step - loss: 0.3913 - val_loss: 0.4501\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.3931 - val_loss: 0.4438\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 54us/step - loss: 0.3930 - val_loss: 0.4434\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 56us/step - loss: 0.3933 - val_loss: 0.4449\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 51us/step - loss: 0.3922 - val_loss: 0.4481\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 59us/step - loss: 0.3912 - val_loss: 0.4499\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.3916 - val_loss: 0.4587\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.3910 - val_loss: 0.4460\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 57us/step - loss: 0.3908 - val_loss: 0.4465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64912001403999242"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(200,activation=\"relu\"))\n",
    "model.add(Dense(20,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_28 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_28 = np.sqrt(min(model_28.history[\"val_loss\"]))\n",
    "RMSE_model_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 4s 164us/step - loss: 0.7829 - val_loss: 0.4283\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4459 - val_loss: 0.4248\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 3s 128us/step - loss: 0.4408 - val_loss: 0.4238\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 3s 151us/step - loss: 0.4391 - val_loss: 0.4234\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 3s 123us/step - loss: 0.4404 - val_loss: 0.4229\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 3s 126us/step - loss: 0.4381 - val_loss: 0.4381\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4382 - val_loss: 0.4233\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 3s 123us/step - loss: 0.4387 - val_loss: 0.4227\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 3s 125us/step - loss: 0.4394 - val_loss: 0.4240\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 3s 126us/step - loss: 0.4363 - val_loss: 0.4521\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 3s 150us/step - loss: 0.4347 - val_loss: 0.4373\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 3s 132us/step - loss: 0.4336 - val_loss: 0.4268\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 3s 127us/step - loss: 0.4338 - val_loss: 0.4408\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4318 - val_loss: 0.4299\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4339 - val_loss: 0.4249\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4306 - val_loss: 0.4241\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 4s 172us/step - loss: 0.4315 - val_loss: 0.4320\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 5s 206us/step - loss: 0.4295 - val_loss: 0.4247\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 4s 189us/step - loss: 0.4288 - val_loss: 0.4290\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 4s 165us/step - loss: 0.4295 - val_loss: 0.4214\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 3s 150us/step - loss: 0.4272 - val_loss: 0.4292\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 3s 145us/step - loss: 0.4279 - val_loss: 0.4459\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 3s 129us/step - loss: 0.4276 - val_loss: 0.4260\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 3s 125us/step - loss: 0.4232 - val_loss: 0.4234\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 3s 129us/step - loss: 0.4265 - val_loss: 0.4222\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4227 - val_loss: 0.4220\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 3s 147us/step - loss: 0.4223 - val_loss: 0.4236\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 3s 126us/step - loss: 0.4210 - val_loss: 0.4253\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 4s 177us/step - loss: 0.4206 - val_loss: 0.4261\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 5s 227us/step - loss: 0.4218 - val_loss: 0.4277\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 3s 157us/step - loss: 0.4177 - val_loss: 0.4263\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 3s 141us/step - loss: 0.4186 - val_loss: 0.4455\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 3s 137us/step - loss: 0.4188 - val_loss: 0.4252\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 3s 142us/step - loss: 0.4167 - val_loss: 0.4280\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 3s 125us/step - loss: 0.4166 - val_loss: 0.4247\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4170 - val_loss: 0.4228\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4172 - val_loss: 0.4244\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4153 - val_loss: 0.4287\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 3s 123us/step - loss: 0.4139 - val_loss: 0.4272\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4143 - val_loss: 0.4383\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4145 - val_loss: 0.4345\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 3s 141us/step - loss: 0.4119 - val_loss: 0.4276\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 3s 127us/step - loss: 0.4116 - val_loss: 0.4458\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 3s 123us/step - loss: 0.4103 - val_loss: 0.4303\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 3s 137us/step - loss: 0.4094 - val_loss: 0.4283\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 3s 121us/step - loss: 0.4090 - val_loss: 0.4312\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4096 - val_loss: 0.4299\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4081 - val_loss: 0.4337\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 3s 121us/step - loss: 0.4058 - val_loss: 0.4326\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 3s 121us/step - loss: 0.4057 - val_loss: 0.4324\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4063 - val_loss: 0.4381\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 3s 120us/step - loss: 0.4059 - val_loss: 0.4340\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 3s 142us/step - loss: 0.4041 - val_loss: 0.4433\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4029 - val_loss: 0.4354\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 3s 121us/step - loss: 0.4034 - val_loss: 0.4342\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 3s 123us/step - loss: 0.4015 - val_loss: 0.4349\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.4003 - val_loss: 0.4385\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.4016 - val_loss: 0.4355\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 3s 120us/step - loss: 0.4004 - val_loss: 0.4417\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 3s 141us/step - loss: 0.4005 - val_loss: 0.4397\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.3992 - val_loss: 0.4389\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.3973 - val_loss: 0.4438\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 3s 125us/step - loss: 0.3989 - val_loss: 0.4374\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 3s 141us/step - loss: 0.3978 - val_loss: 0.4388\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.3961 - val_loss: 0.4390\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 3s 126us/step - loss: 0.3972 - val_loss: 0.4405\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.3954 - val_loss: 0.4440\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 3s 123us/step - loss: 0.3944 - val_loss: 0.4402\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.3948 - val_loss: 0.4455\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.3932 - val_loss: 0.4514\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 3s 144us/step - loss: 0.3924 - val_loss: 0.4521\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.3939 - val_loss: 0.4427\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 3s 122us/step - loss: 0.3933 - val_loss: 0.4444\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 3s 135us/step - loss: 0.3929 - val_loss: 0.4480\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 3s 128us/step - loss: 0.3910 - val_loss: 0.4440\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 3s 119us/step - loss: 0.3906 - val_loss: 0.4482\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 3s 120us/step - loss: 0.3904 - val_loss: 0.4422\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.3886 - val_loss: 0.4481\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 3s 120us/step - loss: 0.3903 - val_loss: 0.4475\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 3s 124us/step - loss: 0.3883 - val_loss: 0.4467\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 3s 119us/step - loss: 0.3898 - val_loss: 0.4459\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 3s 148us/step - loss: 0.3883 - val_loss: 0.4488\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 3s 136us/step - loss: 0.3882 - val_loss: 0.4507\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 3s 137us/step - loss: 0.3871 - val_loss: 0.4495\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 3s 154us/step - loss: 0.3868 - val_loss: 0.4502\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 3s 143us/step - loss: 0.3853 - val_loss: 0.4586\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 3s 136us/step - loss: 0.3862 - val_loss: 0.4476\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 3s 134us/step - loss: 0.3857 - val_loss: 0.4512\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 3s 137us/step - loss: 0.3859 - val_loss: 0.4505\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 3s 131us/step - loss: 0.3855 - val_loss: 0.4474\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 3s 143us/step - loss: 0.3836 - val_loss: 0.4588\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 3s 153us/step - loss: 0.3859 - val_loss: 0.4489\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 3s 140us/step - loss: 0.3850 - val_loss: 0.4512\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 3s 136us/step - loss: 0.3852 - val_loss: 0.4495\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 3s 137us/step - loss: 0.3837 - val_loss: 0.4507\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 3s 135us/step - loss: 0.3844 - val_loss: 0.4532\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 3s 136us/step - loss: 0.3830 - val_loss: 0.4594\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 3s 128us/step - loss: 0.3835 - val_loss: 0.4495\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 3s 140us/step - loss: 0.3818 - val_loss: 0.4518\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 3s 137us/step - loss: 0.3818 - val_loss: 0.4501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6491232872320154"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(1000,activation=\"relu\"))\n",
    "model.add(Dense(100,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_29 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_29 = np.sqrt(min(model_29.history[\"val_loss\"]))\n",
    "RMSE_model_29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22092 samples, validate on 14729 samples\n",
      "Epoch 1/100\n",
      "22092/22092 [==============================] - 2s 79us/step - loss: 3.8604 - val_loss: 0.7422\n",
      "Epoch 2/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.5928 - val_loss: 0.4926\n",
      "Epoch 3/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4817 - val_loss: 0.4495\n",
      "Epoch 4/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4556 - val_loss: 0.4354\n",
      "Epoch 5/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4472 - val_loss: 0.4292\n",
      "Epoch 6/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4436 - val_loss: 0.4257\n",
      "Epoch 7/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4419 - val_loss: 0.4246\n",
      "Epoch 8/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4411 - val_loss: 0.4244\n",
      "Epoch 9/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4408 - val_loss: 0.4239\n",
      "Epoch 10/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4401 - val_loss: 0.4237\n",
      "Epoch 11/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4399 - val_loss: 0.4245\n",
      "Epoch 12/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4396 - val_loss: 0.4317\n",
      "Epoch 13/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4400 - val_loss: 0.4240\n",
      "Epoch 14/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4394 - val_loss: 0.4240\n",
      "Epoch 15/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4392 - val_loss: 0.4235\n",
      "Epoch 16/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4388 - val_loss: 0.4235\n",
      "Epoch 17/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4383 - val_loss: 0.4263\n",
      "Epoch 18/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4384 - val_loss: 0.4266\n",
      "Epoch 19/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4381 - val_loss: 0.4230\n",
      "Epoch 20/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4377 - val_loss: 0.4233\n",
      "Epoch 21/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4374 - val_loss: 0.4237\n",
      "Epoch 22/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4368 - val_loss: 0.4228\n",
      "Epoch 23/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4369 - val_loss: 0.4229\n",
      "Epoch 24/100\n",
      "22092/22092 [==============================] - 1s 42us/step - loss: 0.4360 - val_loss: 0.4226\n",
      "Epoch 25/100\n",
      "22092/22092 [==============================] - 1s 44us/step - loss: 0.4362 - val_loss: 0.4235\n",
      "Epoch 26/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4357 - val_loss: 0.4222\n",
      "Epoch 27/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4356 - val_loss: 0.4230\n",
      "Epoch 28/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4350 - val_loss: 0.4255\n",
      "Epoch 29/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4359 - val_loss: 0.4242\n",
      "Epoch 30/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4354 - val_loss: 0.4248\n",
      "Epoch 31/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4352 - val_loss: 0.4227\n",
      "Epoch 32/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4352 - val_loss: 0.4227\n",
      "Epoch 33/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4354 - val_loss: 0.4228\n",
      "Epoch 34/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4347 - val_loss: 0.4247\n",
      "Epoch 35/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4345 - val_loss: 0.4224\n",
      "Epoch 36/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4343 - val_loss: 0.4233\n",
      "Epoch 37/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4359 - val_loss: 0.4315\n",
      "Epoch 38/100\n",
      "22092/22092 [==============================] - 1s 42us/step - loss: 0.4344 - val_loss: 0.4245\n",
      "Epoch 39/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4349 - val_loss: 0.4231\n",
      "Epoch 40/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4341 - val_loss: 0.4235\n",
      "Epoch 41/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4345 - val_loss: 0.4234\n",
      "Epoch 42/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4336 - val_loss: 0.4233\n",
      "Epoch 43/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4337 - val_loss: 0.4242\n",
      "Epoch 44/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4334 - val_loss: 0.4229\n",
      "Epoch 45/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4339 - val_loss: 0.4234\n",
      "Epoch 46/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4340 - val_loss: 0.4242\n",
      "Epoch 47/100\n",
      "22092/22092 [==============================] - 1s 29us/step - loss: 0.4336 - val_loss: 0.4278\n",
      "Epoch 48/100\n",
      "22092/22092 [==============================] - 1s 30us/step - loss: 0.4342 - val_loss: 0.4256\n",
      "Epoch 49/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4344 - val_loss: 0.4233\n",
      "Epoch 50/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4329 - val_loss: 0.4248\n",
      "Epoch 51/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4333 - val_loss: 0.4232\n",
      "Epoch 52/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4335 - val_loss: 0.4238\n",
      "Epoch 53/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4331 - val_loss: 0.4240\n",
      "Epoch 54/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4326 - val_loss: 0.4235\n",
      "Epoch 55/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4326 - val_loss: 0.4236\n",
      "Epoch 56/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4322 - val_loss: 0.4231\n",
      "Epoch 57/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4329 - val_loss: 0.4348\n",
      "Epoch 58/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4332 - val_loss: 0.4233\n",
      "Epoch 59/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4326 - val_loss: 0.4253\n",
      "Epoch 60/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4331 - val_loss: 0.4240\n",
      "Epoch 61/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4327 - val_loss: 0.4268\n",
      "Epoch 62/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4325 - val_loss: 0.4227\n",
      "Epoch 63/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4323 - val_loss: 0.4233\n",
      "Epoch 64/100\n",
      "22092/22092 [==============================] - 1s 45us/step - loss: 0.4328 - val_loss: 0.4231\n",
      "Epoch 65/100\n",
      "22092/22092 [==============================] - 1s 40us/step - loss: 0.4325 - val_loss: 0.4231\n",
      "Epoch 66/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4324 - val_loss: 0.4237\n",
      "Epoch 67/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4324 - val_loss: 0.4228\n",
      "Epoch 68/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4311 - val_loss: 0.4239\n",
      "Epoch 69/100\n",
      "22092/22092 [==============================] - 1s 39us/step - loss: 0.4317 - val_loss: 0.4228\n",
      "Epoch 70/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4314 - val_loss: 0.4230\n",
      "Epoch 71/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4319 - val_loss: 0.4234\n",
      "Epoch 72/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4317 - val_loss: 0.4228\n",
      "Epoch 73/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4320 - val_loss: 0.4245\n",
      "Epoch 74/100\n",
      "22092/22092 [==============================] - 1s 31us/step - loss: 0.4314 - val_loss: 0.4237\n",
      "Epoch 75/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4326 - val_loss: 0.4239\n",
      "Epoch 76/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4312 - val_loss: 0.4278\n",
      "Epoch 77/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4321 - val_loss: 0.4266\n",
      "Epoch 78/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4324 - val_loss: 0.4245\n",
      "Epoch 79/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4310 - val_loss: 0.4239\n",
      "Epoch 80/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4312 - val_loss: 0.4234\n",
      "Epoch 81/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4313 - val_loss: 0.4231\n",
      "Epoch 82/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4317 - val_loss: 0.4252\n",
      "Epoch 83/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4312 - val_loss: 0.4243\n",
      "Epoch 84/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4312 - val_loss: 0.4245\n",
      "Epoch 85/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4314 - val_loss: 0.4250\n",
      "Epoch 86/100\n",
      "22092/22092 [==============================] - 1s 35us/step - loss: 0.4310 - val_loss: 0.4244\n",
      "Epoch 87/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4310 - val_loss: 0.4257\n",
      "Epoch 88/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4312 - val_loss: 0.4242\n",
      "Epoch 89/100\n",
      "22092/22092 [==============================] - 1s 43us/step - loss: 0.4315 - val_loss: 0.4234\n",
      "Epoch 90/100\n",
      "22092/22092 [==============================] - 1s 38us/step - loss: 0.4315 - val_loss: 0.4237\n",
      "Epoch 91/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4311 - val_loss: 0.4237\n",
      "Epoch 92/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4310 - val_loss: 0.4315\n",
      "Epoch 93/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4313 - val_loss: 0.4239\n",
      "Epoch 94/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4306 - val_loss: 0.4232\n",
      "Epoch 95/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4314 - val_loss: 0.4243\n",
      "Epoch 96/100\n",
      "22092/22092 [==============================] - 1s 34us/step - loss: 0.4316 - val_loss: 0.4261\n",
      "Epoch 97/100\n",
      "22092/22092 [==============================] - 1s 37us/step - loss: 0.4309 - val_loss: 0.4248\n",
      "Epoch 98/100\n",
      "22092/22092 [==============================] - 1s 32us/step - loss: 0.4314 - val_loss: 0.4269\n",
      "Epoch 99/100\n",
      "22092/22092 [==============================] - 1s 36us/step - loss: 0.4310 - val_loss: 0.4288\n",
      "Epoch 100/100\n",
      "22092/22092 [==============================] - 1s 33us/step - loss: 0.4307 - val_loss: 0.4252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64980020261094462"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_200.shape[1],activation='relu'))\n",
    "model.add(Dense(5,activation=\"relu\"))\n",
    "model.add(Dense(5,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_30 = model.fit(X_200,y_200,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_30 = np.sqrt(min(model_30.history[\"val_loss\"]))\n",
    "RMSE_model_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take home message: excluding outliers during training can potentially help, however since real data will have these outliers, this is not the ideal solution. At least we need to develop an intuition about how the models would perform on the outliers they have not seen before? \n",
    "\n",
    "We have the model_24 that performed best so far. We can use this model to develop this intuition.\n",
    "\n",
    "1. Use model_24 to predict outliers in y.price(any item that is above 200 dollars), this is a data set this model haven't seen before, so it would be interesting to compare.\n",
    "\n",
    "2. Fit the same model_24 with entire data set, including outliers. Again predict outliers in y.price(any item that is above 200 dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outlier = X[y.price > 200]\n",
    "X_outlier.shape\n",
    "y_outlier = np.log(y.price[y.price > 200].values+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 2s 92us/step - loss: 2.7304 - val_loss: 0.6369\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.5521 - val_loss: 0.4815\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4890 - val_loss: 0.4637\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 1s 41us/step - loss: 0.4767 - val_loss: 0.4579\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4729 - val_loss: 0.4585\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4701 - val_loss: 0.4536\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4687 - val_loss: 0.4529\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4678 - val_loss: 0.4540\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4673 - val_loss: 0.4531\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4668 - val_loss: 0.4521\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4672 - val_loss: 0.4529\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4669 - val_loss: 0.4524\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4654 - val_loss: 0.4532\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4655 - val_loss: 0.4598\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - ETA: 0s - loss: 0.465 - 1s 40us/step - loss: 0.4654 - val_loss: 0.4522\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4658 - val_loss: 0.4518\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4648 - val_loss: 0.4637\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4647 - val_loss: 0.4520\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4650 - val_loss: 0.4526\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4642 - val_loss: 0.4529\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4649 - val_loss: 0.4526\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4642 - val_loss: 0.4529\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4635 - val_loss: 0.4521\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4639 - val_loss: 0.4524\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4640 - val_loss: 0.4516\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4642 - val_loss: 0.4516\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4642 - val_loss: 0.4550\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 1s 39us/step - loss: 0.4638 - val_loss: 0.4539\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4631 - val_loss: 0.4534\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4638 - val_loss: 0.4530\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4630 - val_loss: 0.4519\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4624 - val_loss: 0.4527\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4633 - val_loss: 0.4533\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4629 - val_loss: 0.4526\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 42us/step - loss: 0.4628 - val_loss: 0.4526\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4627 - val_loss: 0.4525\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4627 - val_loss: 0.4523\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4632 - val_loss: 0.4532\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4622 - val_loss: 0.4555\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 1s 47us/step - loss: 0.4614 - val_loss: 0.4528\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 1s 42us/step - loss: 0.4628 - val_loss: 0.4569\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4621 - val_loss: 0.4523\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4613 - val_loss: 0.4522\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4618 - val_loss: 0.4520\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4616 - val_loss: 0.4522\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4608 - val_loss: 0.4524\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4616 - val_loss: 0.4534\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4603 - val_loss: 0.4529\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4610 - val_loss: 0.4541\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4614 - val_loss: 0.4532\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4605 - val_loss: 0.4527\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4605 - val_loss: 0.4532\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 38us/step - loss: 0.4607 - val_loss: 0.4556\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 39us/step - loss: 0.4617 - val_loss: 0.4538\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 1s 40us/step - loss: 0.4604 - val_loss: 0.4546\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4603 - val_loss: 0.4526\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4609 - val_loss: 0.4552\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 1s 40us/step - loss: 0.4596 - val_loss: 0.4526\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4603 - val_loss: 0.4549\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4600 - val_loss: 0.4523\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4595 - val_loss: 0.4517\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4604 - val_loss: 0.4525\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 1s 38us/step - loss: 0.4599 - val_loss: 0.4520\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 40us/step - loss: 0.4595 - val_loss: 0.4528\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 1s 42us/step - loss: 0.4600 - val_loss: 0.4580\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 1s 46us/step - loss: 0.4600 - val_loss: 0.4518\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4596 - val_loss: 0.4553\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4592 - val_loss: 0.4525\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4584 - val_loss: 0.4527\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4587 - val_loss: 0.4521\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4591 - val_loss: 0.4515\n",
      "Epoch 72/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4587 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4588 - val_loss: 0.4641\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4585 - val_loss: 0.4562\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 37us/step - loss: 0.4579 - val_loss: 0.4533\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4587 - val_loss: 0.4573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4585 - val_loss: 0.4533\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 1s 44us/step - loss: 0.4581 - val_loss: 0.4533\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 1s 38us/step - loss: 0.4581 - val_loss: 0.4530\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4586 - val_loss: 0.4522\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4578 - val_loss: 0.4524\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4584 - val_loss: 0.4533\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4585 - val_loss: 0.4526\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 1s 39us/step - loss: 0.4579 - val_loss: 0.4547\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 36us/step - loss: 0.4577 - val_loss: 0.4536\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4575 - val_loss: 0.4527\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4576 - val_loss: 0.4539\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4572 - val_loss: 0.4530\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4573 - val_loss: 0.4531\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4574 - val_loss: 0.4530\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 35us/step - loss: 0.4569 - val_loss: 0.4586\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4573 - val_loss: 0.4527\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.4573 - val_loss: 0.4530\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 40us/step - loss: 0.4570 - val_loss: 0.4528\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.4570 - val_loss: 0.4535\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4572 - val_loss: 0.4533\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.4565 - val_loss: 0.4538\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4567 - val_loss: 0.4529\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4570 - val_loss: 0.4528\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4573 - val_loss: 0.4532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67192087418613733"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the same model_24 with entire data set\n",
    "y_all = np.log(y.price+1)\n",
    "X_all = X.values\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_all.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_24_all = model.fit(X_all,y_all,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_24_all = np.sqrt(min(model_24_all.history[\"val_loss\"]))\n",
    "RMSE_model_24_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2658624103893263"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use model_24 to predict outliers in y.price(any item that is above 200 dollars), \n",
    "# this is a data set this model haven't seen before, so it would be interesting to compare\n",
    "\n",
    "model_24_outlier_predictions = model_24.model.predict(X_outlier)\n",
    "np.sqrt(mean_squared_error(y_outlier,model_24_outlier_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1084621789448299"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use model_24_all to predict outliers in y.price(any item that is above 200 dollars), \n",
    "# this is a data set this model already seen before, so it would be interesting to compare\n",
    "model_24_all_outlier_predictions = model_24_all.model.predict(X_outlier)\n",
    "np.sqrt(mean_squared_error(y_outlier,model_24_all_outlier_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67352725926510926"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How does the overall performance compare in two models when we compare predictions using the entire training set?\n",
    "# Model 24:\n",
    "model_24_entire_predictions = model_24.model.predict(X)\n",
    "np.sqrt(mean_squared_error(np.log(y.price +1),model_24_entire_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6742127159816419"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model_24_all\n",
    "model_24_all_entire_predictions = model_24_all.model.predict(X)\n",
    "np.sqrt(mean_squared_error(np.log(y.price +1),model_24_all_entire_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something intersting we notice here. \n",
    "\n",
    "1. In terms of outlier data predictions: we note that the deep learning model we trained without the outlier data performed just a little worse than the model that included this portion of the data during training. Therefore, including these data points during training the regression model does not really help that much. \n",
    "\n",
    "2. In terms of entire data set predictions: we note that the model we trained without the outliers performed slightly better than the model that included outliers during the training.\n",
    "\n",
    "**The take home message for our future experiments:** remove the outlier data points before training the regression model. Focus on the bulk of the data when training the model. Don't worry about outliers, since the overall model performance is almost always more important. \n",
    "\n",
    "Next we need to think about developing other ideas on how to further improve our intuition about regression problems, focusing on our example. \n",
    "\n",
    "Note that when we focus on the bulk of the data without the outliers, actually our model is capturing the variance in log.price fairly well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1286c3f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_24_X200_predictions = model_24.model.predict(X_200)\n",
    "plt.scatter(y= y_200,x = model_24_X200_predictions,s = 2, alpha= 0.2, c = \"r\")\n",
    "plt.ylabel(\"Expected\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can further remove outliers and focus our model on log(price) values between 2 and 4.5? This is the region where bulk of the data exists and we want our model to perform in this range as good as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y.price+1)\n",
    "midrange = np.logical_and((y_log>=2),(y_log <= 4.5))\n",
    "y_midrange = pd.DataFrame(y_log)[midrange]\n",
    "X_midrange = X[midrange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33218, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_midrange.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33218, 26)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_midrange.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_midrange = y_midrange.values\n",
    "X_midrange = X_midrange.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19930 samples, validate on 13288 samples\n",
      "Epoch 1/100\n",
      "19930/19930 [==============================] - 2s 97us/step - loss: 3.5013 - val_loss: 0.4801\n",
      "Epoch 2/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.3803 - val_loss: 0.3381\n",
      "Epoch 3/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.3191 - val_loss: 0.3164\n",
      "Epoch 4/100\n",
      "19930/19930 [==============================] - 1s 34us/step - loss: 0.3093 - val_loss: 0.3110\n",
      "Epoch 5/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3061 - val_loss: 0.3076\n",
      "Epoch 6/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3046 - val_loss: 0.3077\n",
      "Epoch 7/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3039 - val_loss: 0.3084\n",
      "Epoch 8/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3035 - val_loss: 0.3078\n",
      "Epoch 9/100\n",
      "19930/19930 [==============================] - 1s 35us/step - loss: 0.3033 - val_loss: 0.3054\n",
      "Epoch 10/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3028 - val_loss: 0.3080\n",
      "Epoch 11/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3031 - val_loss: 0.3076\n",
      "Epoch 12/100\n",
      "19930/19930 [==============================] - 1s 34us/step - loss: 0.3031 - val_loss: 0.3049\n",
      "Epoch 13/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3022 - val_loss: 0.3051\n",
      "Epoch 14/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3021 - val_loss: 0.3064\n",
      "Epoch 15/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3021 - val_loss: 0.3044\n",
      "Epoch 16/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3022 - val_loss: 0.3044\n",
      "Epoch 17/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3018 - val_loss: 0.3048\n",
      "Epoch 18/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.3015 - val_loss: 0.3137\n",
      "Epoch 19/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.3013 - val_loss: 0.3054\n",
      "Epoch 20/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3018 - val_loss: 0.3050\n",
      "Epoch 21/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.3012 - val_loss: 0.3043\n",
      "Epoch 22/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.3015 - val_loss: 0.3044\n",
      "Epoch 23/100\n",
      "19930/19930 [==============================] - 1s 35us/step - loss: 0.3011 - val_loss: 0.3056\n",
      "Epoch 24/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.3012 - val_loss: 0.3059\n",
      "Epoch 25/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.3016 - val_loss: 0.3141\n",
      "Epoch 26/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.3023 - val_loss: 0.3063\n",
      "Epoch 27/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.3008 - val_loss: 0.3053\n",
      "Epoch 28/100\n",
      "19930/19930 [==============================] - 1s 33us/step - loss: 0.3005 - val_loss: 0.3039\n",
      "Epoch 29/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3006 - val_loss: 0.3040\n",
      "Epoch 30/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3011 - val_loss: 0.3051\n",
      "Epoch 31/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.3007 - val_loss: 0.3043\n",
      "Epoch 32/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3003 - val_loss: 0.3066\n",
      "Epoch 33/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3008 - val_loss: 0.3046\n",
      "Epoch 34/100\n",
      "19930/19930 [==============================] - 1s 33us/step - loss: 0.3001 - val_loss: 0.3063\n",
      "Epoch 35/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.3006 - val_loss: 0.3047\n",
      "Epoch 36/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.3008 - val_loss: 0.3046\n",
      "Epoch 37/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.3006 - val_loss: 0.3039\n",
      "Epoch 38/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2991 - val_loss: 0.3093\n",
      "Epoch 39/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2993 - val_loss: 0.3062\n",
      "Epoch 40/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.2999 - val_loss: 0.3043\n",
      "Epoch 41/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.3003 - val_loss: 0.3045\n",
      "Epoch 42/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2994 - val_loss: 0.3057\n",
      "Epoch 43/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2997 - val_loss: 0.3040\n",
      "Epoch 44/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3005 - val_loss: 0.3038\n",
      "Epoch 45/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.3003 - val_loss: 0.3049\n",
      "Epoch 46/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.2995 - val_loss: 0.3048\n",
      "Epoch 47/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.3000 - val_loss: 0.3053\n",
      "Epoch 48/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2999 - val_loss: 0.3037\n",
      "Epoch 49/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2991 - val_loss: 0.3073\n",
      "Epoch 50/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2994 - val_loss: 0.3051\n",
      "Epoch 51/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2989 - val_loss: 0.3043\n",
      "Epoch 52/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2987 - val_loss: 0.3047\n",
      "Epoch 53/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2988 - val_loss: 0.3039\n",
      "Epoch 54/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2991 - val_loss: 0.3046\n",
      "Epoch 55/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2990 - val_loss: 0.3037\n",
      "Epoch 56/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2990 - val_loss: 0.3147\n",
      "Epoch 57/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2995 - val_loss: 0.3041\n",
      "Epoch 58/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.2991 - val_loss: 0.3049\n",
      "Epoch 59/100\n",
      "19930/19930 [==============================] - 1s 30us/step - loss: 0.2990 - val_loss: 0.3058\n",
      "Epoch 60/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2986 - val_loss: 0.3036\n",
      "Epoch 61/100\n",
      "19930/19930 [==============================] - 1s 30us/step - loss: 0.2992 - val_loss: 0.3063\n",
      "Epoch 62/100\n",
      "19930/19930 [==============================] - 1s 30us/step - loss: 0.2990 - val_loss: 0.3077\n",
      "Epoch 63/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.2990 - val_loss: 0.3039\n",
      "Epoch 64/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.2984 - val_loss: 0.3045\n",
      "Epoch 65/100\n",
      "19930/19930 [==============================] - 1s 33us/step - loss: 0.2984 - val_loss: 0.3066\n",
      "Epoch 66/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2985 - val_loss: 0.3038\n",
      "Epoch 67/100\n",
      "19930/19930 [==============================] - 1s 33us/step - loss: 0.2981 - val_loss: 0.3038\n",
      "Epoch 68/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2988 - val_loss: 0.3061\n",
      "Epoch 69/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2984 - val_loss: 0.3045\n",
      "Epoch 70/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.2982 - val_loss: 0.3082\n",
      "Epoch 71/100\n",
      "19930/19930 [==============================] - 1s 34us/step - loss: 0.2983 - val_loss: 0.3036\n",
      "Epoch 72/100\n",
      "19930/19930 [==============================] - 1s 32us/step - loss: 0.2989 - val_loss: 0.3068\n",
      "Epoch 73/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2992 - val_loss: 0.3058\n",
      "Epoch 74/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2981 - val_loss: 0.3048\n",
      "Epoch 75/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2988 - val_loss: 0.3039\n",
      "Epoch 76/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2985 - val_loss: 0.3045\n",
      "Epoch 77/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2982 - val_loss: 0.3059\n",
      "Epoch 78/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.2975 - val_loss: 0.3039\n",
      "Epoch 79/100\n",
      "19930/19930 [==============================] - 1s 35us/step - loss: 0.2984 - val_loss: 0.3036\n",
      "Epoch 80/100\n",
      "19930/19930 [==============================] - 1s 35us/step - loss: 0.2979 - val_loss: 0.3042\n",
      "Epoch 81/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2984 - val_loss: 0.3045\n",
      "Epoch 82/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2983 - val_loss: 0.3036\n",
      "Epoch 83/100\n",
      "19930/19930 [==============================] - 1s 34us/step - loss: 0.2977 - val_loss: 0.3056\n",
      "Epoch 84/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2978 - val_loss: 0.3056\n",
      "Epoch 85/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2979 - val_loss: 0.3041\n",
      "Epoch 86/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2986 - val_loss: 0.3039\n",
      "Epoch 87/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2977 - val_loss: 0.3035\n",
      "Epoch 88/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2978 - val_loss: 0.3049\n",
      "Epoch 89/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2980 - val_loss: 0.3038\n",
      "Epoch 90/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.2984 - val_loss: 0.3042\n",
      "Epoch 91/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2983 - val_loss: 0.3045\n",
      "Epoch 92/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2974 - val_loss: 0.3043\n",
      "Epoch 93/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.2984 - val_loss: 0.3063\n",
      "Epoch 94/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2980 - val_loss: 0.3040\n",
      "Epoch 95/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2980 - val_loss: 0.3038\n",
      "Epoch 96/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2977 - val_loss: 0.3042\n",
      "Epoch 97/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2971 - val_loss: 0.3050\n",
      "Epoch 98/100\n",
      "19930/19930 [==============================] - 1s 33us/step - loss: 0.2975 - val_loss: 0.3063\n",
      "Epoch 99/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2982 - val_loss: 0.3094\n",
      "Epoch 100/100\n",
      "19930/19930 [==============================] - 1s 31us/step - loss: 0.2974 - val_loss: 0.3049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55090407856423618"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_all.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_31 = model.fit(X_midrange,y_midrange,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_31 = np.sqrt(min(model_31.history[\"val_loss\"]))\n",
    "RMSE_model_31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This limited focus significantly improved our model! Let's see how our model performs when we try to predict entire data set, which includes data the model has not seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6882855442995387"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_31_entire_predictions = model_31.model.predict(X_all)\n",
    "np.sqrt(mean_squared_error(y_all,model_31_entire_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the performance of the model significantly drops when we try to predict using the data set that also includes outliers.\n",
    "\n",
    "When we look at our models training range, even here we notice that our model has a lot of room for improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1300027b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_31_predictions = model_31.model.predict(X_midrange)\n",
    "plt.scatter(y= y_midrange,x = model_31_predictions,s = 2, alpha= 0.2, c = \"r\")\n",
    "plt.ylabel(\"Expected\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19930 samples, validate on 13288 samples\n",
      "Epoch 1/100\n",
      "19930/19930 [==============================] - 2s 116us/step - loss: 2.3492 - val_loss: 0.3359\n",
      "Epoch 2/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3145 - val_loss: 0.3110\n",
      "Epoch 3/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3084 - val_loss: 0.3134\n",
      "Epoch 4/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3052 - val_loss: 0.3081\n",
      "Epoch 5/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.3036 - val_loss: 0.3096\n",
      "Epoch 6/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.3039 - val_loss: 0.3067\n",
      "Epoch 7/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.3042 - val_loss: 0.3066\n",
      "Epoch 8/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.3037 - val_loss: 0.3105\n",
      "Epoch 9/100\n",
      "19930/19930 [==============================] - 1s 51us/step - loss: 0.3040 - val_loss: 0.3069\n",
      "Epoch 10/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3037 - val_loss: 0.3053\n",
      "Epoch 11/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.3028 - val_loss: 0.3081\n",
      "Epoch 12/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.3032 - val_loss: 0.3099\n",
      "Epoch 13/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.3020 - val_loss: 0.3088\n",
      "Epoch 14/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3025 - val_loss: 0.3102\n",
      "Epoch 15/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3022 - val_loss: 0.3230\n",
      "Epoch 16/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3029 - val_loss: 0.3057\n",
      "Epoch 17/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3022 - val_loss: 0.3054\n",
      "Epoch 18/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.3026 - val_loss: 0.3118\n",
      "Epoch 19/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.3024 - val_loss: 0.3149\n",
      "Epoch 20/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.3021 - val_loss: 0.3057\n",
      "Epoch 21/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3017 - val_loss: 0.3072\n",
      "Epoch 22/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3023 - val_loss: 0.3196\n",
      "Epoch 23/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.3015 - val_loss: 0.3044\n",
      "Epoch 24/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.3022 - val_loss: 0.3058\n",
      "Epoch 25/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.3007 - val_loss: 0.3066\n",
      "Epoch 26/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.3016 - val_loss: 0.3094\n",
      "Epoch 27/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3024 - val_loss: 0.3055\n",
      "Epoch 28/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3015 - val_loss: 0.3056\n",
      "Epoch 29/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.3007 - val_loss: 0.3043\n",
      "Epoch 30/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.3007 - val_loss: 0.3039\n",
      "Epoch 31/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.3003 - val_loss: 0.3047\n",
      "Epoch 32/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.3005 - val_loss: 0.3041\n",
      "Epoch 33/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2997 - val_loss: 0.3053\n",
      "Epoch 34/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2997 - val_loss: 0.3126\n",
      "Epoch 35/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.3000 - val_loss: 0.3046\n",
      "Epoch 36/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2995 - val_loss: 0.3066\n",
      "Epoch 37/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2995 - val_loss: 0.3036\n",
      "Epoch 38/100\n",
      "19930/19930 [==============================] - 1s 36us/step - loss: 0.3000 - val_loss: 0.3059\n",
      "Epoch 39/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.3013 - val_loss: 0.3037\n",
      "Epoch 40/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2996 - val_loss: 0.3100\n",
      "Epoch 41/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.3000 - val_loss: 0.3061\n",
      "Epoch 42/100\n",
      "19930/19930 [==============================] - 1s 51us/step - loss: 0.2987 - val_loss: 0.3074\n",
      "Epoch 43/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.3002 - val_loss: 0.3040\n",
      "Epoch 44/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.3001 - val_loss: 0.3041\n",
      "Epoch 45/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2991 - val_loss: 0.3040\n",
      "Epoch 46/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2985 - val_loss: 0.3049\n",
      "Epoch 47/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2989 - val_loss: 0.3064\n",
      "Epoch 48/100\n",
      "19930/19930 [==============================] - 1s 48us/step - loss: 0.2991 - val_loss: 0.3053\n",
      "Epoch 49/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2998 - val_loss: 0.3066\n",
      "Epoch 50/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.3003 - val_loss: 0.3055\n",
      "Epoch 51/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2986 - val_loss: 0.3082\n",
      "Epoch 52/100\n",
      "19930/19930 [==============================] - 1s 37us/step - loss: 0.2994 - val_loss: 0.3050\n",
      "Epoch 53/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2992 - val_loss: 0.3120\n",
      "Epoch 54/100\n",
      "19930/19930 [==============================] - 1s 38us/step - loss: 0.2997 - val_loss: 0.3038\n",
      "Epoch 55/100\n",
      "19930/19930 [==============================] - 1s 48us/step - loss: 0.2988 - val_loss: 0.3086\n",
      "Epoch 56/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2987 - val_loss: 0.3056\n",
      "Epoch 57/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2981 - val_loss: 0.3040\n",
      "Epoch 58/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.2981 - val_loss: 0.3051\n",
      "Epoch 59/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2987 - val_loss: 0.3036\n",
      "Epoch 60/100\n",
      "19930/19930 [==============================] - 1s 49us/step - loss: 0.2987 - val_loss: 0.3073\n",
      "Epoch 61/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.2987 - val_loss: 0.3048\n",
      "Epoch 62/100\n",
      "19930/19930 [==============================] - 1s 40us/step - loss: 0.2984 - val_loss: 0.3042\n",
      "Epoch 63/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2995 - val_loss: 0.3071\n",
      "Epoch 64/100\n",
      "19930/19930 [==============================] - 1s 48us/step - loss: 0.3001 - val_loss: 0.3047\n",
      "Epoch 65/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2983 - val_loss: 0.3055\n",
      "Epoch 66/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.2988 - val_loss: 0.3128\n",
      "Epoch 67/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2982 - val_loss: 0.3037\n",
      "Epoch 68/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2973 - val_loss: 0.3038\n",
      "Epoch 69/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.2990 - val_loss: 0.3037\n",
      "Epoch 70/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2981 - val_loss: 0.3067\n",
      "Epoch 71/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2980 - val_loss: 0.3040\n",
      "Epoch 72/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.2978 - val_loss: 0.3058\n",
      "Epoch 73/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2986 - val_loss: 0.3264\n",
      "Epoch 74/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2992 - val_loss: 0.3200\n",
      "Epoch 75/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2980 - val_loss: 0.3084\n",
      "Epoch 76/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2971 - val_loss: 0.3039\n",
      "Epoch 77/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2992 - val_loss: 0.3051\n",
      "Epoch 78/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.2979 - val_loss: 0.3050\n",
      "Epoch 79/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.2977 - val_loss: 0.3046\n",
      "Epoch 80/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2983 - val_loss: 0.3117\n",
      "Epoch 81/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2984 - val_loss: 0.3042\n",
      "Epoch 82/100\n",
      "19930/19930 [==============================] - 1s 42us/step - loss: 0.2970 - val_loss: 0.3082\n",
      "Epoch 83/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2975 - val_loss: 0.3046\n",
      "Epoch 84/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2976 - val_loss: 0.3037\n",
      "Epoch 85/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2967 - val_loss: 0.3042\n",
      "Epoch 86/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2975 - val_loss: 0.3048\n",
      "Epoch 87/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2989 - val_loss: 0.3128\n",
      "Epoch 88/100\n",
      "19930/19930 [==============================] - 1s 48us/step - loss: 0.2980 - val_loss: 0.3101\n",
      "Epoch 89/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2982 - val_loss: 0.3045\n",
      "Epoch 90/100\n",
      "19930/19930 [==============================] - 1s 39us/step - loss: 0.2977 - val_loss: 0.3039\n",
      "Epoch 91/100\n",
      "19930/19930 [==============================] - 1s 41us/step - loss: 0.2970 - val_loss: 0.3036\n",
      "Epoch 92/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2969 - val_loss: 0.3036\n",
      "Epoch 93/100\n",
      "19930/19930 [==============================] - 1s 43us/step - loss: 0.2976 - val_loss: 0.3135\n",
      "Epoch 94/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2973 - val_loss: 0.3059\n",
      "Epoch 95/100\n",
      "19930/19930 [==============================] - 1s 45us/step - loss: 0.2975 - val_loss: 0.3066\n",
      "Epoch 96/100\n",
      "19930/19930 [==============================] - 1s 47us/step - loss: 0.2975 - val_loss: 0.3058\n",
      "Epoch 97/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2969 - val_loss: 0.3035\n",
      "Epoch 98/100\n",
      "19930/19930 [==============================] - 1s 44us/step - loss: 0.2965 - val_loss: 0.3035\n",
      "Epoch 99/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2975 - val_loss: 0.3038\n",
      "Epoch 100/100\n",
      "19930/19930 [==============================] - 1s 46us/step - loss: 0.2968 - val_loss: 0.3040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5509028837689991"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X_all.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(100,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_32 = model.fit(X_midrange,y_midrange,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_32 = np.sqrt(min(model_32.history[\"val_loss\"]))\n",
    "RMSE_model_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take home message: while removing outliers might help to improve model performance, this does not necessarily make the model robust for making predictions when model is exposed to data set that contain outliers. \n",
    "\n",
    "This gives us some intuition, but the model requires more improvement. Perhaps we are lacking certain key features to correctly capture the variance in the system.\n",
    "\n",
    "#### Little experiment: changing the optimizer to rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 1s 38us/step - loss: 2.9808 - val_loss: 0.6118\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.5233 - val_loss: 0.4702\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4826 - val_loss: 0.4604\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4752 - val_loss: 0.4564\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4723 - val_loss: 0.4540\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4699 - val_loss: 0.4531\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4691 - val_loss: 0.4537\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4680 - val_loss: 0.4590\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4683 - val_loss: 0.4588\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4677 - val_loss: 0.4521\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4676 - val_loss: 0.4674\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4677 - val_loss: 0.4532\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4667 - val_loss: 0.4527\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4669 - val_loss: 0.4522\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4665 - val_loss: 0.4578\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4657 - val_loss: 0.4540\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4656 - val_loss: 0.4535\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 0.4653 - val_loss: 0.4570\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4654 - val_loss: 0.4539\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4655 - val_loss: 0.4513\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4654 - val_loss: 0.4523\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4650 - val_loss: 0.4520\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4646 - val_loss: 0.4521\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4649 - val_loss: 0.4565\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4644 - val_loss: 0.4546\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4645 - val_loss: 0.4527\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4643 - val_loss: 0.4525\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4641 - val_loss: 0.4514\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4637 - val_loss: 0.4506\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4637 - val_loss: 0.4508\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4637 - val_loss: 0.4540\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4633 - val_loss: 0.4511\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4636 - val_loss: 0.4508\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4629 - val_loss: 0.4507\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4631 - val_loss: 0.4542\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4627 - val_loss: 0.4512\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4629 - val_loss: 0.4510\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4626 - val_loss: 0.4517\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4628 - val_loss: 0.4510\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4626 - val_loss: 0.4503\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4627 - val_loss: 0.4514\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4623 - val_loss: 0.4507\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4622 - val_loss: 0.4542\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4622 - val_loss: 0.4512\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4619 - val_loss: 0.4582\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4618 - val_loss: 0.4540\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4617 - val_loss: 0.4535\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4617 - val_loss: 0.4508\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4615 - val_loss: 0.4507\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4613 - val_loss: 0.4569\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4611 - val_loss: 0.4601\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4613 - val_loss: 0.4632\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4610 - val_loss: 0.4512\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4609 - val_loss: 0.4602\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4607 - val_loss: 0.4529\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4607 - val_loss: 0.4512\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4607 - val_loss: 0.4504\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4610 - val_loss: 0.4510\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4609 - val_loss: 0.4517\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4608 - val_loss: 0.4572\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4606 - val_loss: 0.4509\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4606 - val_loss: 0.4514\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4607 - val_loss: 0.4529\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4603 - val_loss: 0.4527\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4607 - val_loss: 0.4512\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4602 - val_loss: 0.4520\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4600 - val_loss: 0.4516\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4601 - val_loss: 0.4558\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4604 - val_loss: 0.4519\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4602 - val_loss: 0.4527\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4602 - val_loss: 0.4548\n",
      "Epoch 72/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4602 - val_loss: 0.4512\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 0.4598 - val_loss: 0.4521\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4598 - val_loss: 0.4553\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4600 - val_loss: 0.4524\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4597 - val_loss: 0.4511\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4598 - val_loss: 0.4544\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4598 - val_loss: 0.4524\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4600 - val_loss: 0.4508\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4598 - val_loss: 0.4528\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4600 - val_loss: 0.4548\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4593 - val_loss: 0.4549\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4597 - val_loss: 0.4510\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4592 - val_loss: 0.4512\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4597 - val_loss: 0.4535\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.4596 - val_loss: 0.4600\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 1s 32us/step - loss: 0.4596 - val_loss: 0.4521\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4597 - val_loss: 0.4552\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4593 - val_loss: 0.4532\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4592 - val_loss: 0.4527\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 33us/step - loss: 0.4594 - val_loss: 0.4534\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 34us/step - loss: 0.4595 - val_loss: 0.4524\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4593 - val_loss: 0.4519\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4592 - val_loss: 0.4520\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4588 - val_loss: 0.4529\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4591 - val_loss: 0.4525\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4591 - val_loss: 0.4519\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4592 - val_loss: 0.4517\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4588 - val_loss: 0.4570\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4589 - val_loss: 0.4521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67107471210152037"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"mean_squared_error\")\n",
    "model_33 = model.fit(X,y,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_33 = np.sqrt(min(model_33.history[\"val_loss\"]))\n",
    "RMSE_model_33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22239 samples, validate on 14827 samples\n",
      "Epoch 1/100\n",
      "22239/22239 [==============================] - 1s 45us/step - loss: 5.3687 - val_loss: 1.0082\n",
      "Epoch 2/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.6180 - val_loss: 0.4853\n",
      "Epoch 3/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4883 - val_loss: 0.4631\n",
      "Epoch 4/100\n",
      "22239/22239 [==============================] - 1s 31us/step - loss: 0.4780 - val_loss: 0.4577\n",
      "Epoch 5/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4739 - val_loss: 0.4633\n",
      "Epoch 6/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4716 - val_loss: 0.4604\n",
      "Epoch 7/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4705 - val_loss: 0.4523\n",
      "Epoch 8/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4696 - val_loss: 0.4516\n",
      "Epoch 9/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4686 - val_loss: 0.4544\n",
      "Epoch 10/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4680 - val_loss: 0.4514\n",
      "Epoch 11/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4681 - val_loss: 0.4515\n",
      "Epoch 12/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4676 - val_loss: 0.4509\n",
      "Epoch 13/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4674 - val_loss: 0.4528\n",
      "Epoch 14/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4667 - val_loss: 0.4544\n",
      "Epoch 15/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4661 - val_loss: 0.4534\n",
      "Epoch 16/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4660 - val_loss: 0.4510\n",
      "Epoch 17/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4655 - val_loss: 0.4540\n",
      "Epoch 18/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4656 - val_loss: 0.4509\n",
      "Epoch 19/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4649 - val_loss: 0.4512\n",
      "Epoch 20/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4643 - val_loss: 0.4506\n",
      "Epoch 21/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4653 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4643 - val_loss: 0.4515\n",
      "Epoch 23/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4607\n",
      "Epoch 24/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4645 - val_loss: 0.4503\n",
      "Epoch 25/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4643 - val_loss: 0.4519\n",
      "Epoch 26/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4639 - val_loss: 0.4543\n",
      "Epoch 27/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4637 - val_loss: 0.4504\n",
      "Epoch 28/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4506\n",
      "Epoch 29/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4634 - val_loss: 0.4504\n",
      "Epoch 30/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4632 - val_loss: 0.4516\n",
      "Epoch 31/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4629 - val_loss: 0.4575\n",
      "Epoch 32/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4629 - val_loss: 0.4534\n",
      "Epoch 33/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4633 - val_loss: 0.4511\n",
      "Epoch 34/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4631 - val_loss: 0.4502\n",
      "Epoch 35/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4625 - val_loss: 0.4539\n",
      "Epoch 36/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4627 - val_loss: 0.4522\n",
      "Epoch 37/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4628 - val_loss: 0.4504\n",
      "Epoch 38/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4623 - val_loss: 0.4587\n",
      "Epoch 39/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4619 - val_loss: 0.4542\n",
      "Epoch 40/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4620 - val_loss: 0.4557\n",
      "Epoch 41/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4616 - val_loss: 0.4522\n",
      "Epoch 42/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4616 - val_loss: 0.4502\n",
      "Epoch 43/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4617 - val_loss: 0.4506\n",
      "Epoch 44/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4616 - val_loss: 0.4519\n",
      "Epoch 45/100\n",
      "22239/22239 [==============================] - 1s 29us/step - loss: 0.4611 - val_loss: 0.4506\n",
      "Epoch 46/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4504\n",
      "Epoch 47/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4611 - val_loss: 0.4510\n",
      "Epoch 48/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 0.4611 - val_loss: 0.4499\n",
      "Epoch 49/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4604 - val_loss: 0.4505\n",
      "Epoch 50/100\n",
      "22239/22239 [==============================] - 1s 30us/step - loss: 0.4603 - val_loss: 0.4540\n",
      "Epoch 51/100\n",
      "22239/22239 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4547\n",
      "Epoch 52/100\n",
      "22239/22239 [==============================] - 0s 22us/step - loss: 0.4607 - val_loss: 0.4526\n",
      "Epoch 53/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4603 - val_loss: 0.4506\n",
      "Epoch 54/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4598 - val_loss: 0.4513\n",
      "Epoch 55/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4513\n",
      "Epoch 56/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4599 - val_loss: 0.4506\n",
      "Epoch 57/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4601 - val_loss: 0.4508\n",
      "Epoch 58/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4596 - val_loss: 0.4529\n",
      "Epoch 59/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4593 - val_loss: 0.4507\n",
      "Epoch 60/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4595 - val_loss: 0.4522\n",
      "Epoch 61/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4596 - val_loss: 0.4513\n",
      "Epoch 62/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4589 - val_loss: 0.4518\n",
      "Epoch 63/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4587 - val_loss: 0.4512\n",
      "Epoch 64/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4589 - val_loss: 0.4511\n",
      "Epoch 65/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4584 - val_loss: 0.4509\n",
      "Epoch 66/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4587 - val_loss: 0.4513\n",
      "Epoch 67/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4587 - val_loss: 0.4512\n",
      "Epoch 68/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4581 - val_loss: 0.4591\n",
      "Epoch 69/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4582 - val_loss: 0.4512\n",
      "Epoch 70/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4579 - val_loss: 0.4513\n",
      "Epoch 71/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4579 - val_loss: 0.4511\n",
      "Epoch 72/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4575 - val_loss: 0.4510\n",
      "Epoch 73/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4576 - val_loss: 0.4550\n",
      "Epoch 74/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4573 - val_loss: 0.4500\n",
      "Epoch 75/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4572 - val_loss: 0.4505\n",
      "Epoch 76/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4574 - val_loss: 0.4507\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4572 - val_loss: 0.4515\n",
      "Epoch 78/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4567 - val_loss: 0.4586\n",
      "Epoch 79/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4568 - val_loss: 0.4519\n",
      "Epoch 80/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4568 - val_loss: 0.4506\n",
      "Epoch 81/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4563 - val_loss: 0.4515\n",
      "Epoch 82/100\n",
      "22239/22239 [==============================] - 1s 27us/step - loss: 0.4565 - val_loss: 0.4513\n",
      "Epoch 83/100\n",
      "22239/22239 [==============================] - 1s 28us/step - loss: 0.4563 - val_loss: 0.4500\n",
      "Epoch 84/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4554 - val_loss: 0.4545\n",
      "Epoch 85/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4557 - val_loss: 0.4515\n",
      "Epoch 86/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4556 - val_loss: 0.4510\n",
      "Epoch 87/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4559 - val_loss: 0.4500\n",
      "Epoch 88/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4557 - val_loss: 0.4514\n",
      "Epoch 89/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4556 - val_loss: 0.4544\n",
      "Epoch 90/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4555 - val_loss: 0.4526\n",
      "Epoch 91/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4555 - val_loss: 0.4547\n",
      "Epoch 92/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4551 - val_loss: 0.4501\n",
      "Epoch 93/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4552 - val_loss: 0.4512\n",
      "Epoch 94/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4555 - val_loss: 0.4572\n",
      "Epoch 95/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4552 - val_loss: 0.4505\n",
      "Epoch 96/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4549 - val_loss: 0.4514\n",
      "Epoch 97/100\n",
      "22239/22239 [==============================] - 1s 26us/step - loss: 0.4548 - val_loss: 0.4517\n",
      "Epoch 98/100\n",
      "22239/22239 [==============================] - 0s 21us/step - loss: 0.4547 - val_loss: 0.4505\n",
      "Epoch 99/100\n",
      "22239/22239 [==============================] - 1s 24us/step - loss: 0.4545 - val_loss: 0.4513\n",
      "Epoch 100/100\n",
      "22239/22239 [==============================] - 1s 25us/step - loss: 0.4546 - val_loss: 0.4505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67077493920539943"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = X.shape[1],activation='relu'))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"mean_squared_error\")\n",
    "model_34 = model.fit(X,y,epochs = 100,batch_size=100,validation_split= 0.4)\n",
    "RMSE_model_34 = np.sqrt(min(model_34.history[\"val_loss\"]))\n",
    "RMSE_model_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
