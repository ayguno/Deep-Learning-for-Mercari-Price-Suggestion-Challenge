{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "In this exercise I will use the features I previously engineered using R and Kaggle Mercari Price challenge data set. \n",
    "\n",
    "We will start with loading the libraries and functions we will need during the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import SGD\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_condition_id  price  shipping  no.brand_name  log.excl.description  \\\n",
       "1                  1    8.0         0              1                   0.0   \n",
       "2                  2   39.0         1              1                   0.0   \n",
       "3                  1   30.0         1              0                   0.0   \n",
       "4                  2  470.0         1              0                   0.0   \n",
       "5                  2   22.0         0              0                   0.0   \n",
       "\n",
       "   excl.name  dollar.description  fancy.categories  cheap.categories  \\\n",
       "1          0                   0                 0                 0   \n",
       "2          0                   0                 0                 0   \n",
       "3          0                   0                 0                 0   \n",
       "4          0                   0                 0                 0   \n",
       "5          0                   0                 0                 0   \n",
       "\n",
       "   fancy.brands        ...         now  cheap  buy  excellent  great  \\\n",
       "1             0        ...           0      0    0          0      0   \n",
       "2             0        ...           0      0    0          0      0   \n",
       "3             0        ...           0      0    0          0      0   \n",
       "4             1        ...           0      0    0          0      0   \n",
       "5             0        ...           0      0    0          0      0   \n",
       "\n",
       "   michael.brand  jordan.name  iphon.name  bundl.name  cap.letter.brand  \n",
       "1              0            0           0           0                 1  \n",
       "2              0            0           0           0                 1  \n",
       "3              0            0           0           1                 0  \n",
       "4              0            0           0           0                 6  \n",
       "5              0            0           0           0                 2  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load the mini subtraining set we prepared previously\n",
    "mini_subtrain = pd.read_csv(\"mini_subtrain.csv\", index_col = 0)\n",
    "mini_subtrain.shape\n",
    "mini_subtrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_subtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "mini_subtrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sucessfully reading the verifying the training data set we have previously constructed using R, we can start building a small neural network and training it by using our data.\n",
    "\n",
    "First we start with seperating predictors and response arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = np.array(mini_subtrain.drop([\"price\"], axis=1))\n",
    "# We will log transform the target variable as we have performed in R\n",
    "target = np.array(np.log(mini_subtrain.price + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can start building our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 98us/step - loss: 1.3796 - val_loss: 0.5341\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4969 - val_loss: 0.4515\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4742 - val_loss: 0.4472\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4719 - val_loss: 0.4500\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4720 - val_loss: 0.4457\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 65us/step - loss: 0.4704 - val_loss: 0.4510\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 64us/step - loss: 0.4698 - val_loss: 0.4503\n"
     ]
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_1 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our loss function is \"mean squared error\". We will take the square root of this to follow \"root mean squared error\" (RMSE). We also keep in mind that the target is log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66764026261785125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_1 = np.sqrt(min(model_1.history[\"val_loss\"]))\n",
    "RMSE_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this RMSE is close to what we have obtained other machine learning algorithms previously. Therefore, we will continue our experiment by increasing model complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 25s 953us/step - loss: 0.6853 - val_loss: 0.4837\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 25s 949us/step - loss: 0.4860 - val_loss: 0.4547\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 25s 968us/step - loss: 0.4801 - val_loss: 0.4599\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 29s 1ms/step - loss: 0.4754 - val_loss: 0.4593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67429454720879667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1000,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_2 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_2 = np.sqrt(min(model_2.history[\"val_loss\"]))\n",
    "RMSE_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our first model is already at its capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 111us/step - loss: 1.0952 - val_loss: 0.4894\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4874 - val_loss: 0.4550\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4755 - val_loss: 0.4524\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4727 - val_loss: 0.4561\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4711 - val_loss: 0.4517\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 0.4686 - val_loss: 0.4507\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 68us/step - loss: 0.4679 - val_loss: 0.4551\n",
      "Epoch 8/30\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 0.4683 - val_loss: 0.4571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67136944170640311"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_3 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_3 = np.sqrt(min(model_3.history[\"val_loss\"]))\n",
    "RMSE_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we reached to model capacity even with a relatively simple network. Next, we will try if we can reduce the bias by training a larger data set, which we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrain = pd.read_csv(\"subtrain.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(741269, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741269 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741269 non-null int64\n",
      "price                   741269 non-null float64\n",
      "shipping                741269 non-null int64\n",
      "no.brand_name           741269 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741269 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741269 non-null int64\n",
      "cheap.categories        741269 non-null int64\n",
      "fancy.brands            741269 non-null int64\n",
      "cheap.brands            741269 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741269 non-null int64\n",
      "jordan.name             741269 non-null int64\n",
      "iphon.name              741269 non-null int64\n",
      "bundl.name              741269 non-null int64\n",
      "cap.letter.brand        741269 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "#We need to check for missing values in the data set\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741268 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741268 non-null int64\n",
      "price                   741268 non-null float64\n",
      "shipping                741268 non-null int64\n",
      "no.brand_name           741268 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741268 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741268 non-null int64\n",
      "cheap.categories        741268 non-null int64\n",
      "fancy.brands            741268 non-null int64\n",
      "cheap.brands            741268 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741268 non-null int64\n",
      "jordan.name             741268 non-null int64\n",
      "iphon.name              741268 non-null int64\n",
      "bundl.name              741268 non-null int64\n",
      "cap.letter.brand        741268 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "subtrain = subtrain.dropna()\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(subtrain.price + 1)\n",
    "X = subtrain.drop([\"price\"],axis = 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.5185 - val_loss: 0.4680\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4695 - val_loss: 0.4894\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4674 - val_loss: 0.4629\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4664 - val_loss: 0.4629\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4658 - val_loss: 0.4662\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4654 - val_loss: 0.4617\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4650 - val_loss: 0.4623\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4649 - val_loss: 0.4617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6794526310377843"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_4 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_4 = np.sqrt(min(model_4.history[\"val_loss\"]))\n",
    "RMSE_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.5029 - val_loss: 0.4714\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4684 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4669 - val_loss: 0.4626\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4658 - val_loss: 0.4621\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4655 - val_loss: 0.4626\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4646 - val_loss: 0.4618\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4644 - val_loss: 0.4605\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4641 - val_loss: 0.4640\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4639 - val_loss: 0.4614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67862169258002358"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_5 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_5 = np.sqrt(min(model_5.history[\"val_loss\"]))\n",
    "RMSE_model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4934 - val_loss: 0.4676\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4687 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4662 - val_loss: 0.4616\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4653 - val_loss: 0.4616\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4644 - val_loss: 0.4615\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4638 - val_loss: 0.4613\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4634 - val_loss: 0.4608\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4631 - val_loss: 0.4618\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4629 - val_loss: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67883387278347673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_6 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_6 = np.sqrt(min(model_6.history[\"val_loss\"]))\n",
    "RMSE_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4880 - val_loss: 0.4679\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4714 - val_loss: 0.4882\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4688 - val_loss: 0.4665\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4675 - val_loss: 0.4698\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4666 - val_loss: 0.4640\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 63us/step - loss: 0.4676 - val_loss: 0.4640\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 33s 65us/step - loss: 0.4661 - val_loss: 0.4626\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4653 - val_loss: 0.4641\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 30s 58us/step - loss: 0.4647 - val_loss: 0.4636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68016471861743943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing optimizer to SGD\n",
    "sgd_optimizer = SGD(lr = 0.01)\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= sgd_optimizer, loss= \"mean_squared_error\")\n",
    "model_7 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_7 = np.sqrt(min(model_7.history[\"val_loss\"]))\n",
    "RMSE_model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/20\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4953 - val_loss: 0.4869\n",
      "Epoch 2/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4685 - val_loss: 0.4672\n",
      "Epoch 3/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4669 - val_loss: 0.4635\n",
      "Epoch 4/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4659 - val_loss: 0.4623\n",
      "Epoch 5/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4668\n",
      "Epoch 6/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4623\n",
      "Epoch 7/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4645 - val_loss: 0.4626\n",
      "Epoch 8/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4645 - val_loss: 0.4622\n",
      "Epoch 9/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4639 - val_loss: 0.4612\n",
      "Epoch 10/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4658 - val_loss: 0.4638\n",
      "Epoch 11/20\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4636 - val_loss: 0.4611\n",
      "Epoch 12/20\n",
      "518887/518887 [==============================] - 40s 78us/step - loss: 0.4634 - val_loss: 0.4606\n",
      "Epoch 13/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4632 - val_loss: 0.4617\n",
      "Epoch 14/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4632 - val_loss: 0.4612\n",
      "Epoch 15/20\n",
      "518887/518887 [==============================] - 36s 70us/step - loss: 0.4631 - val_loss: 0.4602\n",
      "Epoch 16/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4629 - val_loss: 0.4640\n",
      "Epoch 17/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4628 - val_loss: 0.4610\n",
      "Epoch 18/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4629 - val_loss: 0.4642\n",
      "Epoch 19/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4625 - val_loss: 0.4607\n",
      "Epoch 20/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4624 - val_loss: 0.4647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67839322279547709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing fixed 20 epochs\n",
    "#estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_8 = model.fit(X,y, epochs= 20, validation_split= 0.3)\n",
    "RMSE_model_8 = np.sqrt(min(model_8.history[\"val_loss\"]))\n",
    "RMSE_model_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.5663 - val_loss: 0.4738\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4740 - val_loss: 0.4702\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4735 - val_loss: 0.4721\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 32s 62us/step - loss: 0.4726 - val_loss: 0.4706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68569114759373795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing final activation function\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_9 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_9 = np.sqrt(min(model_9.history[\"val_loss\"]))\n",
    "RMSE_model_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to learn other means of model optimization and try to test here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the small data set for training once again:\n",
    "os.listdir()\n",
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis= 1).values\n",
    "y = np.log(df.price+1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # This is a Keras wrapper for sklearn we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt I loaded some sklearn functions to use them in model evaluation, as well as a Keras wrapper for sklearn.\n",
    "\n",
    "In this case, the Keras wrappers we will use take a function as argument, which we will define to create the neural network model structure.\n",
    "\n",
    "Let's define the basic_model function:\n",
    "\n",
    "- We use a sequential model structure.\n",
    "- We will have a simple 2-layered model (input and output layer).\n",
    "- We don't use any activation function at the output layer (since we are performing regression, we want values to be evaluated without transformation). \n",
    "- We will use ADAM optimization function and optimize mean squared error as our loss function\n",
    "- We will use the same number of neurons(nodes) as the the number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    \"\"\"Creates, compiles and returns the basic NN\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "    model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define our regression estimator using keras wrapper KerasRegressor we imported above. This estimator function will receive:\n",
    "\n",
    "- the function which creates the NN model\n",
    "- parameters we normally enter in .fit() function (e.g: number of epochs or batch size), note that in this case we are doing the computation in batch mode, so we use nb_epoch argument\n",
    "\n",
    "we will also need to set the random number generator seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4\n",
    "np.random.seed(seed)\n",
    "regression_estimator = KerasRegressor(build_fn= basic_model, \n",
    "                                     nb_epoch = 100,\n",
    "                                     batch_size = 5,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the model using 10-fold cross-validation. Note that this is the actual step in which we are fitting the model on the data. We are collecting the model evaluation in a object we called **results**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c75cb95c7d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set of the k-folds and random state to partition the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregression_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits= 10, random_state= seed) # Set of the k-folds and random state to partition the data\n",
    "results = cross_val_score(estimator=regression_estimator, X = X,y = y,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results) # Note that results is an array that contains MSE scores for each k-fold cv step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4844004 , -0.46776465, -0.49775278, -0.4527221 , -0.46104527,\n",
       "       -0.48631333, -0.46733199, -0.45774577, -0.47870872, -0.46569664])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47194816461575667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mse returned by CV function is negative! This is quite counter intuitive and confusing. Checking the documentation in GitHub, I understand that this is not a useful way of estimating model performance, perhaps the wrapper has problems.\n",
    "\n",
    "I am going back to the original way I fit the models, that is using a single split for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 2.9459 - val_loss: 1.0557\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.9681 - val_loss: 0.8441\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.7625 - val_loss: 0.6377\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.5757 - val_loss: 0.4834\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4858 - val_loss: 0.4520\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4749 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4741 - val_loss: 0.4503\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4745 - val_loss: 0.4501\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4741 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4746 - val_loss: 0.4512\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4745 - val_loss: 0.4561\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4741 - val_loss: 0.4511\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4739 - val_loss: 0.4508\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4743 - val_loss: 0.4525\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4743 - val_loss: 0.4533\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4728 - val_loss: 0.4498\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4720 - val_loss: 0.4519\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4708 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4702 - val_loss: 0.4506\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4692 - val_loss: 0.4498\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4468\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4690 - val_loss: 0.4468\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4689 - val_loss: 0.4471\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4681 - val_loss: 0.4479\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4686 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4471\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4682 - val_loss: 0.4494\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4681 - val_loss: 0.4471\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4680 - val_loss: 0.4492\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4675 - val_loss: 0.4616\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4678 - val_loss: 0.4472\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4675 - val_loss: 0.4474\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4670 - val_loss: 0.4472\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4675 - val_loss: 0.4467\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4468\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4669 - val_loss: 0.4470\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4667 - val_loss: 0.4460\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4673 - val_loss: 0.4503\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4671 - val_loss: 0.4466\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4674 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4666 - val_loss: 0.4492\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4481\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4662 - val_loss: 0.4459\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4670 - val_loss: 0.4470\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4652 - val_loss: 0.4564\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4665 - val_loss: 0.4498\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4660 - val_loss: 0.4462\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4659 - val_loss: 0.4465\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4658 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4661 - val_loss: 0.4469\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4652 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4655 - val_loss: 0.4479\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4650 - val_loss: 0.4469\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4651 - val_loss: 0.4470\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4644 - val_loss: 0.4502\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4650 - val_loss: 0.4466\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4497\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4647 - val_loss: 0.4468\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4489\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4487\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4466\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4653 - val_loss: 0.4474\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4537\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4650 - val_loss: 0.4473\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4649 - val_loss: 0.4471\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4482\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4646 - val_loss: 0.4480\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4645 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4476\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4645 - val_loss: 0.4470\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4483\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4638 - val_loss: 0.4478\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4521\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4476\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4644 - val_loss: 0.4467\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4480\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4646 - val_loss: 0.4481\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4639 - val_loss: 0.4499\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4642 - val_loss: 0.4497\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4640 - val_loss: 0.4472\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4471\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4638 - val_loss: 0.4498\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4475\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4479\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4638 - val_loss: 0.4484\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4570\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4645 - val_loss: 0.4483\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4641 - val_loss: 0.4486\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4641 - val_loss: 0.4474\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4632 - val_loss: 0.4484\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_10 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66776206455463483"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_10 = np.sqrt(min(model_10.history[\"val_loss\"]))\n",
    "RMSE_model_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight improvement over model_1. Let's continue by trying to tune model topology. \n",
    "\n",
    "We can try a more complex network, hoping that this network will extract and recombine more interactions between the available features.\n",
    "\n",
    "- We can set up deeper NNs : i.e: adding layers\n",
    "- We can set up wider NNs: i.e: adding neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 3.2844 - val_loss: 0.9369\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.8241 - val_loss: 0.6616\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5722 - val_loss: 0.4716\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4792 - val_loss: 0.4520\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4741 - val_loss: 0.4498\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4733 - val_loss: 0.4498\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4727 - val_loss: 0.4500\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4733 - val_loss: 0.4486\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4723 - val_loss: 0.4529\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4722 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4724 - val_loss: 0.4530\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4716 - val_loss: 0.4483\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4709 - val_loss: 0.4479\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4558\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4717 - val_loss: 0.4516\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4716 - val_loss: 0.4486\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4713 - val_loss: 0.4526\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4709 - val_loss: 0.4531\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4710 - val_loss: 0.4530\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4480\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4707 - val_loss: 0.4535\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4474\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4476\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4472\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4484\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4708 - val_loss: 0.4478\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4479\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4703 - val_loss: 0.4485\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4700 - val_loss: 0.4495\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4694 - val_loss: 0.4613\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4701 - val_loss: 0.4483\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4690 - val_loss: 0.4471\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4489\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4688 - val_loss: 0.4491\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4477\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4693 - val_loss: 0.4480\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4693 - val_loss: 0.4504\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4694 - val_loss: 0.4470\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4512\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4691 - val_loss: 0.4479\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4483\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4487\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4488\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4471\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4679 - val_loss: 0.4591\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4488\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4688 - val_loss: 0.4475\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4686 - val_loss: 0.4503\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4480\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4677 - val_loss: 0.4486\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4667 - val_loss: 0.4473\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4675 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4660 - val_loss: 0.4477\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4663 - val_loss: 0.4460\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4659 - val_loss: 0.4479\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4654 - val_loss: 0.4449\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4647 - val_loss: 0.4470\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4650 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4461\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4448\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4443\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4632 - val_loss: 0.4518\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4632 - val_loss: 0.4435\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4628 - val_loss: 0.4432\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4623 - val_loss: 0.4445\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4625 - val_loss: 0.4447\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4623 - val_loss: 0.4444\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4622 - val_loss: 0.4436\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4616 - val_loss: 0.4437\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.458 - 1s 32us/step - loss: 0.4616 - val_loss: 0.4450\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4608 - val_loss: 0.4436\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4481\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4609 - val_loss: 0.4447\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4616 - val_loss: 0.4434\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4436\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4455\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4603 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4607 - val_loss: 0.4449\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4602 - val_loss: 0.4436\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4430\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4457\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4609 - val_loss: 0.4438\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4608 - val_loss: 0.4434\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4454\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4601 - val_loss: 0.4586\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4607 - val_loss: 0.4440\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4442\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4599 - val_loss: 0.4439\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4593 - val_loss: 0.4441\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(12, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_11 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66555347845455159"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_11 = np.sqrt(min(model_11.history[\"val_loss\"]))\n",
    "RMSE_model_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a wider network to evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 89us/step - loss: 1.2000 - val_loss: 0.4537\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4759 - val_loss: 0.4514\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4698 - val_loss: 0.4512\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4702 - val_loss: 0.4507\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4654 - val_loss: 0.4462\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4645 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4642 - val_loss: 0.4491\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4646 - val_loss: 0.4495\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4639 - val_loss: 0.4569\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4613 - val_loss: 0.4495\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4597 - val_loss: 0.4499\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4608 - val_loss: 0.4474\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4571 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4570 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4575 - val_loss: 0.4564\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4563 - val_loss: 0.4487\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4567 - val_loss: 0.4704\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4553 - val_loss: 0.4559\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4543 - val_loss: 0.4502\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4536 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4541 - val_loss: 0.4596\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4518 - val_loss: 0.4702\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4522 - val_loss: 0.4524\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4499 - val_loss: 0.4516\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4540 - val_loss: 0.4513\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4498 - val_loss: 0.4483\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4487 - val_loss: 0.4527\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4475 - val_loss: 0.4543\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4480 - val_loss: 0.4518\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4478 - val_loss: 0.4599\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4466 - val_loss: 0.4541\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4460 - val_loss: 0.4571\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4470 - val_loss: 0.4553\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4450 - val_loss: 0.4566\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4466 - val_loss: 0.4526\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4438 - val_loss: 0.4510\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4436 - val_loss: 0.4556\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4425 - val_loss: 0.4542\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4425 - val_loss: 0.4563\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4425 - val_loss: 0.4554\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4423 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4394 - val_loss: 0.4536\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4398 - val_loss: 0.4538\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4401 - val_loss: 0.4545\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4404 - val_loss: 0.4568\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4388 - val_loss: 0.4591\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4384 - val_loss: 0.4603\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4390 - val_loss: 0.4563\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4381 - val_loss: 0.4656\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4367 - val_loss: 0.4635\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4367 - val_loss: 0.4556\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4352 - val_loss: 0.4564\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4398 - val_loss: 0.4584\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4350 - val_loss: 0.4586\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 2s 62us/step - loss: 0.4355 - val_loss: 0.4676\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4363 - val_loss: 0.4846\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4345 - val_loss: 0.4665\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4344 - val_loss: 0.4592\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4322 - val_loss: 0.4600\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4330 - val_loss: 0.4574\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4346 - val_loss: 0.4573\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4321 - val_loss: 0.4560\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4310 - val_loss: 0.4589\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4312 - val_loss: 0.4641\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4299 - val_loss: 0.4615\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4292 - val_loss: 0.4619\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4315 - val_loss: 0.4575\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4292 - val_loss: 0.4591\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4296 - val_loss: 0.4605\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4276 - val_loss: 0.4631\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4274 - val_loss: 0.4616\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4281 - val_loss: 0.4620\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4276 - val_loss: 0.4610\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4301 - val_loss: 0.4593\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4273 - val_loss: 0.4620\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4265 - val_loss: 0.4665\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4262 - val_loss: 0.4634\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4255 - val_loss: 0.4630\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4266 - val_loss: 0.4648\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4243 - val_loss: 0.4786\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4256 - val_loss: 0.4656\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4242 - val_loss: 0.4633\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4230 - val_loss: 0.4642\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4244 - val_loss: 0.4685\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4235 - val_loss: 0.4653\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.421 - 1s 52us/step - loss: 0.4229 - val_loss: 0.4667\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4215 - val_loss: 0.4664\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4212 - val_loss: 0.4697\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4218 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4204 - val_loss: 0.4637\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4224 - val_loss: 0.4678\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4214 - val_loss: 0.4690\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4213 - val_loss: 0.4697\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4206 - val_loss: 0.4886\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4210 - val_loss: 0.4723\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4187 - val_loss: 0.4833\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4196 - val_loss: 0.4683\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4193 - val_loss: 0.4724\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4176 - val_loss: 0.4672\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4178 - val_loss: 0.4779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66800683261370652"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1] * 10, input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(120, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_12 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_12 = np.sqrt(min(model_12.history[\"val_loss\"]))\n",
    "RMSE_model_12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really not any better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can scale some of the predictors to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find which columns do not have values restricted between [0,1]\n",
    "\n",
    "select_col = (df.describe().loc[[\"max\",\"min\"],:].apply(sum,axis = 0)) != 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_condition_id',\n",
       " 'log.excl.description',\n",
       " 'excl.name',\n",
       " 'dollar.description',\n",
       " 'sale',\n",
       " 'free',\n",
       " 'save',\n",
       " 'deal',\n",
       " 'good',\n",
       " 'now',\n",
       " 'cheap',\n",
       " 'buy',\n",
       " 'excellent',\n",
       " 'great',\n",
       " 'cap.letter.brand']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_features =  df.columns[select_col].tolist() # Select the relevant columns except target\n",
    "select_features.remove(\"price\")\n",
    "select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (making features resemble z-distribution)\n",
    "stdscale = preprocessing.StandardScaler().fit(df[select_features]) # Create a preprocessing object on select features\n",
    "df_stdscale = stdscale.transform(df[select_features]) #Note that it returns a np.array with only used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.027829e-15\n",
       "1    -2.288079e-16\n",
       "2     5.450259e-15\n",
       "3     3.437801e-15\n",
       "4     8.985525e-16\n",
       "5     1.658329e-15\n",
       "6     4.924806e-17\n",
       "7     1.064549e-15\n",
       "8    -1.118217e-15\n",
       "9    -1.466933e-15\n",
       "10    1.694972e-15\n",
       "11    1.282229e-15\n",
       "12    1.338746e-16\n",
       "13   -6.921656e-16\n",
       "14   -1.552518e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_stdscale).apply(np.mean,axis = 0) # As we expected means are almost zero after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.0\n",
       "1     1.0\n",
       "2     1.0\n",
       "3     1.0\n",
       "4     1.0\n",
       "5     1.0\n",
       "6     1.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     1.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_stdscale).apply(np.std,axis = 0) # # As we expected standard deviations are 1 after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.0\n",
       "1     1.0\n",
       "2     1.0\n",
       "3     1.0\n",
       "4     1.0\n",
       "5     1.0\n",
       "6     1.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     1.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min-max scaling (making features to stay in 0-1 boundaries)\n",
    "from sklearn import preprocessing\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[select_features]) # Create the processing object\n",
    "df_minmax = minmax_scale.transform(df[select_features])\n",
    "pd.DataFrame(df_minmax).apply(max,axis =0) # As we expected, max is all 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_minmax).apply(min, axis = 0) # As we expected min is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>cheap.brands</th>\n",
       "      <th>steal</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.032003</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price      shipping  no.brand_name  fancy.categories  \\\n",
       "count  37066.000000  37066.000000   37066.000000      37066.000000   \n",
       "mean      26.581045      0.449630       0.428776          0.003480   \n",
       "std       36.188265      0.497463       0.494908          0.058892   \n",
       "min        0.000000      0.000000       0.000000          0.000000   \n",
       "25%       10.000000      0.000000       0.000000          0.000000   \n",
       "50%       17.000000      0.000000       0.000000          0.000000   \n",
       "75%       29.000000      1.000000       1.000000          0.000000   \n",
       "max     1106.000000      1.000000       1.000000          1.000000   \n",
       "\n",
       "       cheap.categories  fancy.brands  cheap.brands         steal  \\\n",
       "count      37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean           0.005126      0.009820      0.001106      0.001025   \n",
       "std            0.071413      0.098611      0.033241      0.032003   \n",
       "min            0.000000      0.000000      0.000000      0.000000   \n",
       "25%            0.000000      0.000000      0.000000      0.000000   \n",
       "50%            0.000000      0.000000      0.000000      0.000000   \n",
       "75%            0.000000      0.000000      0.000000      0.000000   \n",
       "max            1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000  \n",
       "mean        0.008229      0.008013      0.020774      0.051395  \n",
       "std         0.090339      0.089156      0.142628      0.220805  \n",
       "min         0.000000      0.000000      0.000000      0.000000  \n",
       "25%         0.000000      0.000000      0.000000      0.000000  \n",
       "50%         0.000000      0.000000      0.000000      0.000000  \n",
       "75%         0.000000      0.000000      0.000000      0.000000  \n",
       "max         1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine with other features to obtain final data sets:\n",
    "binary_features = df.drop(select_features, axis = 1)\n",
    "binary_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the standardized and scaled data framesfeatures with the binary features to give two data sets to be tested.\n",
    "binary_features = binary_features.drop(\"price\", axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 11)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_minmax = np.concatenate((binary_features,df_minmax), axis = 1)\n",
    "X_minmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stdscale = np.concatenate((binary_features,df_stdscale), axis = 1)\n",
    "X_stdscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqWOGgCR1zBCQpI4ZApLUsf8F3JGeuzUDOTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121ddacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.price.hist(bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nApLUMENAkhr2v6gh0OtofkY1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cbe67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(df.price+1), bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(df.price+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have standardized and scaled data sets ready and we can re-try training our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 4.2032 - val_loss: 1.2055\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.9293 - val_loss: 0.6881\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.6206 - val_loss: 0.5300\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5228 - val_loss: 0.4773\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4916 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4812 - val_loss: 0.4550\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4764 - val_loss: 0.4514\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4746 - val_loss: 0.4505\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4737 - val_loss: 0.4519\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4731 - val_loss: 0.4505\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4715 - val_loss: 0.4504\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4710 - val_loss: 0.4485\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4703 - val_loss: 0.4511\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4702 - val_loss: 0.4479\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4698 - val_loss: 0.4474\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4687 - val_loss: 0.4477\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4689 - val_loss: 0.4481\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4480\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4673 - val_loss: 0.4472\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4671 - val_loss: 0.4470\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4666 - val_loss: 0.4469\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4659 - val_loss: 0.4457\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4469\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4655 - val_loss: 0.4459\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4654 - val_loss: 0.4482\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4648 - val_loss: 0.4454\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4647 - val_loss: 0.4460\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4643 - val_loss: 0.4455\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4645 - val_loss: 0.4450\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4639 - val_loss: 0.4460\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4639 - val_loss: 0.4455\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4635 - val_loss: 0.4481\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4638 - val_loss: 0.4462\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4631 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4630 - val_loss: 0.4459\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4633 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4628 - val_loss: 0.4457\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4631 - val_loss: 0.4457\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4629 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4626 - val_loss: 0.4456\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4627 - val_loss: 0.4456\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4625 - val_loss: 0.4462\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4625 - val_loss: 0.4459\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4632 - val_loss: 0.4464\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4628 - val_loss: 0.4502\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4622 - val_loss: 0.4500\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4625 - val_loss: 0.4464\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4622 - val_loss: 0.4461\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4621 - val_loss: 0.4469\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4621 - val_loss: 0.4464\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4619 - val_loss: 0.4461\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4468\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4616 - val_loss: 0.4463\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4616 - val_loss: 0.4483\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4612 - val_loss: 0.4514\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4618 - val_loss: 0.4464\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4614 - val_loss: 0.4467\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4614 - val_loss: 0.4473\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4615 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4613 - val_loss: 0.4472\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4617 - val_loss: 0.4466\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4614 - val_loss: 0.4488\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4473\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4489\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4614 - val_loss: 0.4464\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4607 - val_loss: 0.4467\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4609 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4607 - val_loss: 0.4478\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4468\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4615 - val_loss: 0.4469\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4608 - val_loss: 0.4486\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4611 - val_loss: 0.4469\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4469\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4611 - val_loss: 0.4467\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4473\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4608 - val_loss: 0.4471\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4609 - val_loss: 0.4470\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4482\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4472\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4606 - val_loss: 0.4467\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4503\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4465\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4464\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4603 - val_loss: 0.4472\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4472\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4469\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4473\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4461\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4607 - val_loss: 0.4467\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4604 - val_loss: 0.4474\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4472\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4605 - val_loss: 0.4485\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4608 - val_loss: 0.4465\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4606 - val_loss: 0.4483\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4612 - val_loss: 0.4470\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 0s 17us/step - loss: 0.4609 - val_loss: 0.4476\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4470\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4602 - val_loss: 0.4479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66694540986076201"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with X_minmax (scaled features)\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_13 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_13 = np.sqrt(min(model_13.history[\"val_loss\"]))\n",
    "RMSE_model_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was promising! Minmax scaling seems to help for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 3.9986 - val_loss: 1.0113\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.8118 - val_loss: 0.6163\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.5915 - val_loss: 0.5279\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5431 - val_loss: 0.5095\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5262 - val_loss: 0.5001\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5154 - val_loss: 0.4930\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5084 - val_loss: 0.4876\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.5021 - val_loss: 0.4829\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4964 - val_loss: 0.4787\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4914 - val_loss: 0.4772\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4871 - val_loss: 0.4721\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4835 - val_loss: 0.4678\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4798 - val_loss: 0.4691\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4779 - val_loss: 0.4633\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4754 - val_loss: 0.4606\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4736 - val_loss: 0.4599\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4711 - val_loss: 0.4604\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4692 - val_loss: 0.4602\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4685 - val_loss: 0.4587\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4560\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4662 - val_loss: 0.4590\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4653 - val_loss: 0.4562\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4650 - val_loss: 0.4571\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4644 - val_loss: 0.4567\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4645 - val_loss: 0.4572\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4634 - val_loss: 0.4548\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4632 - val_loss: 0.4557\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4627 - val_loss: 0.4551\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4624 - val_loss: 0.4555\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4619 - val_loss: 0.4558\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4613 - val_loss: 0.4551\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4610 - val_loss: 0.4559\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4616 - val_loss: 0.4551\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4607 - val_loss: 0.4532\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4605 - val_loss: 0.4542\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4600 - val_loss: 0.4544\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4558\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4602 - val_loss: 0.4532\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4598 - val_loss: 0.4548\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4592 - val_loss: 0.4532\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4590 - val_loss: 0.4532\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4592 - val_loss: 0.4530\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4592 - val_loss: 0.4529\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4589 - val_loss: 0.4566\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4585 - val_loss: 0.4553\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4583 - val_loss: 0.4556\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4581 - val_loss: 0.4543\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4586 - val_loss: 0.4525\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4583 - val_loss: 0.4547\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4579 - val_loss: 0.4544\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4576 - val_loss: 0.4530\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4572 - val_loss: 0.4539\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4574 - val_loss: 0.4539\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4574 - val_loss: 0.4575\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4580 - val_loss: 0.4535\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4569 - val_loss: 0.4520\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4570 - val_loss: 0.4546\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4573 - val_loss: 0.4530\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4569 - val_loss: 0.4528\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4571 - val_loss: 0.4520\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4572 - val_loss: 0.4544\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4564 - val_loss: 0.4540\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4566 - val_loss: 0.4558\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4569 - val_loss: 0.4532\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4557 - val_loss: 0.4553\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4564 - val_loss: 0.4538\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4558 - val_loss: 0.4561\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4564 - val_loss: 0.4526\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4568 - val_loss: 0.4536\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4557 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4559 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4563 - val_loss: 0.4523\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4558 - val_loss: 0.4521\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4563 - val_loss: 0.4519\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4562 - val_loss: 0.4534\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4539\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4557 - val_loss: 0.4520\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4556 - val_loss: 0.4539\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4554 - val_loss: 0.4533\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4556 - val_loss: 0.4541\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4551 - val_loss: 0.4520\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4556 - val_loss: 0.4545\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4554 - val_loss: 0.4538\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4526\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4556 - val_loss: 0.4548\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4551 - val_loss: 0.4521\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4553 - val_loss: 0.4536\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4549 - val_loss: 0.4529\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4553 - val_loss: 0.4525\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4548 - val_loss: 0.4538\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4556 - val_loss: 0.4539\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4551 - val_loss: 0.4542\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4553 - val_loss: 0.4525\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4547 - val_loss: 0.4525\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4553 - val_loss: 0.4534\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4553 - val_loss: 0.4544\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4550 - val_loss: 0.4524\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4546 - val_loss: 0.4557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6722402842940296"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_stdscale.shape[1], input_dim = X_stdscale.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_14 = model.fit(X_stdscale,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_14 = np.sqrt(min(model_14.history[\"val_loss\"]))\n",
    "RMSE_model_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 3.0712 - val_loss: 0.5992\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.5269 - val_loss: 0.4636\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4811 - val_loss: 0.4525\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4736 - val_loss: 0.4504\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4717 - val_loss: 0.4485\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4704 - val_loss: 0.4486\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4480\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4693 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4683 - val_loss: 0.4526\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4681 - val_loss: 0.4488\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4672 - val_loss: 0.4514\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4664 - val_loss: 0.4469\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4656 - val_loss: 0.4460\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4656 - val_loss: 0.4515\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4660 - val_loss: 0.4515\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4654 - val_loss: 0.4478\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4655 - val_loss: 0.4491\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4518\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4503\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4649 - val_loss: 0.4473\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4643 - val_loss: 0.4505\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4642 - val_loss: 0.4460\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4643 - val_loss: 0.4465\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4642 - val_loss: 0.4467\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4644 - val_loss: 0.4464\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4486\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4643 - val_loss: 0.4468\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4635 - val_loss: 0.4463\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4637 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4640 - val_loss: 0.4501\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4635 - val_loss: 0.4587\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4639 - val_loss: 0.4474\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4630 - val_loss: 0.4457\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4637 - val_loss: 0.4465\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4630 - val_loss: 0.4479\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4634 - val_loss: 0.4456\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4631 - val_loss: 0.4472\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4632 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4631 - val_loss: 0.4458\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4488\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4628 - val_loss: 0.4463\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4632 - val_loss: 0.4470\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4637 - val_loss: 0.4466\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4632 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4623 - val_loss: 0.4476\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4633 - val_loss: 0.4450\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4626 - val_loss: 0.4445\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4627 - val_loss: 0.4465\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4615 - val_loss: 0.4548\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4622 - val_loss: 0.4470\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4621 - val_loss: 0.4449\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4627 - val_loss: 0.4453\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4617 - val_loss: 0.4466\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4614 - val_loss: 0.4495\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4625 - val_loss: 0.4453\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4610 - val_loss: 0.4453\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4613 - val_loss: 0.4459\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4610 - val_loss: 0.4444\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4609 - val_loss: 0.4469\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4448\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4604 - val_loss: 0.4505\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4609 - val_loss: 0.4450\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4603 - val_loss: 0.4489\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4604 - val_loss: 0.4450\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4598 - val_loss: 0.4448\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4603 - val_loss: 0.4453\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4451\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4600 - val_loss: 0.4446\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4611 - val_loss: 0.4446\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4598 - val_loss: 0.4451\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4511\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4601 - val_loss: 0.4446\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4596 - val_loss: 0.4450\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4599 - val_loss: 0.4448\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4595 - val_loss: 0.4454\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4597 - val_loss: 0.4443\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4591 - val_loss: 0.4455\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4596 - val_loss: 0.4454\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4587 - val_loss: 0.4446\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4591 - val_loss: 0.4494\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4590 - val_loss: 0.4451\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4591 - val_loss: 0.4445\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4589 - val_loss: 0.4440\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4595 - val_loss: 0.4471\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4589 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4590 - val_loss: 0.4471\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4586 - val_loss: 0.4442\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4590 - val_loss: 0.4447\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4587 - val_loss: 0.4461\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4592 - val_loss: 0.4447\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4588 - val_loss: 0.4450\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4585 - val_loss: 0.4487\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4590 - val_loss: 0.4550\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4594 - val_loss: 0.4445\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4588 - val_loss: 0.4443\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4587 - val_loss: 0.4446\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4579 - val_loss: 0.4449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66633644156462402"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue with X_minmax (scaled features), slighly increasing model complexity\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_15 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_15 = np.sqrt(min(model_15.history[\"val_loss\"]))\n",
    "RMSE_model_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's getting better! Continue increasing model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 2.3449 - val_loss: 0.4819\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4852 - val_loss: 0.4507\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4734 - val_loss: 0.4493\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4706 - val_loss: 0.4488\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4693 - val_loss: 0.4465\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4685 - val_loss: 0.4479\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4679 - val_loss: 0.4522\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4696 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4684 - val_loss: 0.4489\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4678 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4673 - val_loss: 0.4473\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4664 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4654 - val_loss: 0.4448\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4647 - val_loss: 0.4547\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4659 - val_loss: 0.4490\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4655 - val_loss: 0.4485\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4649 - val_loss: 0.4549\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4640 - val_loss: 0.4557\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4463\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4635 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4458\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4636 - val_loss: 0.4456\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4629 - val_loss: 0.4445\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4636 - val_loss: 0.4460\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4624 - val_loss: 0.4521\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4638 - val_loss: 0.4456\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4629 - val_loss: 0.4463\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4620 - val_loss: 0.4445\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4628 - val_loss: 0.4490\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4631 - val_loss: 0.4461\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4616 - val_loss: 0.4549\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4624 - val_loss: 0.4479\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4614 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4624 - val_loss: 0.4458\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4606 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4613 - val_loss: 0.4456\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4608 - val_loss: 0.4482\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4615 - val_loss: 0.4507\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4453\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4630 - val_loss: 0.4455\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4614 - val_loss: 0.4457\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4615 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4614 - val_loss: 0.4461\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4604 - val_loss: 0.4473\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4616 - val_loss: 0.4464\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4610 - val_loss: 0.4457\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4613 - val_loss: 0.4498\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4598 - val_loss: 0.4597\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4600 - val_loss: 0.4458\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4610 - val_loss: 0.4456\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4619 - val_loss: 0.4475\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4609 - val_loss: 0.4489\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4601 - val_loss: 0.4542\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4616 - val_loss: 0.4455\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4597 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4598 - val_loss: 0.4462\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4594 - val_loss: 0.4444\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4595 - val_loss: 0.4457\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4606 - val_loss: 0.4458\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4591 - val_loss: 0.4462\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4598 - val_loss: 0.4465\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4592 - val_loss: 0.4494\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4588 - val_loss: 0.4452\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4585 - val_loss: 0.4455\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4592 - val_loss: 0.4472\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4586 - val_loss: 0.4453\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4592 - val_loss: 0.4474\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4601 - val_loss: 0.4449\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4588 - val_loss: 0.4467\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4592 - val_loss: 0.4530\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4587 - val_loss: 0.4454\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4592 - val_loss: 0.4453\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4589 - val_loss: 0.4449\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4585 - val_loss: 0.4474\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4590 - val_loss: 0.4467\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4581 - val_loss: 0.4469\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4585 - val_loss: 0.4459\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4572 - val_loss: 0.4468\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4577 - val_loss: 0.4467\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4567 - val_loss: 0.4480\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4571 - val_loss: 0.4497\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4567 - val_loss: 0.4457\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4570 - val_loss: 0.4463\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4564 - val_loss: 0.4444\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4569 - val_loss: 0.4495\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4559 - val_loss: 0.4525\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4561 - val_loss: 0.4457\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4554 - val_loss: 0.4455\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4565 - val_loss: 0.4453\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4556 - val_loss: 0.4455\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4560 - val_loss: 0.4454\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4559 - val_loss: 0.4454\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4555 - val_loss: 0.4497\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4549 - val_loss: 0.4664\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4563 - val_loss: 0.4449\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4553 - val_loss: 0.4458\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4549 - val_loss: 0.4452\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4540 - val_loss: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6665976297688434"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1]*2, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1]*2, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_16 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_16 = np.sqrt(min(model_16.history[\"val_loss\"]))\n",
    "RMSE_model_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we hit the model capacity again. Let's try another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 2.5496 - val_loss: 0.4932\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4890 - val_loss: 0.4526\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4753 - val_loss: 0.4499\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4718 - val_loss: 0.4510\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4706 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4698 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4574\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4723 - val_loss: 0.4477\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4706 - val_loss: 0.4511\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4702 - val_loss: 0.4524\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4692 - val_loss: 0.4484\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4696 - val_loss: 0.4471\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4682 - val_loss: 0.4503\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4677 - val_loss: 0.4478\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4688 - val_loss: 0.4489\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4686 - val_loss: 0.4557\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4679 - val_loss: 0.4640\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4671 - val_loss: 0.4495\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4538\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4677 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4666 - val_loss: 0.4483\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4673 - val_loss: 0.4470\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4667 - val_loss: 0.4466\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4656 - val_loss: 0.4465\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4672 - val_loss: 0.4466\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4650 - val_loss: 0.4539\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4668 - val_loss: 0.4496\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4661 - val_loss: 0.4482\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4647 - val_loss: 0.4464\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4657 - val_loss: 0.4503\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4660 - val_loss: 0.4488\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4652 - val_loss: 0.4483\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4652 - val_loss: 0.4509\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4460\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4653 - val_loss: 0.4464\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4635 - val_loss: 0.4461\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4637 - val_loss: 0.4462\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4637 - val_loss: 0.4491\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4643 - val_loss: 0.4552\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4642 - val_loss: 0.4467\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4662 - val_loss: 0.4467\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4639 - val_loss: 0.4473\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4474\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4640 - val_loss: 0.4478\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4644 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4635 - val_loss: 0.4482\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4643 - val_loss: 0.4472\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4648 - val_loss: 0.4460\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4638 - val_loss: 0.4511\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4546\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4630 - val_loss: 0.4467\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4636 - val_loss: 0.4466\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4653 - val_loss: 0.4507\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4646 - val_loss: 0.4500\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4634 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4639 - val_loss: 0.4465\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4633 - val_loss: 0.4476\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4628 - val_loss: 0.4471\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4625 - val_loss: 0.4479\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4634 - val_loss: 0.4464\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4468\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4627 - val_loss: 0.4468\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4629 - val_loss: 0.4481\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4626 - val_loss: 0.4544\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4625 - val_loss: 0.4461\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4622 - val_loss: 0.4469\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4633 - val_loss: 0.4490\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4624 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4636 - val_loss: 0.4508\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4635 - val_loss: 0.4466\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4625 - val_loss: 0.4482\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4635 - val_loss: 0.4542\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4632 - val_loss: 0.4460\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4633 - val_loss: 0.4470\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4628 - val_loss: 0.4462\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4630 - val_loss: 0.4466\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4629 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4628 - val_loss: 0.4500\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4632 - val_loss: 0.4474\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4622 - val_loss: 0.4476\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4626 - val_loss: 0.4468\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4618 - val_loss: 0.4477\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4624 - val_loss: 0.4489\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4614 - val_loss: 0.4462\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4624 - val_loss: 0.4502\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4618 - val_loss: 0.4454\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4612 - val_loss: 0.4466\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4603 - val_loss: 0.4557\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4612 - val_loss: 0.4480\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4602 - val_loss: 0.4457\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4615 - val_loss: 0.4459\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4605 - val_loss: 0.4463\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4599 - val_loss: 0.4459\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4611 - val_loss: 0.4458\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4598 - val_loss: 0.4475\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4591 - val_loss: 0.4721\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4605 - val_loss: 0.4468\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4596 - val_loss: 0.4469\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4588 - val_loss: 0.4464\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4581 - val_loss: 0.4462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66741381955167611"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1], input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_17 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_17 = np.sqrt(min(model_17.history[\"val_loss\"]))\n",
    "RMSE_model_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 1.3357 - val_loss: 0.4499\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4709 - val_loss: 0.4489\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4696 - val_loss: 0.4492\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4716 - val_loss: 0.4545\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4681 - val_loss: 0.4481\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4665 - val_loss: 0.4571\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4684 - val_loss: 0.4472\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4702 - val_loss: 0.4586\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4663 - val_loss: 0.4493\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4666 - val_loss: 0.4503\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4670 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4648 - val_loss: 0.4453\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4637 - val_loss: 0.4496\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4643 - val_loss: 0.4520\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4639 - val_loss: 0.4495\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4644 - val_loss: 0.4689\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4627 - val_loss: 0.4515\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4631 - val_loss: 0.4470\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4641 - val_loss: 0.4456\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4627 - val_loss: 0.4581\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4609 - val_loss: 0.4554\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4633 - val_loss: 0.4516\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4595 - val_loss: 0.4474\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4639 - val_loss: 0.4477\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4596 - val_loss: 0.4458\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4607 - val_loss: 0.4491\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4604 - val_loss: 0.4477\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4594 - val_loss: 0.4449\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4598 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4605 - val_loss: 0.4538\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4592 - val_loss: 0.4472\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4591 - val_loss: 0.4486\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4588 - val_loss: 0.4531\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4610 - val_loss: 0.4452\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4571 - val_loss: 0.4456\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4585 - val_loss: 0.4447\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4573 - val_loss: 0.4509\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4581 - val_loss: 0.4503\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4581 - val_loss: 0.4455\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4574 - val_loss: 0.4470\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4560 - val_loss: 0.4481\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4570 - val_loss: 0.4469\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4566 - val_loss: 0.4486\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4574 - val_loss: 0.4468\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4560 - val_loss: 0.4487\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4557 - val_loss: 0.4502\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4572 - val_loss: 0.4465\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4552 - val_loss: 0.4555\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4550 - val_loss: 0.4492\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4545 - val_loss: 0.4504\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4535 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.456 - 1s 42us/step - loss: 0.4571 - val_loss: 0.4500\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4543 - val_loss: 0.4468\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4541 - val_loss: 0.4505\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4538 - val_loss: 0.4551\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4534 - val_loss: 0.4506\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4515 - val_loss: 0.4471\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4510 - val_loss: 0.4492\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4509 - val_loss: 0.4481\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4524 - val_loss: 0.4465\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4498 - val_loss: 0.4471\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4493 - val_loss: 0.4469\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4489 - val_loss: 0.4577\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4481 - val_loss: 0.4477\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 37us/step - loss: 0.4469 - val_loss: 0.4476\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4488 - val_loss: 0.4462\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4462 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4479 - val_loss: 0.4486\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4462 - val_loss: 0.4504\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4457 - val_loss: 0.4480\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4463 - val_loss: 0.4536\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4450 - val_loss: 0.4505\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4459 - val_loss: 0.4482\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4449 - val_loss: 0.4494\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4438 - val_loss: 0.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4440 - val_loss: 0.4499\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4429 - val_loss: 0.4549\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4441 - val_loss: 0.4535\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4420 - val_loss: 0.4642\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4423 - val_loss: 0.4532\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4420 - val_loss: 0.4545\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4416 - val_loss: 0.4517\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 35us/step - loss: 0.4412 - val_loss: 0.4532\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4408 - val_loss: 0.4520\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4411 - val_loss: 0.4519\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4401 - val_loss: 0.4539\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4391 - val_loss: 0.4602\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4393 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4380 - val_loss: 0.4531\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 36us/step - loss: 0.4392 - val_loss: 0.4546\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 41us/step - loss: 0.4398 - val_loss: 0.4539\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 38us/step - loss: 0.4384 - val_loss: 0.4544\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4391 - val_loss: 0.4694\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4378 - val_loss: 0.4526\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4368 - val_loss: 0.4836\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 39us/step - loss: 0.4382 - val_loss: 0.4552\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4371 - val_loss: 0.4560\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4356 - val_loss: 0.4530\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 40us/step - loss: 0.4351 - val_loss: 0.4589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66687018587000157"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(100, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(100, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_18 = model.fit(X_minmax,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_18 = np.sqrt(min(model_18.history[\"val_loss\"]))\n",
    "RMSE_model_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/10\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 1.0771 - val_loss: 0.4512\n",
      "Epoch 2/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4770 - val_loss: 0.4632\n",
      "Epoch 3/10\n",
      "25946/25946 [==============================] - 2s 69us/step - loss: 0.4751 - val_loss: 0.4510\n",
      "Epoch 4/10\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4734 - val_loss: 0.4515\n",
      "Epoch 5/10\n",
      "25946/25946 [==============================] - 2s 76us/step - loss: 0.4726 - val_loss: 0.4470\n",
      "Epoch 6/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4711 - val_loss: 0.4505\n",
      "Epoch 7/10\n",
      "25946/25946 [==============================] - 2s 69us/step - loss: 0.4704 - val_loss: 0.4613\n",
      "Epoch 8/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4699 - val_loss: 0.4468\n",
      "Epoch 9/10\n",
      "25946/25946 [==============================] - 2s 61us/step - loss: 0.4692 - val_loss: 0.4544\n",
      "Epoch 10/10\n",
      "25946/25946 [==============================] - 2s 63us/step - loss: 0.4676 - val_loss: 0.4505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66841383692321843"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the batch and only performing 10 epochs\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_minmax.shape[1]*2, input_dim = X_minmax.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(X_minmax.shape[1]*2, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_19 = model.fit(X_minmax,y,epochs = 10,validation_split= 0.3)\n",
    "RMSE_model_19 = np.sqrt(min(model_19.history[\"val_loss\"]))\n",
    "RMSE_model_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take home message\n",
    "\n",
    "For this data set, minmax scaling of the data helped only a little bit for training a better model. We will save this version of the predictors to continue model tuning as we learn different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_minmax).to_csv(\"mini_subtrain_X_minmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).to_csv(\"mini_subtrain_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
