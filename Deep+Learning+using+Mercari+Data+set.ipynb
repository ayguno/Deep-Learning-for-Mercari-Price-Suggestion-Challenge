{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "In this exercise I will use the features I previously engineered using R and Kaggle Mercari Price challenge data set. \n",
    "\n",
    "We will start with loading the libraries and functions we will need during the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import SGD\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_condition_id  price  shipping  no.brand_name  log.excl.description  \\\n",
       "1                  1    8.0         0              1                   0.0   \n",
       "2                  2   39.0         1              1                   0.0   \n",
       "3                  1   30.0         1              0                   0.0   \n",
       "4                  2  470.0         1              0                   0.0   \n",
       "5                  2   22.0         0              0                   0.0   \n",
       "\n",
       "   excl.name  dollar.description  fancy.categories  cheap.categories  \\\n",
       "1          0                   0                 0                 0   \n",
       "2          0                   0                 0                 0   \n",
       "3          0                   0                 0                 0   \n",
       "4          0                   0                 0                 0   \n",
       "5          0                   0                 0                 0   \n",
       "\n",
       "   fancy.brands        ...         now  cheap  buy  excellent  great  \\\n",
       "1             0        ...           0      0    0          0      0   \n",
       "2             0        ...           0      0    0          0      0   \n",
       "3             0        ...           0      0    0          0      0   \n",
       "4             1        ...           0      0    0          0      0   \n",
       "5             0        ...           0      0    0          0      0   \n",
       "\n",
       "   michael.brand  jordan.name  iphon.name  bundl.name  cap.letter.brand  \n",
       "1              0            0           0           0                 1  \n",
       "2              0            0           0           0                 1  \n",
       "3              0            0           0           1                 0  \n",
       "4              0            0           0           0                 6  \n",
       "5              0            0           0           0                 2  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load the mini subtraining set we prepared previously\n",
    "mini_subtrain = pd.read_csv(\"mini_subtrain.csv\", index_col = 0)\n",
    "mini_subtrain.shape\n",
    "mini_subtrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_subtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "mini_subtrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sucessfully reading the verifying the training data set we have previously constructed using R, we can start building a small neural network and training it by using our data.\n",
    "\n",
    "First we start with seperating predictors and response arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = np.array(mini_subtrain.drop([\"price\"], axis=1))\n",
    "# We will log transform the target variable as we have performed in R\n",
    "target = np.array(np.log(mini_subtrain.price + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can start building our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 98us/step - loss: 1.3796 - val_loss: 0.5341\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4969 - val_loss: 0.4515\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 1s 56us/step - loss: 0.4742 - val_loss: 0.4472\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4719 - val_loss: 0.4500\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4720 - val_loss: 0.4457\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 65us/step - loss: 0.4704 - val_loss: 0.4510\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 64us/step - loss: 0.4698 - val_loss: 0.4503\n"
     ]
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_1 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our loss function is \"mean squared error\". We will take the square root of this to follow \"root mean squared error\" (RMSE). We also keep in mind that the target is log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66764026261785125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_1 = np.sqrt(min(model_1.history[\"val_loss\"]))\n",
    "RMSE_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this RMSE is close to what we have obtained other machine learning algorithms previously. Therefore, we will continue our experiment by increasing model complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 25s 953us/step - loss: 0.6853 - val_loss: 0.4837\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 25s 949us/step - loss: 0.4860 - val_loss: 0.4547\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 25s 968us/step - loss: 0.4801 - val_loss: 0.4599\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 29s 1ms/step - loss: 0.4754 - val_loss: 0.4593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67429454720879667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1000,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_2 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_2 = np.sqrt(min(model_2.history[\"val_loss\"]))\n",
    "RMSE_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our first model is already at its capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/30\n",
      "25946/25946 [==============================] - 3s 111us/step - loss: 1.0952 - val_loss: 0.4894\n",
      "Epoch 2/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4874 - val_loss: 0.4550\n",
      "Epoch 3/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4755 - val_loss: 0.4524\n",
      "Epoch 4/30\n",
      "25946/25946 [==============================] - 2s 72us/step - loss: 0.4727 - val_loss: 0.4561\n",
      "Epoch 5/30\n",
      "25946/25946 [==============================] - 2s 66us/step - loss: 0.4711 - val_loss: 0.4517\n",
      "Epoch 6/30\n",
      "25946/25946 [==============================] - 2s 71us/step - loss: 0.4686 - val_loss: 0.4507\n",
      "Epoch 7/30\n",
      "25946/25946 [==============================] - 2s 68us/step - loss: 0.4679 - val_loss: 0.4551\n",
      "Epoch 8/30\n",
      "25946/25946 [==============================] - 2s 84us/step - loss: 0.4683 - val_loss: 0.4571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67136944170640311"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (predictors.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_3 = model.fit(predictors,target, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_3 = np.sqrt(min(model_3.history[\"val_loss\"]))\n",
    "RMSE_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we reached to model capacity even with a relatively simple network. Next, we will try if we can reduce the bias by training a larger data set, which we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Deep+Learning+using+Mercari+Data+set.ipynb',\n",
       " 'mini_subtrain.csv',\n",
       " 'subtrain.csv',\n",
       " 'validation.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrain = pd.read_csv(\"subtrain.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(741269, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741269 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741269 non-null int64\n",
      "price                   741269 non-null float64\n",
      "shipping                741269 non-null int64\n",
      "no.brand_name           741269 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741269 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741269 non-null int64\n",
      "cheap.categories        741269 non-null int64\n",
      "fancy.brands            741269 non-null int64\n",
      "cheap.brands            741269 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741269 non-null int64\n",
      "jordan.name             741269 non-null int64\n",
      "iphon.name              741269 non-null int64\n",
      "bundl.name              741269 non-null int64\n",
      "cap.letter.brand        741269 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "#We need to check for missing values in the data set\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 741268 entries, 1 to 741269\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       741268 non-null int64\n",
      "price                   741268 non-null float64\n",
      "shipping                741268 non-null int64\n",
      "no.brand_name           741268 non-null int64\n",
      "log.excl.description    741268 non-null float64\n",
      "excl.name               741268 non-null int64\n",
      "dollar.description      741268 non-null float64\n",
      "fancy.categories        741268 non-null int64\n",
      "cheap.categories        741268 non-null int64\n",
      "fancy.brands            741268 non-null int64\n",
      "cheap.brands            741268 non-null int64\n",
      "sale                    741268 non-null float64\n",
      "free                    741268 non-null float64\n",
      "save                    741268 non-null float64\n",
      "deal                    741268 non-null float64\n",
      "good                    741268 non-null float64\n",
      "steal                   741268 non-null float64\n",
      "now                     741268 non-null float64\n",
      "cheap                   741268 non-null float64\n",
      "buy                     741268 non-null float64\n",
      "excellent               741268 non-null float64\n",
      "great                   741268 non-null float64\n",
      "michael.brand           741268 non-null int64\n",
      "jordan.name             741268 non-null int64\n",
      "iphon.name              741268 non-null int64\n",
      "bundl.name              741268 non-null int64\n",
      "cap.letter.brand        741268 non-null int64\n",
      "dtypes: float64(14), int64(13)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "subtrain = subtrain.dropna()\n",
    "subtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(subtrain.price + 1)\n",
    "X = subtrain.drop([\"price\"],axis = 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.5185 - val_loss: 0.4680\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4695 - val_loss: 0.4894\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4674 - val_loss: 0.4629\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4664 - val_loss: 0.4629\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4658 - val_loss: 0.4662\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4654 - val_loss: 0.4617\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4650 - val_loss: 0.4623\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4649 - val_loss: 0.4617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6794526310377843"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(10,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_4 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_4 = np.sqrt(min(model_4.history[\"val_loss\"]))\n",
    "RMSE_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.5029 - val_loss: 0.4714\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4684 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4669 - val_loss: 0.4626\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4658 - val_loss: 0.4621\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4655 - val_loss: 0.4626\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4646 - val_loss: 0.4618\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4644 - val_loss: 0.4605\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4641 - val_loss: 0.4640\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4639 - val_loss: 0.4614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67862169258002358"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_5 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_5 = np.sqrt(min(model_5.history[\"val_loss\"]))\n",
    "RMSE_model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4934 - val_loss: 0.4676\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4687 - val_loss: 0.4653\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 38s 72us/step - loss: 0.4662 - val_loss: 0.4616\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 37s 70us/step - loss: 0.4653 - val_loss: 0.4616\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4644 - val_loss: 0.4615\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4638 - val_loss: 0.4613\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4634 - val_loss: 0.4608\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 37s 72us/step - loss: 0.4631 - val_loss: 0.4618\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 37s 71us/step - loss: 0.4629 - val_loss: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67883387278347673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slightly changing the model complexity\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_6 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_6 = np.sqrt(min(model_6.history[\"val_loss\"]))\n",
    "RMSE_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4880 - val_loss: 0.4679\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4714 - val_loss: 0.4882\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4688 - val_loss: 0.4665\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4675 - val_loss: 0.4698\n",
      "Epoch 5/30\n",
      "518887/518887 [==============================] - 33s 64us/step - loss: 0.4666 - val_loss: 0.4640\n",
      "Epoch 6/30\n",
      "518887/518887 [==============================] - 33s 63us/step - loss: 0.4676 - val_loss: 0.4640\n",
      "Epoch 7/30\n",
      "518887/518887 [==============================] - 33s 65us/step - loss: 0.4661 - val_loss: 0.4626\n",
      "Epoch 8/30\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4653 - val_loss: 0.4641\n",
      "Epoch 9/30\n",
      "518887/518887 [==============================] - 30s 58us/step - loss: 0.4647 - val_loss: 0.4636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68016471861743943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing optimizer to SGD\n",
    "sgd_optimizer = SGD(lr = 0.01)\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(50,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= sgd_optimizer, loss= \"mean_squared_error\")\n",
    "model_7 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_7 = np.sqrt(min(model_7.history[\"val_loss\"]))\n",
    "RMSE_model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/20\n",
      "518887/518887 [==============================] - 38s 73us/step - loss: 0.4953 - val_loss: 0.4869\n",
      "Epoch 2/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4685 - val_loss: 0.4672\n",
      "Epoch 3/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4669 - val_loss: 0.4635\n",
      "Epoch 4/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4659 - val_loss: 0.4623\n",
      "Epoch 5/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4668\n",
      "Epoch 6/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4652 - val_loss: 0.4623\n",
      "Epoch 7/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4645 - val_loss: 0.4626\n",
      "Epoch 8/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4645 - val_loss: 0.4622\n",
      "Epoch 9/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4639 - val_loss: 0.4612\n",
      "Epoch 10/20\n",
      "518887/518887 [==============================] - 34s 66us/step - loss: 0.4658 - val_loss: 0.4638\n",
      "Epoch 11/20\n",
      "518887/518887 [==============================] - 34s 65us/step - loss: 0.4636 - val_loss: 0.4611\n",
      "Epoch 12/20\n",
      "518887/518887 [==============================] - 40s 78us/step - loss: 0.4634 - val_loss: 0.4606\n",
      "Epoch 13/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4632 - val_loss: 0.4617\n",
      "Epoch 14/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4632 - val_loss: 0.4612\n",
      "Epoch 15/20\n",
      "518887/518887 [==============================] - 36s 70us/step - loss: 0.4631 - val_loss: 0.4602\n",
      "Epoch 16/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4629 - val_loss: 0.4640\n",
      "Epoch 17/20\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4628 - val_loss: 0.4610\n",
      "Epoch 18/20\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4629 - val_loss: 0.4642\n",
      "Epoch 19/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4625 - val_loss: 0.4607\n",
      "Epoch 20/20\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.4624 - val_loss: 0.4647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67839322279547709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing fixed 20 epochs\n",
    "#estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(25,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(25,activation= \"relu\"))\n",
    "model.add(Dense(1,activation = \"relu\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_8 = model.fit(X,y, epochs= 20, validation_split= 0.3)\n",
    "RMSE_model_8 = np.sqrt(min(model_8.history[\"val_loss\"]))\n",
    "RMSE_model_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 518887 samples, validate on 222381 samples\n",
      "Epoch 1/30\n",
      "518887/518887 [==============================] - 35s 68us/step - loss: 0.5663 - val_loss: 0.4738\n",
      "Epoch 2/30\n",
      "518887/518887 [==============================] - 35s 67us/step - loss: 0.4740 - val_loss: 0.4702\n",
      "Epoch 3/30\n",
      "518887/518887 [==============================] - 36s 69us/step - loss: 0.4735 - val_loss: 0.4721\n",
      "Epoch 4/30\n",
      "518887/518887 [==============================] - 32s 62us/step - loss: 0.4726 - val_loss: 0.4706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68569114759373795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing final activation function\n",
    "estop_monitor = EarlyStopping(patience= 2)\n",
    "pre_shape = (X.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation= \"relu\", input_shape = pre_shape))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_9 = model.fit(X,y, epochs= 30, callbacks= [estop_monitor], validation_split= 0.3)\n",
    "RMSE_model_9 = np.sqrt(min(model_9.history[\"val_loss\"]))\n",
    "RMSE_model_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to learn other means of model optimization and try to test here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the small data set for training once again:\n",
    "os.listdir()\n",
    "df = pd.read_csv(\"mini_subtrain.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37066 entries, 1 to 37066\n",
      "Data columns (total 27 columns):\n",
      "item_condition_id       37066 non-null int64\n",
      "price                   37066 non-null float64\n",
      "shipping                37066 non-null int64\n",
      "no.brand_name           37066 non-null int64\n",
      "log.excl.description    37066 non-null float64\n",
      "excl.name               37066 non-null int64\n",
      "dollar.description      37066 non-null int64\n",
      "fancy.categories        37066 non-null int64\n",
      "cheap.categories        37066 non-null int64\n",
      "fancy.brands            37066 non-null int64\n",
      "cheap.brands            37066 non-null int64\n",
      "sale                    37066 non-null int64\n",
      "free                    37066 non-null int64\n",
      "save                    37066 non-null int64\n",
      "deal                    37066 non-null int64\n",
      "good                    37066 non-null int64\n",
      "steal                   37066 non-null int64\n",
      "now                     37066 non-null int64\n",
      "cheap                   37066 non-null int64\n",
      "buy                     37066 non-null int64\n",
      "excellent               37066 non-null int64\n",
      "great                   37066 non-null int64\n",
      "michael.brand           37066 non-null int64\n",
      "jordan.name             37066 non-null int64\n",
      "iphon.name              37066 non-null int64\n",
      "bundl.name              37066 non-null int64\n",
      "cap.letter.brand        37066 non-null int64\n",
      "dtypes: float64(2), int64(25)\n",
      "memory usage: 7.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis= 1).values\n",
    "y = np.log(df.price+1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066, 26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37066,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # This is a Keras wrapper for sklearn we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt I loaded some sklearn functions to use them in model evaluation, as well as a Keras wrapper for sklearn.\n",
    "\n",
    "In this case, the Keras wrappers we will use take a function as argument, which we will define to create the neural network model structure.\n",
    "\n",
    "Let's define the basic_model function:\n",
    "\n",
    "- We use a sequential model structure.\n",
    "- We will have a simple 2-layered model (input and output layer).\n",
    "- We don't use any activation function at the output layer (since we are performing regression, we want values to be evaluated without transformation). \n",
    "- We will use ADAM optimization function and optimize mean squared error as our loss function\n",
    "- We will use the same number of neurons(nodes) as the the number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    \"\"\"Creates, compiles and returns the basic NN\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "    model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define our regression estimator using keras wrapper KerasRegressor we imported above. This estimator function will receive:\n",
    "\n",
    "- the function which creates the NN model\n",
    "- parameters we normally enter in .fit() function (e.g: number of epochs or batch size), note that in this case we are doing the computation in batch mode, so we use nb_epoch argument\n",
    "\n",
    "we will also need to set the random number generator seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4\n",
    "np.random.seed(seed)\n",
    "regression_estimator = KerasRegressor(build_fn= basic_model, \n",
    "                                     nb_epoch = 100,\n",
    "                                     batch_size = 5,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the model using 10-fold cross-validation. Note that this is the actual step in which we are fitting the model on the data. We are collecting the model evaluation in a object we called **results**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c75cb95c7d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set of the k-folds and random state to partition the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregression_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits= 10, random_state= seed) # Set of the k-folds and random state to partition the data\n",
    "results = cross_val_score(estimator=regression_estimator, X = X,y = y,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results) # Note that results is an array that contains MSE scores for each k-fold cv step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4844004 , -0.46776465, -0.49775278, -0.4527221 , -0.46104527,\n",
       "       -0.48631333, -0.46733199, -0.45774577, -0.47870872, -0.46569664])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47194816461575667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mse returned by CV function is negative! This is quite counter intuitive and confusing. Checking the documentation in GitHub, I understand that this is not a useful way of estimating model performance, perhaps the wrapper has problems.\n",
    "\n",
    "I am going back to the original way I fit the models, that is using a single split for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 2.9459 - val_loss: 1.0557\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.9681 - val_loss: 0.8441\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.7625 - val_loss: 0.6377\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.5757 - val_loss: 0.4834\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4858 - val_loss: 0.4520\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4749 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4741 - val_loss: 0.4503\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4745 - val_loss: 0.4501\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4741 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4746 - val_loss: 0.4512\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4745 - val_loss: 0.4561\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4741 - val_loss: 0.4511\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4739 - val_loss: 0.4508\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4743 - val_loss: 0.4525\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4743 - val_loss: 0.4533\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4728 - val_loss: 0.4498\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4720 - val_loss: 0.4519\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4708 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4702 - val_loss: 0.4506\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4692 - val_loss: 0.4498\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4468\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4690 - val_loss: 0.4468\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4689 - val_loss: 0.4471\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4681 - val_loss: 0.4479\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4686 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4680 - val_loss: 0.4471\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4682 - val_loss: 0.4494\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4681 - val_loss: 0.4471\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4680 - val_loss: 0.4492\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4675 - val_loss: 0.4616\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4678 - val_loss: 0.4472\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4675 - val_loss: 0.4474\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4670 - val_loss: 0.4472\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4675 - val_loss: 0.4467\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4671 - val_loss: 0.4468\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 19us/step - loss: 0.4669 - val_loss: 0.4470\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4667 - val_loss: 0.4460\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4673 - val_loss: 0.4503\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4665 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4671 - val_loss: 0.4466\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 0s 18us/step - loss: 0.4674 - val_loss: 0.4469\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 0s 19us/step - loss: 0.4666 - val_loss: 0.4492\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4669 - val_loss: 0.4481\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4662 - val_loss: 0.4459\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4670 - val_loss: 0.4470\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 34us/step - loss: 0.4652 - val_loss: 0.4564\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 31us/step - loss: 0.4665 - val_loss: 0.4498\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4660 - val_loss: 0.4462\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4663 - val_loss: 0.4481\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4659 - val_loss: 0.4465\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4658 - val_loss: 0.4485\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4661 - val_loss: 0.4469\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4652 - val_loss: 0.4462\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4655 - val_loss: 0.4479\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4650 - val_loss: 0.4469\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4651 - val_loss: 0.4470\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4658 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4644 - val_loss: 0.4502\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4650 - val_loss: 0.4466\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4497\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4647 - val_loss: 0.4468\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4489\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4487\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4466\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4653 - val_loss: 0.4474\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4641 - val_loss: 0.4472\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4645 - val_loss: 0.4537\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4650 - val_loss: 0.4473\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4649 - val_loss: 0.4471\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4646 - val_loss: 0.4482\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4646 - val_loss: 0.4480\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4645 - val_loss: 0.4475\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4645 - val_loss: 0.4476\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4645 - val_loss: 0.4470\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4641 - val_loss: 0.4483\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4644 - val_loss: 0.4484\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4638 - val_loss: 0.4478\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4521\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4476\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4644 - val_loss: 0.4467\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4637 - val_loss: 0.4480\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 20us/step - loss: 0.4646 - val_loss: 0.4481\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4639 - val_loss: 0.4499\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4642 - val_loss: 0.4497\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4640 - val_loss: 0.4472\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4644 - val_loss: 0.4471\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4638 - val_loss: 0.4498\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4641 - val_loss: 0.4475\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 21us/step - loss: 0.4641 - val_loss: 0.4479\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4638 - val_loss: 0.4484\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4640 - val_loss: 0.4570\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4645 - val_loss: 0.4483\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4641 - val_loss: 0.4486\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4641 - val_loss: 0.4474\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4632 - val_loss: 0.4484\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_10 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66776206455463483"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_10 = np.sqrt(min(model_10.history[\"val_loss\"]))\n",
    "RMSE_model_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight improvement over model_1. Let's continue by trying to tune model topology. \n",
    "\n",
    "We can try a more complex network, hoping that this network will extract and recombine more interactions between the available features.\n",
    "\n",
    "- We can set up deeper NNs : i.e: adding layers\n",
    "- We can set up wider NNs: i.e: adding neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 3.2844 - val_loss: 0.9369\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.8241 - val_loss: 0.6616\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.5722 - val_loss: 0.4716\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4792 - val_loss: 0.4520\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4741 - val_loss: 0.4498\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4733 - val_loss: 0.4498\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4727 - val_loss: 0.4500\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4733 - val_loss: 0.4486\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4723 - val_loss: 0.4529\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4722 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4724 - val_loss: 0.4530\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4716 - val_loss: 0.4483\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4709 - val_loss: 0.4479\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4558\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4717 - val_loss: 0.4516\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4716 - val_loss: 0.4486\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4713 - val_loss: 0.4526\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4709 - val_loss: 0.4531\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4710 - val_loss: 0.4530\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4712 - val_loss: 0.4480\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4707 - val_loss: 0.4535\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4474\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4476\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4706 - val_loss: 0.4472\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4484\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4708 - val_loss: 0.4478\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4479\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4703 - val_loss: 0.4485\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4700 - val_loss: 0.4495\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4694 - val_loss: 0.4613\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4701 - val_loss: 0.4483\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4690 - val_loss: 0.4471\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4700 - val_loss: 0.4489\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4688 - val_loss: 0.4491\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4697 - val_loss: 0.4477\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4693 - val_loss: 0.4480\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4693 - val_loss: 0.4504\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4694 - val_loss: 0.4470\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4707 - val_loss: 0.4512\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4691 - val_loss: 0.4479\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4477\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4483\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4695 - val_loss: 0.4487\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4488\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4691 - val_loss: 0.4471\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4482\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4679 - val_loss: 0.4591\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4488\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4689 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4699 - val_loss: 0.4492\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4688 - val_loss: 0.4475\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4686 - val_loss: 0.4503\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4691 - val_loss: 0.4480\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4676 - val_loss: 0.4465\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4677 - val_loss: 0.4486\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4669 - val_loss: 0.4461\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4667 - val_loss: 0.4473\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 33us/step - loss: 0.4675 - val_loss: 0.4463\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 32us/step - loss: 0.4660 - val_loss: 0.4477\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4663 - val_loss: 0.4460\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4659 - val_loss: 0.4479\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4654 - val_loss: 0.4449\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4647 - val_loss: 0.4470\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4650 - val_loss: 0.4471\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4461\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4640 - val_loss: 0.4448\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4648 - val_loss: 0.4443\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4632 - val_loss: 0.4518\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4631 - val_loss: 0.4445\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4632 - val_loss: 0.4435\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4628 - val_loss: 0.4432\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4623 - val_loss: 0.4445\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4625 - val_loss: 0.4447\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4623 - val_loss: 0.4444\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 22us/step - loss: 0.4622 - val_loss: 0.4436\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 26us/step - loss: 0.4616 - val_loss: 0.4437\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.458 - 1s 32us/step - loss: 0.4616 - val_loss: 0.4450\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4608 - val_loss: 0.4436\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4481\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4609 - val_loss: 0.4447\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4616 - val_loss: 0.4434\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4436\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4613 - val_loss: 0.4455\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4603 - val_loss: 0.4486\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 30us/step - loss: 0.4607 - val_loss: 0.4449\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4602 - val_loss: 0.4436\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4611 - val_loss: 0.4430\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4605 - val_loss: 0.4457\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4609 - val_loss: 0.4438\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 27us/step - loss: 0.4608 - val_loss: 0.4434\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 24us/step - loss: 0.4599 - val_loss: 0.4454\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 29us/step - loss: 0.4601 - val_loss: 0.4586\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 28us/step - loss: 0.4607 - val_loss: 0.4440\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4606 - val_loss: 0.4442\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 25us/step - loss: 0.4599 - val_loss: 0.4439\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 23us/step - loss: 0.4593 - val_loss: 0.4441\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(12, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_11 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66555347845455159"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_model_11 = np.sqrt(min(model_11.history[\"val_loss\"]))\n",
    "RMSE_model_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a wider network to evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25946 samples, validate on 11120 samples\n",
      "Epoch 1/100\n",
      "25946/25946 [==============================] - 2s 89us/step - loss: 1.2000 - val_loss: 0.4537\n",
      "Epoch 2/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4759 - val_loss: 0.4514\n",
      "Epoch 3/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4698 - val_loss: 0.4512\n",
      "Epoch 4/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4702 - val_loss: 0.4507\n",
      "Epoch 5/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4654 - val_loss: 0.4462\n",
      "Epoch 6/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4645 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4642 - val_loss: 0.4491\n",
      "Epoch 8/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4646 - val_loss: 0.4495\n",
      "Epoch 9/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4639 - val_loss: 0.4569\n",
      "Epoch 10/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4613 - val_loss: 0.4495\n",
      "Epoch 11/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4597 - val_loss: 0.4499\n",
      "Epoch 12/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4608 - val_loss: 0.4474\n",
      "Epoch 13/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4571 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4570 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4575 - val_loss: 0.4564\n",
      "Epoch 16/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4563 - val_loss: 0.4487\n",
      "Epoch 17/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4567 - val_loss: 0.4704\n",
      "Epoch 18/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4553 - val_loss: 0.4559\n",
      "Epoch 19/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4543 - val_loss: 0.4502\n",
      "Epoch 20/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4536 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "25946/25946 [==============================] - 1s 42us/step - loss: 0.4541 - val_loss: 0.4596\n",
      "Epoch 22/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4518 - val_loss: 0.4702\n",
      "Epoch 23/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4522 - val_loss: 0.4524\n",
      "Epoch 24/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4499 - val_loss: 0.4516\n",
      "Epoch 25/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4540 - val_loss: 0.4513\n",
      "Epoch 26/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4498 - val_loss: 0.4483\n",
      "Epoch 27/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4487 - val_loss: 0.4527\n",
      "Epoch 28/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4475 - val_loss: 0.4543\n",
      "Epoch 29/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4480 - val_loss: 0.4518\n",
      "Epoch 30/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4478 - val_loss: 0.4599\n",
      "Epoch 31/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4466 - val_loss: 0.4541\n",
      "Epoch 32/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4460 - val_loss: 0.4571\n",
      "Epoch 33/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4470 - val_loss: 0.4553\n",
      "Epoch 34/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4450 - val_loss: 0.4566\n",
      "Epoch 35/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4466 - val_loss: 0.4526\n",
      "Epoch 36/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4438 - val_loss: 0.4510\n",
      "Epoch 37/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4436 - val_loss: 0.4556\n",
      "Epoch 38/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4425 - val_loss: 0.4542\n",
      "Epoch 39/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4425 - val_loss: 0.4563\n",
      "Epoch 40/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4425 - val_loss: 0.4554\n",
      "Epoch 41/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4423 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4394 - val_loss: 0.4536\n",
      "Epoch 43/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4398 - val_loss: 0.4538\n",
      "Epoch 44/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4401 - val_loss: 0.4545\n",
      "Epoch 45/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4404 - val_loss: 0.4568\n",
      "Epoch 46/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4388 - val_loss: 0.4591\n",
      "Epoch 47/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4384 - val_loss: 0.4603\n",
      "Epoch 48/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4390 - val_loss: 0.4563\n",
      "Epoch 49/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4381 - val_loss: 0.4656\n",
      "Epoch 50/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4367 - val_loss: 0.4635\n",
      "Epoch 51/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4367 - val_loss: 0.4556\n",
      "Epoch 52/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4352 - val_loss: 0.4564\n",
      "Epoch 53/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4398 - val_loss: 0.4584\n",
      "Epoch 54/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4350 - val_loss: 0.4586\n",
      "Epoch 55/100\n",
      "25946/25946 [==============================] - 2s 62us/step - loss: 0.4355 - val_loss: 0.4676\n",
      "Epoch 56/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4363 - val_loss: 0.4846\n",
      "Epoch 57/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4345 - val_loss: 0.4665\n",
      "Epoch 58/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4344 - val_loss: 0.4592\n",
      "Epoch 59/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4322 - val_loss: 0.4600\n",
      "Epoch 60/100\n",
      "25946/25946 [==============================] - 1s 54us/step - loss: 0.4330 - val_loss: 0.4574\n",
      "Epoch 61/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4346 - val_loss: 0.4573\n",
      "Epoch 62/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4321 - val_loss: 0.4560\n",
      "Epoch 63/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4310 - val_loss: 0.4589\n",
      "Epoch 64/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4312 - val_loss: 0.4641\n",
      "Epoch 65/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4299 - val_loss: 0.4615\n",
      "Epoch 66/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4292 - val_loss: 0.4619\n",
      "Epoch 67/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4315 - val_loss: 0.4575\n",
      "Epoch 68/100\n",
      "25946/25946 [==============================] - 1s 50us/step - loss: 0.4292 - val_loss: 0.4591\n",
      "Epoch 69/100\n",
      "25946/25946 [==============================] - 1s 46us/step - loss: 0.4296 - val_loss: 0.4605\n",
      "Epoch 70/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4276 - val_loss: 0.4631\n",
      "Epoch 71/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4274 - val_loss: 0.4616\n",
      "Epoch 72/100\n",
      "25946/25946 [==============================] - 1s 51us/step - loss: 0.4281 - val_loss: 0.4620\n",
      "Epoch 73/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4276 - val_loss: 0.4610\n",
      "Epoch 74/100\n",
      "25946/25946 [==============================] - 2s 60us/step - loss: 0.4301 - val_loss: 0.4593\n",
      "Epoch 75/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4273 - val_loss: 0.4620\n",
      "Epoch 76/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4265 - val_loss: 0.4665\n",
      "Epoch 77/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4262 - val_loss: 0.4634\n",
      "Epoch 78/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4255 - val_loss: 0.4630\n",
      "Epoch 79/100\n",
      "25946/25946 [==============================] - 1s 48us/step - loss: 0.4266 - val_loss: 0.4648\n",
      "Epoch 80/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4243 - val_loss: 0.4786\n",
      "Epoch 81/100\n",
      "25946/25946 [==============================] - 1s 57us/step - loss: 0.4256 - val_loss: 0.4656\n",
      "Epoch 82/100\n",
      "25946/25946 [==============================] - 1s 55us/step - loss: 0.4242 - val_loss: 0.4633\n",
      "Epoch 83/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4230 - val_loss: 0.4642\n",
      "Epoch 84/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4244 - val_loss: 0.4685\n",
      "Epoch 85/100\n",
      "25946/25946 [==============================] - 1s 47us/step - loss: 0.4235 - val_loss: 0.4653\n",
      "Epoch 86/100\n",
      "25946/25946 [==============================] - ETA: 0s - loss: 0.421 - 1s 52us/step - loss: 0.4229 - val_loss: 0.4667\n",
      "Epoch 87/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4215 - val_loss: 0.4664\n",
      "Epoch 88/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4212 - val_loss: 0.4697\n",
      "Epoch 89/100\n",
      "25946/25946 [==============================] - 1s 52us/step - loss: 0.4218 - val_loss: 0.4694\n",
      "Epoch 90/100\n",
      "25946/25946 [==============================] - 1s 49us/step - loss: 0.4204 - val_loss: 0.4637\n",
      "Epoch 91/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4224 - val_loss: 0.4678\n",
      "Epoch 92/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4214 - val_loss: 0.4690\n",
      "Epoch 93/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4213 - val_loss: 0.4697\n",
      "Epoch 94/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4206 - val_loss: 0.4886\n",
      "Epoch 95/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4210 - val_loss: 0.4723\n",
      "Epoch 96/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4187 - val_loss: 0.4833\n",
      "Epoch 97/100\n",
      "25946/25946 [==============================] - 1s 45us/step - loss: 0.4196 - val_loss: 0.4683\n",
      "Epoch 98/100\n",
      "25946/25946 [==============================] - 1s 53us/step - loss: 0.4193 - val_loss: 0.4724\n",
      "Epoch 99/100\n",
      "25946/25946 [==============================] - 1s 44us/step - loss: 0.4176 - val_loss: 0.4672\n",
      "Epoch 100/100\n",
      "25946/25946 [==============================] - 1s 43us/step - loss: 0.4178 - val_loss: 0.4779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66800683261370652"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1] * 10, input_dim = X.shape[1], kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(120, kernel_initializer= \"normal\", activation= \"relu\"))\n",
    "model.add(Dense(1, kernel_initializer= \"normal\"))\n",
    "model.compile(optimizer= \"adam\", loss= \"mean_squared_error\")\n",
    "model_12 = model.fit(X,y,batch_size=100,epochs = 100,validation_split= 0.3)\n",
    "RMSE_model_12 = np.sqrt(min(model_12.history[\"val_loss\"]))\n",
    "RMSE_model_12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really not any better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can scale some of the predictors to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>no.brand_name</th>\n",
       "      <th>log.excl.description</th>\n",
       "      <th>excl.name</th>\n",
       "      <th>dollar.description</th>\n",
       "      <th>fancy.categories</th>\n",
       "      <th>cheap.categories</th>\n",
       "      <th>fancy.brands</th>\n",
       "      <th>...</th>\n",
       "      <th>now</th>\n",
       "      <th>cheap</th>\n",
       "      <th>buy</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>michael.brand</th>\n",
       "      <th>jordan.name</th>\n",
       "      <th>iphon.name</th>\n",
       "      <th>bundl.name</th>\n",
       "      <th>cap.letter.brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "      <td>37066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.902876</td>\n",
       "      <td>26.581045</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>0.078266</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.102412</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>1.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.901044</td>\n",
       "      <td>36.188265</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>0.494908</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.231276</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>0.071413</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.089156</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>1.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.332853</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping  no.brand_name  \\\n",
       "count       37066.000000  37066.000000  37066.000000   37066.000000   \n",
       "mean            1.902876     26.581045      0.449630       0.428776   \n",
       "std             0.901044     36.188265      0.497463       0.494908   \n",
       "min             1.000000      0.000000      0.000000       0.000000   \n",
       "25%             1.000000     10.000000      0.000000       0.000000   \n",
       "50%             2.000000     17.000000      0.000000       0.000000   \n",
       "75%             3.000000     29.000000      1.000000       1.000000   \n",
       "max             5.000000   1106.000000      1.000000       1.000000   \n",
       "\n",
       "       log.excl.description     excl.name  dollar.description  \\\n",
       "count          37066.000000  37066.000000        37066.000000   \n",
       "mean               0.461340      0.078266            0.018319   \n",
       "std                0.985842      0.559334            0.231276   \n",
       "min                0.000000      0.000000            0.000000   \n",
       "25%                0.000000      0.000000            0.000000   \n",
       "50%                0.000000      0.000000            0.000000   \n",
       "75%                0.693147      0.000000            0.000000   \n",
       "max               11.332853     21.000000           14.000000   \n",
       "\n",
       "       fancy.categories  cheap.categories  fancy.brands        ...         \\\n",
       "count      37066.000000      37066.000000  37066.000000        ...          \n",
       "mean           0.003480          0.005126      0.009820        ...          \n",
       "std            0.058892          0.071413      0.098611        ...          \n",
       "min            0.000000          0.000000      0.000000        ...          \n",
       "25%            0.000000          0.000000      0.000000        ...          \n",
       "50%            0.000000          0.000000      0.000000        ...          \n",
       "75%            0.000000          0.000000      0.000000        ...          \n",
       "max            1.000000          1.000000      1.000000        ...          \n",
       "\n",
       "                now         cheap           buy     excellent         great  \\\n",
       "count  37066.000000  37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean       0.032752      0.005315      0.031835      0.031134      0.102412   \n",
       "std        0.198214      0.076683      0.211528      0.175689      0.330199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        4.000000      3.000000      6.000000      2.000000      4.000000   \n",
       "\n",
       "       michael.brand   jordan.name    iphon.name    bundl.name  \\\n",
       "count   37066.000000  37066.000000  37066.000000  37066.000000   \n",
       "mean        0.008229      0.008013      0.020774      0.051395   \n",
       "std         0.090339      0.089156      0.142628      0.220805   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cap.letter.brand  \n",
       "count      37066.000000  \n",
       "mean           1.589732  \n",
       "std            1.198543  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            1.000000  \n",
       "75%            2.000000  \n",
       "max           17.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find which columns do not have values restricted between [0,1]\n",
    "\n",
    "select_col = (df.describe().loc[[\"max\",\"min\"],:].apply(sum,axis = 0)) != 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-2e7ac6accb6e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-2e7ac6accb6e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pd.Series(select_features[x != \"price\" for x in select_features])\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "select_features =  df.columns[select_col].tolist()\n",
    "select_features[x != \"price\" for x in select_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_features[:] != \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "X_stdscale = preprocessing.StandardScaler().fit(df[select_features]) # Create a preprocessing object on select features\n",
    "df_X_stdscale = X_stdscale.transform(df[select_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.027829e-15\n",
       "1    -2.288079e-16\n",
       "2     5.450259e-15\n",
       "3     3.437801e-15\n",
       "4     8.985525e-16\n",
       "5     1.658329e-15\n",
       "6     4.924806e-17\n",
       "7     1.064549e-15\n",
       "8    -1.118217e-15\n",
       "9    -1.466933e-15\n",
       "10    1.694972e-15\n",
       "11    1.282229e-15\n",
       "12    1.338746e-16\n",
       "13   -6.921656e-16\n",
       "14   -1.552518e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_X_stdscale).apply(np.mean,axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
